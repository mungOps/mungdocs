[{"id":0,"href":"/erp/docs/OpenStack/OpenStack/openstack/","title":"OpenStack 개요","section":"OpenStack docs","content":" 인프라 환경 변화의 시작, 클라우드 # 클라우드 컴퓨팅의 정의와 종류 # # 클라우드 컴퓨팅(Cloud Computing) # 인터넷이 가능한 디바이스(스마트폰, 스마트패드, 스마트TV 등)로 클라우드에서 데이터를 처리하며, 저장 및 관리하는 컴퓨팅 시스템\n클라우드 서비스의 종류\nIaaS(Infrastrcture as a Service): 서버, 스토리지, 네트워크를 가상화 환경으로 만 들어 필요에 따라 인프라 자원을 제공하는 서비스\nPaaS(Platform as a Service): 웹에서 개발 플랫폼을 제공하는 서비스\nSaaS(Software as a Service): 온디맨드 소프트웨어(On-demand Software)라고도 하며, 중앙에서 호스팅 되는 소프트웨어를 웹 브라우저 등 클라우이언트로 이용하는 서비스\nDaas(Desktop as a Service): 클라우드 인프라를 이용해 os가 설치된 인스턴스를 제공하는 서비스\nBaaS(Backend as a Service): 모바일 환경에 맞춰 구현하기 힘든 백엔드 부분을 제공하는 서비스\nPublic Cloud: 언제든지 접근이 가능한 클라우드 서비스\nPrivate Cloud: 외부에서는 접근이 불가능한 사내 클라우드 서비스\nHybrid Cloud Management System: 퍼블릭 클라우드와, 프라이빗 클라우드를 혼용하는 클라우드 서비스\n# 클라우드 핵심 서비스 컴퓨트와 스토리지 # 컴퓨트 서비스(Compute Service)\n사용자가 원하는 운영체제가 탑재된 컴퓨터나 서버를 인터넷에서 사용할 수 있게 제공하는 서비스 스토리지 서비스(Storage Service)\n사용자가 소유한 데이터나 음악, 동영상, 문서 파일을 인터넷에 있는 스토리지에 저장, 삭제 공유할 수 있는 서비스 # 하이퍼바이저의 정의와 종류 # # 하이퍼바이저의 정의 # # 하이퍼바이저(Hypervisor)\n가상 머신 모니터라고도 하며, 호스트 컴퓨터 한 대에서 운영체제 다수를 동시에 실행하는 논리적 플랫폼을 의미 하이퍼바이저의 분류\nNative, 베어메탈 방식: 하드웨어에 직접 설치해서 실행되는 방식 Hosted 방식: 애플리케이션처럼 프로그램으로 설치되는 방식 가상화 방식에 따른 하이퍼바이저의 분류\n전가상화 방식(Full Virtualization): 하드웨어를 모두 가상화하는 방식으로, 게스트 운영체제를 변경하지 않고, 다양한 운영체제로 이용할 수 있음. Native 방식이 이에 해당\n반가상화 방식(Para Virtualization): 하이퍼바이저로만 제어가 가능한 방식으로, 높은 성능의 유지가 가능하지만, 오픈 소스가 아니면 운영이 불가능\n# 하이퍼바이저의 종류 # KVM(for Kerne-based VirtualMachine):\n오픈스택의 거본 하이퍼바이저로 전가상화 방식을 지원 반드시 Inter VT나 AMD-V가 있어야만 사용이 가능 리눅스, 윈도 이미지를 수정하지 않고 여러 가상 머신으로 실행이 가능 Xen과 Xen Server:\nCenter를 이용한 관리 기능, 스토리지 지원과 실시간 마이그레이션, 고가용성 기능처럼 데이터센터에서 요구하는 확장 기능을 제공 Hyper-V:\n디바이스 드라이버가 부모 파티션 위에서 동작하며, 콘솔 OS의 역할을 부모 파티션이 수행 다른 하이퍼바이저의 비해 크기가 작아 오류 코드가 포함될 확류이 낮음 Inter VT, AMD-V x64를 지원하는 하드웨어가 있어야 가상화가 가능 VMware vSphere ESX:\n적은 하드웨어서도 애플리케이션을 통합할 수 있도록 서버를 가상화해주는 무료 베어메탈 하이퍼바이저 ESX는 가상 머신의 업무를 지원하는 역할을 수행, 가상 머신이 발생시킨 명령어를 하이퍼바이저가 받아 재작업 후, 가상 환경에서 잘 구동하도록 바이너리 변환 방식을 사용 Inter, VT, AMD-V 같은 가상화를 지원하는 디바이스가 없어도 가상화를 구현할 수 있음 Docker:\n리눅스 기반의 컨테이너 런타임 오픈 소스로, 가상 머신과 기능이 유사하며, 가상 머신보다 훨씬 가벼운 형태로 배포가 가능 컨테이너의 개념으로 가상 머신처럼 Docker Engine을 호스트 웨어서 수행하며, 리눅스 기반의 운영체제만 수행이 가능 가상 머신처럼 하드웨어를 가상화하는 것이 아니라, 게스트 OS를 분리시켜 제공 호스트 운영체제의 프로세스 공간을 공유한다고 할 수 있음 VirtualBox:\n리눅스, OS X, 솔라리스, 윈도를 게스트 운영체제로 가상화하는 x86 가상화 소프트웨어 다른 하이퍼바이저와 비교했을 때는 기능이 부족 원격 데스크톱 프로토콜(RDP), iSCSI 지원, RDP를 거치는 원격 디바이스의 USB 지원처럼 원격 가상 컴퓨터를 제어할 수 있는 기능이 있음 Inter VT와 AMD-V를 지원 VMware Workstation:\n게스트 운영체제에 설치할 수 있는 다리이버 및 기타 소프트웨어의 묶음 게스트 머신이 고해상도 화면에 접근할 수 있게 하는 VESA호한 그래픽, 네트워크 인터페이스 카드용 네트워크 드라이버, 호스트와 게스트 간 클립보드 공유, 시간 동기화 기능 등을 제공 Parallels Desktop:\n맥용 인텔 프로세서가 있는 매킨토시 컴퓨터에 하드웨어 가상화를 제공하려고 만든 소프트웨어 MS-DOS, 윈도, 맥, 리눅스, 솔라시스 등 다양한 운영체제를 가상화 할 수 있음 # 하이퍼바이저별 이미지 포맷 # KVM: img, qcow2, vmdk VMWARE: vmdk 오라클 VirtualBOx: vdi, vmdk, qcow2, vhd 마이크로소프트 Hyper-V: vhd, vmdk, vdi Xen, Xen Server: qcow2, vhd # 이미지포맷 설명 # qcow2: QEMU Copy On Write 2 vdi: Virtual Disk Image vmdk: VMware Virtual Disk DevelopmentKit vhd: Virtual Hard Disk # 클라우드에서 알아야 할 네트워크 상식 # # 고정 IP, 유동 IP\n고정IP (Fixed IP): 인터넷 공유기를 연결해 고정으로 할당받는 IP 유동IP (Floating IP): 가상 인스턴스가 외부에서 접근할 수 있도록 할당하는 인터넷이 가능한 IP 클래스의 범위\nA 클래스: 1 ~ 126 B 클래스: 128 ~ 191 C 클래스: 192 ~ 223 D 클래스: 224 ~ 239 E 클래스: 240 ~ 254 멀티캐스트는 D 클래스, E 연구 개발 목적으로 예약된 클래스 # CIDR(Classless Inter-Domain Routing) # 클래스가 없는 도메인간 라우팅 기법으로 기존 IP할당 방식인 네트워크 클래스를 대체 급격히 부족해지는 IPv4 주소를 좀 더 효율적으로 사용 접두어를 이용한 주소 지정 방식의 계층적 구조를 사용해 인터넷 라우팅의 부담을 덜어 줌 # SDN(Software Defined Networking) # 네트워크 제어 기능이 물리적 네트워크와 분리되도록 프로그래밍한 네트워크 구조를 뜻함 네트워크 제어 기능을 데이터 전달 기능과 분리해서 구현해야 한다. 네트워크 제어 기능이 개발되고 실행될 수 있는 환경을 분리해 낮은 성능의CPU가 있는 하드위어 위에 스위치에 더 이상 위치시키지 않는다. # 오픈플로(OpenFlow) # SDN의 근간이 되는 기술로 SDN 아키텍처의 컨트롤 레이어와 인프라스트럭처 레이어 사이에 정의된 최초의 표준 통신 인터페이스 흐름정보로 패킷의 전달 경로와 방식을 제어 오픈플로는 오픈플로 컨트롤러와 오픈플로로 지원 네트워크 장비(라우터, 스위치) 사이에서 커뮤니케이션 역할을 담당 일반적인 네트워크 장비(라우터, 스위치)는 플로 테이블을 이용해서 네트워크 트래픽을 처리하는 반면, 오픈플로는 소프트웨어 컨트롤러로 플로테이블을 조작하고 데이터 경로를 설정 # 네트워크 장비 # # 라우터(Router):\n인터넷 등 서로 다른 네트워크를 연결할 때 사용하는 장비 데이터 패킷이 목적지까지 갈 수 있는 경로를 검사하여 최적의 경로를 탐색하는 것을 라우팅이라 함 경로가 결정되면 결정된 길로 데이터 패킷하는 것을 스위칭이라고 함 허브(Hub):\n인터넷이 등장하기 이전, 컴퓨터와 컴퓨터를 연결해 네트워크를 구성하는 장비 멀티포트(Multiport) 또는 리피터(Repeater)라고도 할 수 있습니다. CSMA/CD(Carrier Sense Multiple Access/Collision Detect):\n이더넷 전송 프로토콜로 IEEE 802.3 표준에 규격화되어 있습니다. 브리지(Bridge):\n콜리전(충돌) 도메인을 나누어 서로 통신이 가능하도록 다리처럼 연결해 주는 네트워크 장비 분리된 콜리전 도메인을 세그먼트라고 한다. 스위치(Switch)\n브리지와 역할이 동일하지만, 소프트웨어적으로 처리하는 스위치가 소프트웨어적으로 처리하는 브리지보다 속도가 더 빠르다. 스위치가 브리지 보다 많은 포트 개수를 제공(20~ 100) 브리지는 Store-and-forward라는 프레임 처리 방식만 지원하지만, 스위치는 Cut-through, Store-and-forward라는 프레임 처리 방식을 지원 스위치 관련 용어\n프레임: 데이터를 주고받을 때 데이터를 적절한 크기로 묶어 놓은 것 프레임 처리 방식: 입력되는 프레임을 스위칭하는 방식입니다. Store-and-forward: 들어오는 프레임 전부를 일단 버퍼에 담아 두고, CRC 등 오류 검출을 완전히 처리한 후 전달(포워딩)하는 스위칭 기법 Cut-through: 스위칭 시스템에서 수신된 패킷 부분만 검사해 이를 곧바로 스위칭하는 방식 # 블록 스토리지와 오브젝트 스토리지 # # 블록 스토리지(Block Storage)와 오브젝트 스토리지(Object Storage) 블록 스토리지: 컴퓨터의 용량을 추가하는 것처럼 클라우드 상의 하드 디스크를 블록 스토리지라고 함 오브젝트 스토리지: 사용자 계정별로 저장 공간을 할당할 수 있는 스토리지 시스템으로 블록 스토리지와는 다르게 단독으로 구성이 가능하며, 계정의 컨테이너 파일이나 데이터를 저장할 수 있는 저장 공간 # 대표적인 스토리지 서비스 # # 아마존의 EBS와 S3:\nEBS(Elastic Block Store)는 블록 스토리지에 해당하는 서비스 EC2(Elastic Compute Cloud)은 생성한 인스턴스에 확장해서 사용할 수 있는 스토리지 서비스 S3는 오브젝트 스토리지에 해당하는 서비스로 사용자 계정에 해당하는 Owner, 컨테이너에 해당하는 Bucket, 파일이나 해당데이터에 해당하는 오브젝트로 구성되어있다. 오픈스택의 Cinder와 Swift\nCinder는 오픈스택의 기본 서비스 중 하나로 블록 스토리지 서비스를 제공한다. Cinder는 cinder-volume, cinder-backup, cinder-scheduler, Volume Provider, cinder-api로 구성 Nova에서 제공하는 인스턴스의 확장 스토리지로 사용할 수 있다. Swift는 오픈스택의 기본 서비스 중 하나로 오브젝트 스토리지 서비스를 제공한다. Swift는 proxy-server, account-server, container-server, object-server, swift-api로 구성된다. proxy-server는 여러 대의 스토리지 노드로 구성된 account-server, container-server, object-server을 관리한다. Ceph의 RBD와 RADOS\nCeph는 모든 종류의 스토리지 서비스를 모아 놓은 오픈 소스 서비스라고 할 수 있다. RADOS라는 스토리지 노드 위에 LIBRADOS라는 RADOS 라이브러리가 있다. 아마존의 S3, 오픈스택의 Swift와 연동하는 RADOSGW(게이트웨이)가 있다 QEMU나 KVM에서 생성한 인스턴스를 블록 스토리지로 사용하는 RBD(Rados Block Device), 사용자의 편의성을 제공하려고 POSIX(표준 운영체제 인터페이스)를 제공하는 Ceph FS로 구성되어 있다. # # OpenStack # # 오픈스택과 아키텍처 # # 오픈스택 # 오픈스택은 컴퓨트, 오브젝트 스토리지, 이미지, 인증 서비스 등이 유기적으로 연결되어 하나의 커다한 클라우드 컴퓨팅 시스템을 구축하는 것.\n개념 아키텍처로 살펴보는 오픈스택의 변화\n오픈스택의 변화 ... 백사버전부터는 컴퓨트 서비스에는 Nova 추가 스토리지 서비스에는 Swift 추가 이미지 관리 서비스에는 Glance 추가 Nova, Swift, Glance의 인증을 담당하는 Keystone 추가 서비스를 보다 쉽게 이용하려고 사용자에게 대시보드를 제공하는 Horizon 추가 폴섬 버전부터는 네트워크 서비스와 블록 스로리지 서비스를 Quantum와 Cinder 로 분류함 Quantum은 기존 nova-network와 다르게 OpenFlow를 사용해서 여러 네트워크 컨트롤러의 지원이 가능 하바나버전부터는 오케스트레이션 서비스인 Heat와 텔레미터 서비스인 Ceilometer가 있습니다. 킬로 이후 버전부터는 빅데이터 프로세싱 프레임워크인 Sahara 추가 데이터베이스 서비스인 Trove 추가 PXE나 IPMI를 사용해 베어메탈을 프로비저닝하는 Ironic 추가 코어 서비스 6개와 이를 지원하는 많은 서비스를 표현한 빅텐트(Big-tent)라는 개념 추가 # 클라우드 서비스(오픈스택을 기준으로) # # 시스템 관련 클라우드 서비스\nNova 스토리지 관련 클라우드 서비스\nSwift : 객체 스토리지 Cinder : 블록 스토리지 네트워크 관련 클라우드 서비스\nNeutron 데이터 관련 클라우드 서비스\nGlance Trove 기타 클라우드 서비스\nHorizon Keystone # 논리 아키텍처로 살펴보는 오픈스택의 변화 오픈스택의 변화 ... 상황별 오픈스택 구성 요소\n사내 클라우드 컴퓨팅 환경을 구축할 때나 퍼블릭 클라우드 서비스를 구성할 때 오픈스택을 주로 채택 회사의 클라우드 환경을 어떤 목적으로 사용하느냐에 따라 선택해야할 서비스가 달라질 수 있음 # HTC(High Throughput Computing): HTC 사용자는 종종 Nova 컴퓨트로 전환해 Horizon 대시보드로 단일 API 엔드포인트를 사용자에게 제공함. Keystone은 일반적으로 사용자 계정이 저장되는 LDAP 백엔드를 연결하는 데 사용 이런 종류의 프로젝트를 구성하려면 다음이 서비스가 필요함 대시보드 서비스 Horizon 텔레미터 서비스 Ceilometer 블록 스토리지 서비스 Cinder 오케스트레이션 서비스 Heat 이미지 서비스 Glance 인증 서비스 keystone 컴퓨트 서비스 Nova # 웹 호스팅: 웹 호스팅 회사 중 하나로 수백만 개의 호스팅 사용하는 데 오픈스택을 활용 Nova, Neutron, Keystone, Glance, Horizon 같은 일반적인 코어 서비스를 이용 사용자 계정 데이터를 수집하고 요금을 청구할 때 일부 기술로 Ceilometer를 활용 네트워크 서비스 Neutron 대시보드 서비스 Horizon 텔레미터 서비스 Ceilometer 이미지 서비스 Glance 인증 서비스 keystone 컴퓨트 서비스 Nova # 퍼블릭 클라우드: 오픈스택은 전 세계 사용자에에 IaaS를 제공하는 퍼블릭 클라우드를 지원 Nova, Glance, Keystone, Cinder, Neutron 같은 서비스를 제공 Swift를 사용해 오브젝트 스토리지 서비스를 제공, Designate는 DNSaaS(DNS as a Service)를 제공함 네트워크 서비스 Neutron 도메인 네임 서비스 Designate 블록 스토리지 서비스 Cinder 오브젝트 스토리지 서비스 Swift 이미지 서비스 Glance 인증 서비스 Keystone 컴퓨트 서비스 Nova # 웹 서비스, 전자상거래: 이베이, 오버스톡닷컴, 베스트바이 등 많은 회사가 오픈스택을 이용해 웹 서비스도 하고 전자상거래의 백엔드로도 사용 상황에 맞춰 오픈스택 클라우드는 PCI 표준처럼 구성하기도 함 Trove는 내부 고객에게 데이터베이스 서비스인 DaaS를 제공, 네트워크 정의 소프트웨어 SDN은 Neutron을 제공 네트워크 서비스 Neutron 대시보드 서비스 Horizon 데이터베이스 서비스 Trove 블록 스토리지 서비스 Cinder 이미지 서비스 Glance 인증 서비스 Keystone 컴퓨트 서비스 Nova # 컴퓨트 스타터 키트(Compute Starter Kit): 더 많은 사람이 오픈스택을 사용할 수 있도록 하는 것이 컴퓨터 스타터 키트라고 함 스타터 키트는 추가 기능으로 클라우드 확장할 수 있는 방법을 문서화로 제공하는 단순한 프로젝트를 의미함 네트워크 서비스 Neutron 이미지 서비스 Glance 인증 서비스 Keystone 컴퓨트 서비스 Nova # 빅데이터: 다양한 리소스 데이터를 분석하는 빅데이터에도 활동 됨 빅데이터 분석 서비스인 Sahara 프로젝트는 오픈스택 위에 빅데이터 응용프로그램(Hadoop, Spark)을 간단하게 제공할 수 있음 네트워크 서비스 Neutron 대시보드 서비스 Horizon 베이메탈 서비스 Ironic 빅데이터 서비스 Sahara 이미지 서비스 Glance 인증 서비스 Keystone 컴퓨트 서비스 Nova # DBaaS: 대부분의 회사는 응용프로그램을 백업하려 데이터베이스에 크게 의존하며 일반적인 관리 자동화 및 스케일 아웃을 최우선으로 생각 오픈스택 Trove 프로젝트는 이 기능을 제공 및 여러 SQL 및 NoSQL 백엔드를 지원 Ironic 프로젝트는 데이터베이스의 성능을 극대화하려고 베어메탈 프로비저닝을 제공 네트워크 서비스인 Neutron 대시보드 서비스 Horizon 데이터베이스 서비스 Trove 도메인 네임 서비스 Designate 베어메탈 서비스 Ironic 블록 스토리지 서비스 Cinder 오브젝트 스토리지 서비스 Swift 이미지 서비스 Glance 인증 서비스 Keystone 컴퓨트 서비스 Nova # 비디오 처리와 콘텐츠 전달: 제작 스튜디오나 주요 케이블 서비스 제공 업체 같은 곳의 비디오 처리(Video Processing), 콘텐츠 전달(Contents Delivery)은 오픈스택의 보편적인 사용 예시임 Keystone에서 선보인 인증 표준은 이제 동일한 대시보드와 인증을 사용해 프라이빗 클라우드 및 퍼블릭 클라우드에서 비디오 콘텐츠를 원할하게 이동시킬 수 있음 네트워크 서비스 Neutron 오브젝트 스토리지 서비스 Swift 인증 서비스 Keystone 컴퓨트 서비스 Nova # 컨테이너 서비스: 가상머신, 컨테이너, 베어메탈에서 실행되는 워크로드를 단일 클라우드에서 운영할 수 있도록 개발되었음 Kubernetes, Mesos, Docker 같은 새로운 컨테이너 오케스트레이션 엔징(COE, container Orchestration Engices)와 통합하려고 Magnum 프로젝트에 엑세스 할 수 있음 네트워크 서비스 Neutron 대시보드 서비스 Horizon 베어메탈 서비스 Ironic 블록 스토리지 서비스 Cinder 이미지 서비스 Glance 인증 서비스 Keystone 컨테이너 서비스 Management 컴퓨트 서비스 Nova # 오픈스택 적용사례 # 업종별·목적별 클라우드 활용 사례 웹 사이트에서 클라우드 활용 소셜 게임의 클라우드 활용 애플리케이션 개발/테스트 환경에서의 클라우드 활용 스타트업 기업에서의 클라우드 활용 BCP(비지니스 연속성 계획)의 클라우드 활용 ERP(통합 기간 업무 시스템)에서의 클라우드 활용 제조업의 클라우드 활용 지자체 클라우드 교육 분야의 클라우드 활용 농업 분야의 클라우드 활용 빅 데이터 이용을 위한 클라우드의 활용 IoT에서 클라우드 활용 인공 지능 등의 새로운 산업 영역에서의 클라우드 활용 참고 홈페이지 # 오픈스택 릴리스 웹 사이트 Nalee의 IT 이야기 # 오픈스택 파운데이션과 커뮤니티 오픈스택 파운데이션: 나라별로 오픈스택 사용자 그룹을 운영하고 있다. 사용자 그룹은 공식 사용자 그룹과 일반 사용자 그룹으로 나뉨 오픈스택 사용자 그룹은 총 112개, 이중 공시기 사용자 그룹은 18개이며, 엠버서더로 활동하는 구성원은 총 12명, 아시아는 6명이다. 오픈스택은 버전별 컨트리뷰터 활동을 그래프와 표로 보여주는 http://stackalytics.com/을 운영한다. "},{"id":1,"href":"/erp/docs/OpenStack/OpenStack/Keystone/","title":"Keystone","section":"OpenStack docs","content":" 인증을 관리하는 서비스 : Keyston # # 인증을 관리하는 서비스 : Keystone # # Keystone은 인증 토큰을 비롯해 테넌트 및 사용자 관리, 서비스의 엔드포인트 URL 등을 관리하는 서비스 Keystone은 openstack의 백엔드에서 RBAD ( Role Based Access Control )을 통해 사용자의 접근을 제어하는 등의 인증 ( Identify ) 서비스로 사용되며 다음과 같은 기능으로 이루어져 있음 # # Keystone의 구성요소 # # # 구성요소 역할 user 사람 또는 오픈 스택 서비스를 이용하는 서비스 ( nova, neutron, cinder 등 )을 의미 User은 특정 프로젝트에 할당할 수 있으며, 증복을 허용하지 않음 Authentication 사용자의 신분을 확인하는 절차로, 특정 값을 통해 Keystone이 이를 검증 보통 인증을 위한 자료로는 ID, PW가 사용되며 Keystone은 인증확인 시 인증토큰을 방행 Token RBAD의 신분을 증명하기 위해 사용되는 텍스트 데이터 Token type fernet, uuid, pki, pkiz 어떤 자원에 접근이 가능한지 범위가 지정되어 있음 ( 시간 제한 있음 ) Project Keystone V2까지 Tenant라는 이름으로 사용 ( V3 이후 Project ) 어떤 자원이나 어플리케이션에 대한 권리를 가진 보안그룹 프로젝트는 특정 도메인에 의해 소유 Endpoint 사용자가 서비스를 이용하기 위해 연결정보를 제공하는 접근 가능한 네트워크 주소 ( URL ) EndPoint type admin, internal, public Role 사용자가 어떤 동작을 수행하도록 허용하는 규칙 사용자가 가지는 역할은 사용자에게 발행된 토큰을 통해 확인 사용자가 서비스를 호출하면, 서비스는 토큰에 저장된 사용자의 역할을 해석하여 허용유무 결정 Domain 구성요소를 효과적으로 관리하기 위한 사용자, 그룹, 프로젝트의 집합 사용자들은 한 도메인의 관리자의 권한 등을 부여받는 방식으로 역할을 부여가능 # Domain, Project, Group, User, Rule 개념과 관계 # Keystone은 위에도 언급하였 듯이 사용자 인증 부분과 서비스 인증 부분을 관리 사용자일 때는 사용자 ID와 패스워드, 사용자 권한의 롤( Roll )을 등록 서비스일 때는 서비스를 등록하고 해당 서비스의 엔드포인트 URL을 등록 도메인(Domain)은 서로 분리되어 있음 각 도메인에는 프로젝트와 사용자가 있음 프로젝트는 사용자를 가질 수 있음 사용자에게는 롤이 있으며, 여러 프로젝트의 구성원이 될 수 있음 관리자 롤(Admin Role)을 가진 사용자끼리, 일반 사용자롤(Member Role)을 가진 사용자간의 그룹핑(Grouping)을 할 수 있음 # # Keystone의 논리 아키텍처 # # Keystone의 논리 아키텍처는 토큰(Token), 카탈로그(Catalog), 정책(Poliy), 인증(Identity) 으로 구성 # 구성요소 역할 Token Backend 사용자별 토큰을 관리 Catalog Backend 오픈스택에서 모든 서비스의 엔드포인트 URL을 관리 Policy Backend 테넌트, 사용자 계정, 롤 등을 관리 Identity Backend 사용자 인증을 관리 # # Openstack에서 Keystone 위치 # # # Openstack Keystone은 모든 서비스를 관장하는 위치 모든 User, Service는 Keystone의 인증을 통해서만 요청, 응답이 가능 Keystone은 타인이나 해커에게서 시스템을 안전하게 보호하고 사용자 등록, 삭제, 권한 관리, 사용자가 접근할 수 있는 서비스 포인트 관리와 다른 API들의 인증 등의 전체적인 인증 프로세스를 관리하는 역할을 수행 # "},{"id":2,"href":"/erp/docs/OpenStack/OpenStack/Glance/","title":"Glance","section":"OpenStack docs","content":" 이미지를 관리하는 서비스 : Glance # # 이미지를 관리하는 서비스 : Glance # # Cloud Computing을 사용하기 위해서는 Virtual Machine을 생성하기 위한 이미지가 필요로 하며, Glance는 Nova에서 생성하는 인스턴스의 운영체제에 해당하는 이미지를 관리하는 서비스 # # Glance의 구성요소 # # # Glance는 위의 그림과 같이 3가지의 구성요소로 이루어져 있다 # 구성요소 역할 Glance-api 이미지를 확인/ 복구/ 저장하는 등의 질의를 하기 이한 api 요청/ 응답을 담당 Glance-registry 이미지에 대한 메타데이터를 저장하고 처리하는 역할을 담당 및 Glance database에 저장된 데이터를 불러들이는 역할을 수행 Glance-database 이미지의 관련 정보들을 보관 # # 논리 아키텍처의 Glance # # Glance 사용자들은 glance-api로 이미지를 등록, 삭제, 관리 glance-api는 glance-registry와 Glance database에서 이미지를 관리 이미지를 등록할 때는 glance-registry로 Glance database에 저장 등록된 이미지를 사용할 때는 Glance database에 바로 사용을 요청 관리자는 운영하려는 운영체제의 이미지를 glance-registry로 Glance database에 등록 # # 가상 머신 이미지 포맷 # # aki: 아마존 커널 이미지 ami: 아마존 머신 이미지 ari: 아마존 ram 디스크 이미지 iso: 광학 디스크나 CD-ROM의 데이터 콘텐츠를 지원하는 아카이브 포맷 qcow2: QEMU 에뮬레이터가 지원하는 포맷, 동적으로 확장할 수 있으며, Copy on Write를 지원 raw: 구조화되지 않은 디스크 포맷 vdi: VirtalBox 모니터와 QEMU 에뮬레이터가 지원하는 디스크 포맷 vhd: VHD 디스크 포맷은 VMware, Xen 마이크로소프트, VirtualBox 같은 가상 머신 모니터가 사용하는 일반적인 디스크 포맷 vhdx: VHDX 디스크 포맷은 큰 디스크 크기를 지원하는 VHD 형식의 향상된 버전 vmdk: 일반적인 디스크 포맷으로 여러 가상 머신 모니터가 지원 # # 컨테이너 포맷(container Format) # # aki: 아마존 커널 이미지 ami: 아마존 머신 이미지 bare: 아마존 ram 디스크 이미지 docker: Docker 컨테이너 포맷 ova: tar 파일의 OVF 패키지 ovf: OVF 컨테이너 포맷 # # # # Glance 명령어 # 현재 이미지 목록 확인 openstack image list # 특정 이미지의 자세한 정보 확인 openstack image show [이미지 이름] # 이미지 삭제 openstack image delete [이미지 이름] # 이미지 추가 openstack image create --public --container-format bare --disk-format qcow2 --file [경로를 포함한 이미지 파일 이름] [이미지 이름] # # 커스텀 이미지 생성 # # xming 윈도우에 설치 # CentOS 준비 후 CentOS에 가상머신 프로그램 설치 및 실행 $ yum install qemu kvm qemu-kvm libvirt virt-install bridge-utils virt-manager dejavu-lgc-sans-fonts virt-viewer $ systemctl restart libvirtd # ISO 파일로 qcow2 각 이미지에 맞는 파일 생성 qemu-img create -f qcow2 [이미지 파일 위치] [이미지 파일 크기] qemu-img create -f qcow2 /test/centos7.qcow2 10G # ISO로 가상머신 생성 $ virt-install --name centos \\ --ram 1024 --disk \\ [비어있는 이미지 파일 위치],format=qcow2 \\ --network network=default \\ --graphics vnc,listen=0.0.0.0 \\ --noautoconsole \\ --os-type=linux \\ --os-variant=centos7.0 \\ --location=[ISO 위치] # 본체 윈도우에서 putty x11 설정 Putty -\u0026gt; SSH -\u0026gt; X11 -\u0026gt; Enable X11 Forwarding 체크 -\u0026gt; X display location : localhost:0 설정 후 접속 # virt-manager 생성한 QEMU 가상머신 설정 SELINUX 끄기 acpid 설치 및 설정 cloud-init 및 cloud-utils 설치 및 설정 /etc/sysconfig/network qemu-guest-agent 설치 및 설정 grub 수정 # 생성한 가상머신에서 이미지 작업 ( 커스터 마이징 ) # 설치 후 설정 yum install -y /usr/bin/virt-sysprep virt-sysprep -d centos \u0026lt;-네트워크 장치의 MAC주소와 같은 정보를 삭제하는 작업 virsh undefine centos \u0026lt;-가상머신 삭제하는 작업 "},{"id":3,"href":"/erp/docs/OpenStack/OpenStack/Nova/","title":"Nova","section":"OpenStack docs","content":" 가상의 서버를 생성하는 서비스 : Nova # # 가상의 서버를 생성하는 서비스 : Nova # # Nova는 compute 서비스의 핵심 compute 서비스란, 가상머신이 필요한 자원을 할당하고, 관리하는 서비스로 하이퍼바이저, 메시지 Queue, 인스턴스 접속을 하는 콘솔 등의 다양한 기능이 유기적으로 연결되어 가상 서버를 생성할 수 있는 시스템을 구성하는 시스템 # # Nova 서비스의 고려사항 # # 고려사항 설명 CPU compute 서비스가 동작할 호스트 시스템의 cpu가 기본적으로 자체 하드웨어 가상화를 지원이 필수 Hypervisor 서비스에 사용할 하이퍼바이저를 맞게 설정해야 하며, 기본적으로 사용하는 Hypervisor은 KVM/QEMU Storage compute 서비스를 통해 인스턴스가 생성되면서 시스템의 디스크 용량의 제한을 가할 수 있음, 이를 위해 넉넉한 공간이 필요 Overcommit 기본적으로 자원을 할당하는 경우 1:1이 아닌 CPU는 16:1, Memory는 1.5:1로 할당 되어짐 네트워킹 생성된 인스턴스의 경우 nova가 독자적으로 구현하는 것이 아닌 다른 network 서비스를 연게해서 사용해야하며, 주로 Neutron 네트워크 서비스와 함께 사용 # # Nova의 논리 아키텍처 # # # 서비스 역할 nova-api 최종 사용자즈이 API콜을 통해 서비스 간 질의 응답을 담당 nova-compute 가상화 API를 이용하여 가상 머신 인스턴스를 생성하고 종료하는 역할을 수행 nova-scheduler compute host가 다수인 경우 큐를 통해 받은 메시지를 누구에게 명령할 것인지를 결정 nova-conductor 코디네이션과 데이터베이스 쿼리를 지원하는 서버 데몬 nova-cert X509 인증서에 대한 Nova Cert서비스를 제공하는 서버 데몬 nova-consoleauth 데몬, 콘솔 프록시를 제공하는 사용자에 대한 인증 토큰 제공 Guest Agent 실제 compute 시스템 상에 구축된 인스턴스로 Nova-compute 서비스에 의해 제어되어짐 nova-api-metadata 인스턴스의 메타데이터의 요청을 처리 nova-novncproxy VNC 콘솔화면을 제공 nova-novaclient: nova REST API를 사용하는 클라이언트 프로그램 nova-network | 인스턴스의 네트워크 기능을 수행 nova-compute-kvm | 인스턴스(가상 머신)와 관련된 모든 프로세스를 처리 python-guestfs | 파일 생성 기능을 지원하는 Python 라이브러리 qemu-kvm | KVM 하이퍼바이저\n# 위와 같이 많은 서비스들이 존재 # Nova는 대시보드나 콘솔에서 호출하는 nova-api에서 시작 Queue를 이용해 nova-compute에 인스턴스를 생성하라는 명령을 전달 nova-compute는 하이퍼바이저 라이브러리를 이용해 하이퍼바이저에 인스턴스를 생성하려는 명령어를 전달 Hypervisor을 통해 인스턴스를 생성 생성된 인스턴스는 nova-api로 접근할 수 있으며 Nova의 모든 기능은 메시지 Queue로 처리할 수 있음 # # Nova가 지원하는 하이퍼바이저의 종류 # 기본 하이퍼바이저는 KVM과 QEMU 프로바이더가 테스트하는 Hyper-V, VMware, XenServer, Sen via libvirt 몇 번의 테스트만 하는 하이퍼바이저 드라이버인 베어메탈, Docker, LXC via libvirt # "},{"id":4,"href":"/erp/docs/OpenStack/OpenStack/Neutron/","title":"Neutron","section":"OpenStack docs","content":" 네트워크를 관리하는 서비스: Neutron # # 네트워크를 관리하는 서비스: Neutron # Neutron은 네트워크 서비스로 여러 노드에 여러 프로세스를 배치하는 독립형 서비스 프로세스는 서로 및 다른 OpenStack의 서비스와 상화 작용 # # Neutron의 논리 아키텍처 # # # 구성요소 기능 neutron-server network api의 기능 및 네트워크 확장 기능을 서비스하며, 각 포트의 대한 모델 및 Pfmf 지정, AMQP를 사용하여 데이터베이스와 통신하는 플러그인을 통해 수행 neutron-L2-agent OVS 가상 Bridge 사이에서데이터 패킷을 전달하기 위한 중계장치 neutron-l3-agent 태넌트 네트워크에서 VM의 외부 네트워크 엑세서를 위한 L3/ NAT 전달을 제공 neutron-dhcp-agent 테넌트 네트워크에 DHCP 서비스를 제공, DHCP agent는 메시지 큐에 엑세스할 수 있는 권한이 필요 Queue 다른 서비스 간의 통신의 역할을 수행 Neutron Database Neutron 서비스를 수행하기 위한 일련의 정보들은 보관, 관리하는 DB Neutron 3rd Party Plugin Neutron 서비스의 안정적인 통신 역할을 수행 plugin agent 각 compute node에서 실행되며 로컬 vswitch을 구성 및 관리 network provider services 테넌트 네트워크에 추가 네트워킹 서비스를 제공 # Neutron은 다양한 네트워크 플러그인이나 네트워크 모델을 지원 사용자는 Neutron API를 이용해 neutron-server로 IP 할당을 요청 neutron-server 들어온 요청을 Queue로 다시 요청 Queue는 neutron-dhcp-agent와 Neutron 3rd Party plugin으로 IP 할당 지시를 내림 neutron-dhcp-agent와 Neutron 3rd Party Plugin은 지시 받은 작업 수행을 시작 neutron-server는 수시로 작업 상태를 Neutron database에 저장 할당된 IP를 인스턴스에서 사용 가능 # # Neutron의 네트워킹 프로세스 # # neutron-server에 의해 명령을 요청을 받음 plugin을 토대로 Messae queue를 통해 각 agent의 기능을 수행 이와 함께 SDN 서비스를 수행 # # Neutron network의 종류 # # # 네트워크의 종류 기능 Management network OpenStack 구성 요소 간의 내부 통신에 사용, 기본적으로 IP 주소는 데이터 센터 내에서만 사용이 가능 Guest network 클라우드 배포 내에서 인스턴스 데이터 통신에 사용되며, 네트워킹 플러그인 및 테넌트가 만든 가상 네트워크의 구성 선택에 따라 변동 External network 외부에서 인스턴스에 대한 엑세스를 위해 제공되는 네트워크 API network OpneStack API를 외부에 노출시키는 네트워크 # # Neutron과 VRRP, DVR # # VRRP(Virtual Router Redundancy Protocl)로 랜에서 정적으로 설정된 기본 라우터를 사용할 때, 하나 이상의 백업 라우터를 사용하는 방법을 제공하는 인터넷 프로토콜\nDVR(Distributed Virtual Router)이란 VRRP 기능을 향상시키고, 분산 라우팅 기능과 HA(High Availability), 로드밸런싱 기능을 사용할 수 있음\n기존 레거시 HA 라우터와 마찬가지로 DVR/ SNAT(Static NAT), HA 라우터는 다른 노드에서 실행되는 L3 Agent의 백업 DVR/ SNAT 라우터에서 SNAT 서비스 장애를 빠르게 해결 가능\n# # 네트워크 관련 명령어 # $ openstack network list # 네트워크 확인 $ openstack network show [네트워크 이름] # 네트워크 정보 조회 $ ip netns # 라우터 정보 조회 $ ip netns exec [라우터이름] [리눅스 명령어] netstat -r arp -an ifconfig ping # 라우터의 자세한 정보 조회 $ openstack network create --provider-network-type [타입] [네트워크 이름] # 네트워크 생성 # openstack subnet create --network [네트워크 이름] --gateway [GW주소] --subnet-range [서브넷 범위] [서브넷 이름 # 서브넷 생성 $ openstack router list # 라우터 목록 확인 $ openstack router show [라우터 이름] # 라우터 정보 조회 $ openstack router add subnet [라우터 이름] [서브넷 이름] # 라우터에 서브넷 추가 $ openstack port create --network [네트워크 이름] --fixed-ip subnet=[서브넷 이름] [포트 이름] # 포트 생성 $ openstack router add port [라우터 이름] [포트 이름] # 라우터에 포트 추가 # fixed-ip, floating-ip # # IP 역할 Fixed IP 가상머신에 할당되는 내부 IP를 의미 Floating IP 클라우드 내의 가상머신이 인터넷 외부망과 연결되기 위해 배정 받는 IP를 의미 # Security Group # # 인스턴스에 대한 인바운드 및 아웃바운드 트래픽을 제어하는 가상의 네트워크 방화벽 하나의 인스턴스에 여러 개의 보안 그룹 적용도 가능 # # # "},{"id":5,"href":"/erp/docs/OpenStack/OpenStack/Cinder/","title":"Cinder","section":"OpenStack docs","content":" 블록 스토리지 서비스 : Cinder # # 블록 스토리지 서비스 : Cinder # # Cinder은 bloack storage 서비스로 storage를 가상화 시켜 여러 스토리지로 사용하거나, 공유할 수 있는 서비스 Cinder은 하나 이상의 노드에서 실행되도록 설계되었으며, SQL 기반 중앙 데이터베이스를 사용 Cinder은 집계 시스템을 사용하여 여러 데이터 저장소로 이동이 가능 # # Cinder 구성요소 # # # 구성요소 역할 DB 데이터저장을 위한 SQL 데이터베이스로, 모든 구성요소에서 사용 Web Dashboard API와 통신할 수 있는 외부 인터페이스 api http 요청을 받고 명령을 변환하여 대기열 또는 http를 통해 다른 구성요소와 통신 Auth Manager 사용자/ 프로젝트/ 역할에 따라, 리소스의 대한 허용을 결정 Scheduler 볼륨을 가져올 호스트를 결정 volume 동적으로 부착 가능한 블록 장치를 관리 backup 블록 저장 장치의 백업을 관리 # 외부 인터페이스인 dash-board에서 요청을 보내면, Cinder-api가 keyston에게 인증을 확인하기 위해 요청을 보낸다. 인증이 완료되면 DB를 읽어 알맞은 리소스를 생성, 혹은 할당하는 프로세스를 가진다. # # Cinder의 논리 아키텍처 # # # 구성요소 역할 Cinder-api 요청에 따라 storage를 할당, 확장하는 기능을 수행 Queue 각 구성요소 간의 통신기능을 수행 Cinder database Cinder 서비스를 수행하기 위한 일련의 정보들은 보관, 관리하는 DB cinder voulme Cinder 서비스를 통해 가상화되어진 Storage voulme, voulme을 관리 및 업데이트 volume provider storage volume을 생성, 확장하는 서비스 cinder scheduler 요청이 다수인 경우 큐를 통해 받은 메시지를 수행 # Nova에서 생성된 인스턴스에서 확장해서 사용할 수 있는 저장 공간을 생성, 삭제하고 인스턴스에 연결 할 수 있는 기능을 제공 Cinder는 물리 하드 디스크를 LVM(Logical Volume Manager)으로 설정 설정된 LVM은 cinder-conf와 nova.conf의 환경을 설정해서 cinder-volume을 할당 cinder-api로 생성된 볼륨은 단일 인스턴스 또는 여러 인스턴스에 할당 할 수 있음 # # Cinder driver type # # Cinder 기본 블록 스토리지 드라이버는 iSCSI 기반의 LVM LVM이란 하드 디스크를 파티션 대신 논리 볼륨으로 할당하고, 디스크 여러 개를 좀 더 효율적이고 유연하게 관리할 수 있는 방식을 의미 # 블록 장치는 물리 볼륨으로 초기화해야 하며, 논리 볼륨으로 생성하려면 물리 볼륨을 볼륨 그룹으로 통합해야 함 # "},{"id":6,"href":"/erp/docs/OpenStack/OpenStack/Ceilometer/","title":"Ceilometer","section":"OpenStack docs","content":" 리소스의 사용량과 부하를 관리하는 서비스 : Ceilometer # # 리소스의 사용량과 부하를 관리하는 서비스 : Ceilometer # # # Ceilometer은 각 서비스들의 예상 부하에 따라 추가 작업과 노드를 관리하는 역할을 수행 클라우드에서 배포된 자원의 사용량과 성능을 측정해 사용자가 자원 상태를 모니터링 할 수 있는 기능을 제공 Ceilomete는 리버티 버전에서 기존에 알람 서비스를 하던 부분을 aodh로 분리 # 핵심 서비스 역할 Polling agent OpenStack 서비스를 폴링하고 미터를 빌드하도록 설계된 데몬 프로세스 Notification agent 메시지 큐에서 알림을 듣고 이벤트 및 샘플로 변환하며 파이프 라인 조치를 적용하도록 설계된 데몬 프로세스 # Ceilometer로 표준화 및 수집 된 데이터는 다양한 대상으로 전송 Gnocchi는 이러한 스토리지 및 쿼리를 최적화하기 위해 시계열 형식으로 측정 데이터를 캡처하도록 개발 Aodh는 사용자 정의 규칙이 깨졋을 때 경고를 보낼 수 있는 경보 서비스 Ceilomter은 이와 같이 리소스를 감시 및 예상하여 다른 작업을 수행할 수 있도록 하는 서비스를 의미 # # 데이터 수집 # # # 위의 그림과 같이 Polling agents에서 각 서비스들의 API의 리소스를 읽어 데이터를 수집 Notification agents는 Polling agents에서 수집한 데이터들을 토대로 Ceilomter 서비스 혹은 Events로 변환하는 역할을 수행 # # 데이터 처리 # # # 수집된 데이터를 토대로 Notification bus를 통해 엔드 포인트로 재분배하여 이벤트 및 샘플로 처리하느 역할을 수행 # # 데이터 요청 # # # 데이터의 요청은 수집된 자료들을 토대로 서로 데이터를 주고 받으며, Polling agents라는 클라우드 컨트롤러에서 처리되며, 여러 플러그인을 사용하여 데이터를 통신합니다. # # 데이터 처리 및 변형 # # # # 위의 그림은 수집된 데이터를 수집, 분석 및 변형 배포하는 과정을 나타낸 그림으로 Ceilomter은 각 서비스들의 데이터를 수집하여, 변형시키는 여러 소스를 제공 # # OpenStack에서의 Ceilomter의 위치 # # # 구성요소 역할 Ceilometer-colletcor 중앙 관리서버에서 실행되며, Queue 모니터링 하는 서비스 Ceilometer-agent-notification ceilometer-collector와 함꼐 중앙 관리서버에서 실행, Queue를 이용해 이벤트와 미러링 데이터를 기록 Ceilometer-agent-compute 각 컴퓨팅 노드에 설치해서 실행, 자원 활용 통계로 사용 Ceilometer-account-central 중앙 관리 서버에서 실행, 인스턴스에 연결되지 않은 자원이나 컴퓨터 노드의 활용 가능한 자원 통계를 폴링 Ceilometer-alarm-evaluator 하나 이상의 중앙 관리 서버에서 실행, 슬라이딩 시간대에 임계 값을 추가할 때 발생하는 경보 시점을 결정 Ceilometer-alarm-nottifier 하나 이상의 중앙 관리 서버에서 실행되며, 샘플을 수집하는 임계 값 평가에 따라 알람을 설정 Ceilometer database 수집한 데이터를 저장할 Ceilometer 데이터 베이스 Ceilometer-api 하나 또는 그 이상의 중앙 관리 서버에서 실행되며 데이터베이스에서 데이터 엑세스를 제공 # "},{"id":7,"href":"/erp/docs/OpenStack/OpenStack/Horizon/","title":"Horizon","section":"OpenStack docs","content":" 외부 인터페이스 대시보드 서비스 : Horizon # # 외부 인터페이스 대시보드 서비스 : Horizon # # # 사용자가 웹 UI로 인스턴스 생성, 삭제, 관리 등을 쉽고 빠르게 처리할 수 있게 지원 Horizon은 아파치 웹 서버를 사용 및 Python, Django 프레임워크로 구현 # # 논리 아키텍처의 Horizon # # 논리 아키텍처에서 보이는 Horizon은 단순히 Horizon 자체 모듈만 존재 모든 서비스의 API와 연동해서 사용자에게 웹 서비스를 제공 "},{"id":8,"href":"/erp/docs/OpenStack/OpenStack/Swift/","title":"Swift","section":"OpenStack docs","content":" 오브젝트 스토리지 관리 서비스 : Swift # # 다른 서비스와는 다르게 단독으로 구성되며, 클라우드 스토리지 서비스 ( ex : 네이버 클라우드 ) Swift 서비스는 Object Storage 서비스 중 하나 분산 구조의 Object 데이터의 저장 스토리지 체계로 구성 빠른 성능 및 대용량 저장공간이 필요 할 때 사용 동영상, 이미지, 디스크 이미지 등의 대용량, 비정형 데이터를 파일과 메타데이터로 저장하여 분산 관리 # # 오브젝트 스토리지 관리 서비스 : Swift # # # Swift의 논리 아키텍처 # # 구성요소 역할 swift-proxy-server 사용자가 서비스를 다루기 위한 REST API 서비스 swift-account-server 계정 정보 및 사용자 정보를 관리하고 각 컨테이너 별 정보를 관리하기 위한 데몬 프로세스 swift-container-server 사용자 게정의 컨테이너를 간리하는 서비스 swift-object-server 실제 데이터를 저장하고 관리하는 서비스 어카운트, 컨테이너는 별도의 메타데이터가 데이터베이스로 관리 오브젝트는 저장 공간에 직접 저장되는 방식으로 구성 swift-proxy-server는 오픈스택의 Object API를 제공 사용자는 API를 사용해 데이터를 사용 # # Swift의 논리적 구성 요소 # # Swift서비스는은 스토리지 공간 여러 개를 합쳐 하나의 커다란 공간으로 가상화하고, 그 안에서 사용자만의 별도 스토리지 공간이 있는 것처럼 다시 가상화\nswift-proxy-server는 스토리지 노드 여러 개를 관리하며 사용자 인증을 담당, 최근에는 Keystone으로 인증을 처리하며, 프록시 서버와 함께 설치\n기본적으로 스토리지 노드에는 swift-account-server, swift,compute-server, swift-object-server가 실행되며 실제 메타데이터파일이나 오브젝트에 해당하는 데이터 파일을 저장\nSwift 역시도 Nova를 구성할 떄와 마찬가지로 스토리지 노드가 여러 호스트로 구성이 가능\n각 스토리지 노드에는 swift-account-server, swift-container-server, swift-object-server가 실행\n서버들은 관리자가 설정한 해당 포트로 서로 통신\n스토리지 노드 중 하나라도 손상이 되면 데이터를 잃지 않도록 데이터 복제(Replica)프로세스가 함께 실행\n# # Swift Ring # # 스토리지 파일은 자신이 관리하는 데이터를 서로 공유하려고 Ring인 파일이 어느 노드에, 어떤 데이터가 들어 있는 지를 인지 이를 확인하기 위해 사용도는 것이 Ring 파일 Ring파일은 프록시 노드에서 생성해 모든 스토리지 노드가 동일하게 소유 Ring 파일에는 디바이스 정보 디바이스를 구분하는 ID 존(Zone) 번호 스토리지 노드 IP 포트 디바이스 이름 디바이스 비중 기타 디바이스 정보 # # Swift의 데이터 관리 방법 # # Swift는 사용자 게정을 관리하는 어카운트, 디렉터리 개념의 컨테이너, 실제 파일을 표현하는 오브젝트로 구성\nSwift는 어카운트가 컨테이너를 포함하고, 컨테이너가 오브젝트를 포함하도록 관리\n기본적으로 Swift에서는 프록시 노드 한 대에 스토리지 노드 다섯 대를 구성하기를 권장\n프록시 노드들은 로드밸런서로 묶여 있어 사용자는 특정 URL 하나만 호출해도 스토리지 서비스를 자유롭게 사용가능\n파일을 올릴 때는 설정된 리플리카로 여러 스토리지 노드로 분산해서 저장, 다운로드 시 그 중 한 곳을 사용\n# # Swift와 Keystone # # Swift에는 SwAuth를 이용하는 인증 방법과 Keystone을 이용\n최근에는 Keystone을 이용해서 주로 인증하며, Keystone에는 프로젝트, 사용자, 롤이 있음\n관리자(admin, swiftoperator)는 사용자와 컨테이너를 생성, 삭제할 수 있음\n관리자는 오브젝트도 올리기, 내려받기, 삭제를 할 수 있음\n일반 사용자(member)은 사용자와 컨테이너를 생성할 수 없음\n일반 사용자는 관리자가 미리 생성해서 권한을 준 컨테이너만 사용할 수 있음\n일반 사용자는 관리자가 설정한 권한으로 오브젝트 목록을 확인할 수 있음\n일반 사용자는 관리자가 설장한 권한으로 데이터를 올리고 내릴 수 있음\n특정 사용자에게 관리자(admin) 권한을 부여하려면 리셀러어드민(ResellerAdmin) 롤을 주어야 함\n해당 사용자는 관리자가 할 수 있는 기능을 모두 사용할 수 있음\n# # Swift의 이레이저 코딩(Eraure Coding) 기능과 스토리지 정책 # # 스토리지 저장 공간을 효율적으로 관리하는 것이 이레이져 코딩\n다양한 물리 스토리지 디바이스를 정책별로 사용할 수 있게 지원하는 스토리지 정책(Storage Policy)기능이 있음\n이레이져 코딩은 HDFS, RAID 외의 스토리지에서 데이터 저장 공간의 효율성을 높이려고 설계된 데이터 복제 방식\n이레이져 코딩은 이레이져 코드를 사용해 데이터를 인코딩하고, 데이터가 손실되면 디코딩 과정을 거쳐 원본 데이터를 복구하는 기법 중 하나\n이레이저 코드와 각 코드마다 사용하는 알고리즘은 다양한데 이런 알고리즘에 Reed-Solom on Code, Tahoe-Lafs, Weaver Code 등이 있음\n스토리지 정책은 여러 오브젝트링을 생성해 다양한 목적으로 클러스터를 세그먼트화 할 수 있음\n수정된 해시링을 통해 클러스터에서 데이터가 있어야 할 위치를 결정\n이레이저 코드를 사용해 콜드 스토리지(Cold Storage: 저전력 스토리지)도 정의 할 수 있음\n# "},{"id":9,"href":"/erp/docs/OpenStack/OpenStack/Heat/","title":"Heat","section":"OpenStack docs","content":" 오케스트레이션 서비스 : Heat # # Heat # # Heat는 탬블릿과 Stack을 사용하여 자동으로 인스턴스의 리소스를 추가하거나 줄이는 서비스 오케스트레이션은 자원 관리, 배치, 정렬을 자동화하는 것 오케스트레이션은 인스턴스 생성에 대한 일련의 과정을 자동화해서 인프라를 쉽게 배포할 수 있도록 하는 탬플릿 기반 엔진 오케스트레이션에서 사용되는 템플릿 언어는 인프라, 서비스, 응용프로그램, 프로비저닝, 자동화 컴퓨팅, 스토리지, 네트워킹, 자동 스케일링 등에 사용 가능 # Heat의 논리 아키텍처 # # # 구성요소 역할 heat-api RPC heat 엔진에 전송해서 요청된 API를 처리한 REST API를 제공 heat-api-cfn AWS CloudFormation과 호환되는 AWS 타입의 Query API를 제공 heat-engine 템플릿을 생성하고, Consumer(API를 사용하려고 접근하는 애플리케이션이나 서비스)를 다시 이벤트로 제공하는 오케스트레이션의 주 작업을 수행 queue 각 서비스들이 통신을 하기 위한 서비스 # "},{"id":10,"href":"/erp/docs/OpenStack/OpenStack/","title":"OpenStack docs","section":"SAP","content":" OpenStack\nOpenStack의 개요\nKeyston : 인증을 관리하는 서비스\nGlance : 이미지를 관리하는 서비스\nNova : 가상의 서버를 생성하는 서비스\nNeutron : 네트워크를 관리하는 서비스\nCinder : 블록 스토리지 서비스\nCeilometer : 리소스의 사용량과 부하를 관리하는 서비스\nHorizon : 외부 인터페이스 대시보드 서비스\nSwift : 오브젝트 스토리지 관리 서비스\nHeat : 오케트스트레션 서비스\nTrove : 데이터베이스 서비스\nSahara : 데이터 프로세싱 서비스\nIronic : 베어메탈 서비스\n# # OpenStack Training\nOpenStack Ussuri\nOpenStack Ussuri : Overview\nOpenStack Ussuri : 환경설정\nOpenStack Ussuri : Keystone\nOpenStack Ussuri : Glance\nOpenStack Ussuri : Nova\nOpenStack Ussuri : Neutron\nOpenStack Ussuri : Cinder\nOpenStack Ussuri : Horizon\nOpenStack Ussuri : Swift\nOpenStack Ussuri : Heat\nOpenStack Ussuri : Gnocch\nOpenStack Ussuri : Trove\nOpenStack Ussuri : Designate\nOpenStack Ussuri : Brabican\nOpenStack Ussuri : Rally\nOpenStack Ussuri : Manila\n# OpenStack Stain # DevStack # PackStack # # "},{"id":11,"href":"/erp/docs/OpenStack/OpenStackTraining/","title":"OpenStack Training","section":"SAP","content":" OpenStack Training\nOpenStack Ussuri\nOpenStack Ussuri : Overview\nOpenStack Ussuri : 환경설정\nOpenStack Ussuri : Keystone\nOpenStack Ussuri : Glance\nOpenStack Ussuri : Nova\nOpenStack Ussuri : Neutron\nOpenStack Ussuri : Cinder\nOpenStack Ussuri : Horizon\nOpenStack Ussuri : Swift\nOpenStack Ussuri : Heat\nOpenStack Ussuri : Gnocch\nOpenStack Ussuri : Trove\nOpenStack Ussuri : Designate\nOpenStack Ussuri : Brabican\nOpenStack Ussuri : Rally\nOpenStack Ussuri : Manila\n# OpenStack Stain # DevStack # PackStack # # OpenStack\nOpenStack의 개요\nKeyston : 인증을 관리하는 서비스\nGlance : 이미지를 관리하는 서비스\nNova : 가상의 서버를 생성하는 서비스\nNeutron : 네트워크를 관리하는 서비스\nCinder : 블록 스토리지 서비스\nCeilometer : 리소스의 사용량과 부하를 관리하는 서비스\nHorizon : 외부 인터페이스 대시보드 서비스\nSwift : 오브젝트 스토리지 관리 서비스\nHeat : 오케트스트레션 서비스\nTrove : 데이터베이스 서비스\nSahara : 데이터 프로세싱 서비스\nIronic : 베어메탈 서비스\n# # "},{"id":12,"href":"/erp/docs/OpenStack/OpenStack/Trove/","title":"Trove","section":"OpenStack docs","content":" 데이터베이스 서비스 : Trove # Trove는 관계형 데이터베이스 기능을 활용 클라우드 사용자와 데이터 베이스 관리자는 필요에 따라 Trove를 통해 데이터베이스 인스턴스를 제공, 관리 서비스 # Trove의 논리 아키텍처 # # # 구성요소 역할 python-troveclient 클라이언트에서 콘솔로 trove-api를 실행할 수 있게 지원 trove-api RESTful API 방식의 JSON을 지원, Trove인스턴스를 관리하고 프로비저닝 trove-taskmanager 인스턴스 프로비저닝을 지원, 라이프 사이클 관리 및 운영하는 작업을 수행 trove-conductor 호스트에서 실행되는 서비스로 호스트 정보를 업데이트 및 게스트 인스턴스 메시지를 수신 trove-guestagent 게스트 인스턴스 안에서 실행, 데이터 베이스 작업을 실행, 관리 "},{"id":13,"href":"/erp/docs/OpenStack/OpenStackTraining/OpenStack-Ussuri-01/","title":"OpenStack Ussuri : Overview","section":"OpenStack Training","content":" OpenStack Ussuri : Overview # # OpenStack Ussuri : Overview # # # OpenStack Ussuri 설치는 위의 그림과 표에 맞춰 설치가 진행됩니다. minimal 기본 설치는 keystone, glance, nova, neutron, cinder, horizon이며 여기서는 가능한 모든 서비스를 설치하도록 하겠습니다. # OS HOST NAME CPU/thead RAM DISK Network Interface-1 Network Interface-2 CentOS8 controller 4/8 6144 100G Nat host1 CentOS8 network 2/4 2048 40G Nat host1 CentOS8 compute 4/8 4096 40G host1 CentOS8 storage1 1/2 1024 50G host1 CentOS8 storage2 1/2 1024 50G host1 CentOS8 storage3 1/2 1024 50G host1 # Service Code Name Description Identity Service Keystone User Management Compute Service Nova Virtual Machine Management Image Service Glance Manages Virtual image like kernel image or disk image Dashboard Horizon Provides GUI console via Web browser Object Storage Swift Provides Cloud Storage Block Storage Cinder Storage Management for Virtual Machine Network Service Neutron Virtual Networking Management Orchestration Service Heat Provides Orchestration function for Virtual Machine Metering Service Ceilometer Provides the function of Usage measurement for accounting Database Service Trove Database resource Management Data Processing Service Sahara Provides Data Processing function Bare Metal Provisioning Ironic Provides Bare Metal Provisioning function Messaging Service Zaqar Provides Messaging Service function Shared File System Manila Provides File Sharing Service DNS Service Designate Provides DNS Server Service Key Manager Service Barbican Provides Key Management Service # "},{"id":14,"href":"/erp/docs/OpenStack/OpenStack/Sahara/","title":"Sahara","section":"OpenStack docs","content":" Sahara # # 데이터 프로세싱 서비스 Sahara # 오픈스택 위 빅데이터를 다루기 위한 Hadoop이나 Spark를 쉽게 제공할 수 있게 도와주는 서비스 Sahara는 다음 요소로 구성 Auth: 클라이언트 인증 및 권한을 부여, 오픈스택 인증 서비스 Keystone과 통신 DAL: Data Access Layer의 약어로 데이터 엑세스 계층을 의미, DB의 내부 모델을 유지 Secure Storage Access Layer: 암호 및 개인 키 같은 인증 데이터를 안전한 저장소에 보관 Provisioning Engine: 오픈스택 컴퓨트 서비스 Nova, Heat, Cinder, Glance, Designate와 통신을 담당하는 구성 요소 Vendor Plugins: 프로비저닝된 VM에서 데이터 처리 프레임워크를 구성하고 시작하는 기능을 담당하는 플러그 가능한 메커니즘 EDP: Elastic Data Processing의 약어로 Sahara가 제공하는 클러스테에서 데이터 처리 작업을 예약하고 관리 REST API: REST HTTP 인터페이스로 Sahara 기능을 호출 Python Sahara Client: 다른 오픈스택 구성 요소와 마찬가지로 Sahara에는 자체 Python 클라이언트가 있음 Sahara Pages: Sahara용 GUI로 오픈스택 대시보드인 Horizon에 있음 "},{"id":15,"href":"/erp/docs/OpenStack/OpenStackTraining/OpenStack-Ussuri-02/","title":"OpenStack Ussuri : 환경설정","section":"OpenStack Training","content":" OpenStack Ussuri : 기본 환경설정 # # ----------------------- | [ Controller Node ] | | | | MariaDB RabbitMQ | | Memcached | ----------------------- # OpenStack Ussuri : 기본 환경설정 # # 앞 기본 환경설정을 모든 노드에서 진행한 후, DB, RabbitMQ, Memcached는 controller에서만 설치를 진행합니다. $ all\u0026gt; $ controller\u0026gt; $ controller ~(keystone)\u0026gt; $ compute\u0026gt; $ network\u0026gt; # 위와 같은 호스트를 주의헤 주세요 ! # (keystone)은 keystone 설치 후 인증 받은 터미널입니다. # Ussuri repository 등록 # # OpenStack 구현을 위해 Ussuri repository를 구현합니다. $ all\u0026gt; dnf -y install centos-release-openstack-ussuri $ all\u0026gt; sed -i -e \u0026#34;s/enabled=1/enabled=0/g\u0026#34; /etc/yum.repos.d/CentOS-OpenStack-ussuri.repo $ all\u0026gt; dnf --enablerepo=centos-openstack-ussuri -y upgrade # # NTP ( Network Time Protocol ) Server 설치 # # NTP Server는 모든 Node에서 설정을 진행합니다. $ all\u0026gt; dnf --enablerepo=centos-openstack-ussuri -y install openstack-selinux # SELinux를 설치합니다. $ all\u0026gt; dnf install -y wget # wget을 설치합니다. $ all\u0026gt; dnf --enablerepo=powertools -y install epel-release # epel 레포지터리를 등록합니다. $ all\u0026gt; dnf --enablerepo=powertools -y install checkpolicy # 만약 Checkpolicy가 설치되어 있지 않으면, 패키지를 다운 받습니다. $ all\u0026gt; dnf -y install chrony $ all\u0026gt; vi /etc/chrony.conf # pool 2.centos.pool.ntp.org iburst pool ntp.nict.jp iburst allow 10.10.10.0/24 $ all\u0026gt; systemctl enable --now chronyd # chrony 파일을 수정합니다. allow에는 사용대역을 기입합니다. $ all\u0026gt; firewall-cmd --add-service=ntp --permanent $ all\u0026gt; firewall-cmd --reload $ all\u0026gt; init 6 $ all\u0026gt; chronyc sources ^+ ntp-k1.nict.jp 1 6 17 10 -588us[-2093us] +/- 28ms ^+ ntp-a3.nict.go.jp 1 6 17 10 -2468us[-3973us] +/- 30ms ^* ntp-b3.nict.go.jp 1 6 17 10 +1015us[ -490us] +/- 22ms ^- ntp-b2.nict.go.jp 1 6 17 10 +2720us[+2720us] +/- 22ms # 방화벽을 등록 후 확인합니다. # # Controller MariaDB 설치 # # $ controller\u0026gt; dnf module -y install mariadb:10.3 $ controller\u0026gt; vi /etc/my.cnf.d/charaset.cnf [mysqld] character-set-server = utf8mb4 [client] default-character-set = utf8mb4 # mariadb를 설치 후, charaset 설정을 변경하기 위해 파일을 수정합니다. $ controller\u0026gt; systemctl restart --now mariadb $ controller\u0026gt; systemctl enable --now mariadb # DB를 재시작 합니다. $ controller\u0026gt; firewall-cmd --add-service=mysql --permanent $ controller\u0026gt; firewall-cmd --reload # 방화벽을 설정합니다. $ controller\u0026gt; mysql_secure_installation $ controller\u0026gt; mysql -u root -p # 설정을 초기화 후, 비빌번호를 생성합니다. # # RabbitMQ, Memcached 설치 # # RabbitMQ는 오픈 소스 메시지 브로커 소프트웨어이며, AMQP를 구현합니다. RabbitMQ는 OpenStack에서는 서로간의 통신을 위해 사용됩니다. Memcached이란 Memcached 는 범용 분산 캐시 시스템로, OpenStack에서 캐시값을 관리합니다. RabbitMq, Memcached는 Controller에서만 설치를 진행합니다. $ controller\u0026gt; dnf --enablerepo=powertools -y install rabbitmq-server memcached $ controller\u0026gt; vi /etc/my.cnf.d/mariadb-server.cnf [mysqld] ..... ..... max_connections=500 # 인증허용 시간 값을 추가합니다. $ controller\u0026gt; vi /etc/sysconfig/memcached OPTIONS=\u0026#34;-l 0.0.0.0,::\u0026#34; # 모두가 사용할 수 있도록 값을 수정합니다. $ controller\u0026gt; systemctl restart mariadb rabbitmq-server memcached $ controller\u0026gt; systemctl enable mariadb rabbitmq-server memcached # RabbitMQ, Memcached 서비스를 등록합니다. $ controller\u0026gt; rabbitmqctl add_user openstack qwer1234 $ controller\u0026gt; rabbitmqctl set_permissions openstack \u0026#34;.*\u0026#34; \u0026#34;.*\u0026#34; \u0026#34;.*\u0026#34; # rabbitmq를 사용할 openstack 유저를 패스워드 qwer1234로 생성하고 모든 권한을 줍니다. $ controller\u0026gt; vi rabbitmqctl.te module rabbitmqctl 1.0; require { type rabbitmq_t; type rabbitmq_var_log_t; type rabbitmq_var_lib_t; type etc_t; type init_t; class file write; class file getattr; } #============= rabbitmq_t ============== allow rabbitmq_t etc_t:file write; #============= init_t ================== allow init_t rabbitmq_var_lib_t:file getattr; allow init_t rabbitmq_var_log_t:file getattr; $ controller\u0026gt; checkmodule -m -M -o rabbitmqctl.mod rabbitmqctl.te $ controller\u0026gt; semodule_package --outfile rabbitmqctl.pp --module rabbitmqctl.mod $ controller\u0026gt; semodule -i rabbitmqctl.pp # rabbitmq의 설정을 추가 후 등록시킵니다. $ controller\u0026gt; firewall-cmd --add-service=memcache --permanent $ controller\u0026gt; firewall-cmd --add-port=5672/tcp --permanent $ controller\u0026gt; firewall-cmd --reload # 방화벽을 설정합니다. "},{"id":16,"href":"/erp/docs/OpenStack/OpenStack/Ironic/","title":"Ironic","section":"OpenStack docs","content":" Ironic # # 베어메탈 서비스 Ironic # 물리적인 컴퓨터를 관리하고 자원을 제공하는 구성요소의 모음\nIronic은 구성에 따라 다음과 같은 다른 여러 오픈스택 서비스와 상호 작용할 수 있음\nIPMI 메트릭을 사용하는 오픈스택 텔레미터 모듈(Ceilometer)\n인증 요청 및 다른 오픈스택 서비스를 인증하는 오픈스택 인증 서비스(Keystone)\n이미지 및 이미지 메타데이터를 검색할 수 있는 오픈스택 이미지 서비스(Glance)\nDHCP 및 네트워크를 구성하는 오픈스택 네트워크 서비스(Neutron)\n오픈스택 네트워크 서비스인 Nova는 베어메탈 서비스와 함꼐 작동하고, 인스턴스를 관리하는 사용자용 API를 제공\n오픈스택 컴퓨트 서비스는 베어메탈 서비스가 제공하지 않는 예약 기능, 테넌트 할당량, IP 할당, 기타 서비스를 제공\n오픈스택 오브젝트 스토리지 서비스인 Swift는 드라이브 설정, 사용자 이미지, 배포 로그 및 점검 데이터 임시 저장 장소를 제공\nIronic은 다음 요소로 구성\nironic-api: 응용프로그램 요청을 원격 프로시저 호출(RPC)을 이용해서 ironic-conductor로 전송한 후 응용프로그램 요청을 처리하는 RESTful API\nironic-conductor: 노드를 추가, 편집, 삭제하며 IPMI 또는 SSH를 사용해 노드를 켜고 끌수 있음, 베어메탈 노드를 프로비저닝, 배치 정리 수행\nironic-python-agent: 원격 엣세스, 하드웨어 제어, 하드웨어 기본 스펙으로 ironic-conductor 및 ironic-inspector 서비스를 제공하려고 임시 RAM 디스크에서 실행되는 python 서비스\n"},{"id":17,"href":"/erp/docs/OpenStack/OpenStackTraining/OpenStack-Ussuri-03/","title":"OpenStack Ussuri : Keystone","section":"OpenStack Training","content":" OpenStack Ussuri : Keystone # # ----------------------- | [ Controller Node ] | | | | MariaDB RabbitMQ | | Memcached Keystone | | httpd | ----------------------- # OpenStack Ussuri : Keystone # # Keystone은 OpenStack에서 인증 서비스를 구성하고 있습니다. Keystone에 대한 자세한 설명은 Keystone을 참조해주세요. # Keystone 유저와 DB를 생성합니다. # # $ controller\u0026gt; mysql -u root -p $ MariaDB\u0026gt; create database keystone; $ MariaDB\u0026gt; grant all privileges on keystone.* to keystone@\u0026#39;localhost\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; grant all privileges on keystone.* to keystone@\u0026#39;%\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; flush privileges; $ MariaDB\u0026gt; exit; # # Keystone을 설치합니다. # # $ controller\u0026gt; dnf --enablerepo=centos-openstack-ussuri,epel,powertools -y install openstack-keystone python3-openstackclient httpd mod_ssl python3-mod_wsgi python3-oauth2client # keystone 및 관련 모듈을 설치합니다. $ controller\u0026gt; vi /etc/keystone/keystone.conf [cache] memcache_servers = controller:11211 [database] connection = mysql+pymysql://keystone:qwer1234@controller/keystone [token] provider = fernet $ controller\u0026gt; su -s /bin/bash keystone -c \u0026#34;keystone-manage db_sync\u0026#34; $ controller\u0026gt; keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone $ controller\u0026gt; keystone-manage credential_setup --keystone-user keystone --keystone-group keystone # Keystone DB를 임포트 시킵니다. $ controller\u0026gt; keystone-manage bootstrap --bootstrap-password qwer1234 \\ --bootstrap-admin-url http://controller:5000/v3/ \\ --bootstrap-internal-url http://controller:5000/v3/ \\ --bootstrap-public-url http://controller:5000/v3/ \\ --bootstrap-region-id RegionOne $ controller\u0026gt; setsebool -P httpd_use_openstack on $ controller\u0026gt; setsebool -P httpd_can_network_connect on $ controller\u0026gt; setsebool -P httpd_can_network_connect_db on $ controller\u0026gt; vi keystone-httpd.te module keystone-httpd 1.0; require { type httpd_t; type keystone_log_t; class file create; class dir { add_name write }; } #============= httpd_t ============== allow httpd_t keystone_log_t:dir { add_name write }; allow httpd_t keystone_log_t:file create; $ controller\u0026gt; checkmodule -m -M -o keystone-httpd.mod keystone-httpd.te $ controller\u0026gt; semodule_package --outfile keystone-httpd.pp --module keystone-httpd.mod $ controller\u0026gt; semodule -i keystone-httpd.pp $ controller\u0026gt; firewall-cmd --add-port=5000/tcp --permanent $ controller\u0026gt; firewall-cmd --reload # 방화벽 및 SELinux를 설정합니다. $ controller\u0026gt; vi /etc/httpd/conf/httpd.conf ServerName controller:80 # 99번 줄에 추가합니다. $ controller\u0026gt; ln -s /usr/share/keystone/wsgi-keystone.conf /etc/httpd/conf.d/ $ controller\u0026gt; systemctl enable --now httpd # httpd 서비스를 등록합니다. # # Keystone Project 생성 # $ controller\u0026gt; vi ~/admin_key export OS_PROJECT_DOMAIN_NAME=default export OS_USER_DOMAIN_NAME=default export OS_PROJECT_NAME=admin export OS_USERNAME=admin export OS_PASSWORD=qwer1234 export OS_AUTH_URL=http://controller:5000/v3 export OS_IDENTITY_API_VERSION=3 export OS_IMAGE_API_VERSION=2 export PS1=\u0026#39;[\\u@\\h \\W~(keystone)]\\$ \u0026#39; $ controller\u0026gt; chmod 600 ~/admin_key $ controller\u0026gt; source ~/admin_key $ controller\u0026gt; echo \u0026#34;source ~/admin_key \u0026#34; \u0026gt;\u0026gt; ~/.bash_profile # keystone 인증파일 생성 후 시작시 등록되게 등록시킵니다. $ controller ~(keystone)\u0026gt; openstack project create --domain default --description \u0026#34;Service Project\u0026#34; service +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | Service Project | | domain_id | default | | enabled | True | | id | 7c10c02365be496fb47f12bfd40fe4a7 | | is_domain | False | | name | service | | options | {} | | parent_id | default | | tags | [] | +-------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack project list +----------------------------------+---------+ | ID | Name | +----------------------------------+---------+ | 7c10c02365be496fb47f12bfd40fe4a7 | service | | c76211c24a1f460ca67274d655d46725 | admin | +----------------------------------+---------+ # "},{"id":18,"href":"/erp/docs/OpenStack/OpenStackTraining/OpenStack-Ussuri-04/","title":"OpenStack Ussuri : Glance","section":"OpenStack Training","content":" OpenStack Ussuri : Glance # # ----------------------- | [ Controller Node ] | | | | MariaDB RabbitMQ | | Memcached Keystone | | httpd Glance | ----------------------- # OpenStack Ussuri : Glance # # Glance는 OpenStack에서 이미지 생성에 필요한 Iamge 관리 서비스를 구성하고 있습니다. Glance에 자세한 설명은 Glance를 참조해주세요. # Glance service 및 User 생성 # $ contoller ~(keystone)\u0026gt; openstack user create --domain default --project service --password qwer1234 glance +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | default_project_id | 7c10c02365be496fb47f12bfd40fe4a7 | | domain_id | default | | enabled | True | | id | 03f5b16a7be84cb688617d1943c8fe8c | | name | glance | | options | {} | | password_expires_at | None | +---------------------+----------------------------------+ $ contoller ~(keystone)\u0026gt; openstack role add --project service --user glance admin $ contoller ~(keystone)\u0026gt; openstack service create --name glance --description \u0026#34;OpenStack Image service\u0026#34; image +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | OpenStack Image service | | enabled | True | | id | af365771c17a4a25ae1d0c659e2dc0eb | | name | glance | | type | image | +-------------+----------------------------------+ $ contoller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne image public http://controller:9292 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | cc65faecd7b042ffafd0f262cd7547df | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | af365771c17a4a25ae1d0c659e2dc0eb | | service_name | glance | | service_type | image | | url | http://controller:9292 | +--------------+----------------------------------+ $ contoller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne image internal http://controller:9292 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | ea41c7b17c844e658ac83c547eddcf6d | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | af365771c17a4a25ae1d0c659e2dc0eb | | service_name | glance | | service_type | image | | url | http://controller:9292 | +--------------+----------------------------------+ $ contoller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne image admin http://controller:9292 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 1393a64ef0ec428ba437602ac5b390f6 | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | af365771c17a4a25ae1d0c659e2dc0eb | | service_name | glance | | service_type | image | | url | http://controller:9292 | +--------------+----------------------------------+ # # Glance 유저의 DB를 생성합니다. # # $ controller\u0026gt; mysql -u root -p $ MariaDB\u0026gt; create database glance; $ MariaDB\u0026gt; grant all privileges on glance.* to glance@\u0026#39;localhost\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; grant all privileges on glance.* to glance@\u0026#39;%\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; flush privileges; $ MariaDB\u0026gt; exit; # # Glance 설치 # $ controller\u0026gt; dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install openstack-glance # Glacne 및 관련 모듈을 설치합니다. $ controller\u0026gt; vi /etc/glance/glance-api.conf [DEFAULT] bind_host = 0.0.0.0 [glance_store] stores = file,http default_store = file filesystem_store_datadir = /var/lib/glance/images/ [database] connection = mysql+pymysql://glance:qwer1234@controller/glance [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = glance password = qwer1234 [paste_deploy] flavor = keystone # glacne 파일을 수정합니다. $ controller\u0026gt; su -s /bin/bash glance -c \u0026#34;glance-manage db_sync\u0026#34; $ controller\u0026gt; systemctl enable --now openstack-glance-api # Glance DB를 임포트 시킨후 서비스를 등록합니다. # $ controller\u0026gt; setsebool -P glance_api_can_network on $ controller\u0026gt; vi glanceapi.te module glanceapi 1.0; require { type glance_api_t; type httpd_config_t; type iscsid_exec_t; class dir search; class file { getattr open read }; } #============= glance_api_t ============== allow glance_api_t httpd_config_t:dir search; allow glance_api_t iscsid_exec_t:file { getattr open read }; $ controller\u0026gt; checkmodule -m -M -o glanceapi.mod glanceapi.te $ controller\u0026gt; semodule_package --outfile glanceapi.pp --module glanceapi.mod $ controller\u0026gt; semodule -i glanceapi.pp $ controller\u0026gt; firewall-cmd --add-port=9292/tcp --permanent $ controller\u0026gt; firewall-cmd --reload # # Glance Image 생성 # # $ controller ~(keystone)\u0026gt; mkdir -p /var/kvm/images $ controller ~(keystone)\u0026gt; wget http://download.cirros-cloud.net/0.5.1/cirros-0.5.1-x86_64-disk.img # 이미지를 다운받습니다. $ controller ~(keystone)\u0026gt; openstack image create \u0026#34;cirros\u0026#34; --file cirros-0.5.1-x86_64-disk.img --disk-format qcow2 +------------------+------------------------------------------------------------ -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -+ | Field | Value | +------------------+------------------------------------------------------------ -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -+ | checksum | 1d3062cd89af34e419f7100277f38b2b | | container_format | bare | | created_at | 2020-08-06T11:08:39Z | | disk_format | qcow2 | | file | /v2/images/dc7c2474-8ec9-4f74-a1c3-7cf6a9ad3d16/file | | id | dc7c2474-8ec9-4f74-a1c3-7cf6a9ad3d16 | | min_disk | 0 | | min_ram | 0 | | name | Cirros | | owner | c76211c24a1f460ca67274d655d46725 | | properties | os_hash_algo=\u0026#39;sha512\u0026#39;, os_hash_value=\u0026#39;553d220ed58cfee7dafe0 03c446a9f197ab5edf8ffc09396c74187cf83873c877e7ae041cb80f3b91489acf687183adcd689b 53b38e3ddd22e627e7f98a09c46\u0026#39;, os_hidden=\u0026#39;False\u0026#39;, owner_specified.openstack.md5=\u0026#39; 1d3062cd89af34e419f7100277f38b2b\u0026#39;, owner_specified.openstack.object=\u0026#39;images/Cirr os\u0026#39;, owner_specified.openstack.sha256=\u0026#39;c4110030e2edf06db87f5b6e4efc27300977683d5 3f040996d15dcc0ad49bb5a\u0026#39;, self=\u0026#39;/v2/images/dc7c2474-8ec9-4f74-a1c3-7cf6a9ad3d16\u0026#39; | | protected | False | | schema | /v2/schemas/image | | size | 16338944 | | status | active | | tags | | | updated_at | 2020-08-06T11:08:39Z | | visibility | shared | +------------------+------------------------------------------------------------ -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -+ # 이미지를 등록합니다. $ controller ~(keystone)\u0026gt; openstack image list +--------------------------------------+--------+--------+ | ID | Name | Status | +--------------------------------------+--------+--------+ | dc7c2474-8ec9-4f74-a1c3-7cf6a9ad3d16 | Cirros | active | +--------------------------------------+--------+--------+ # 이미지를 확인합니다. # "},{"id":19,"href":"/erp/docs/OpenStack/OpenStackTraining/OpenStack-Ussuri-05/","title":"OpenStack Ussuri : Nova","section":"OpenStack Training","content":" OpenStack Ussuri : Nova # # ----------------------- ----------------------- | [ Controller Node ] | | [ Compute Node ] | | | | Libvirt | | MariaDB RabbitMQ | | Nova-compute | | Memcached Keystone | | Open vSwitch | | httpd nova | | L2 Agent | | Nova-API | ----------------------- ----------------------- # OpenStack Ussuri : Nova # # Nova는 OpenStack에서 인스턴스를 생성하는 서비스입니다. Nova에 대한 자세한 설명은 Nova를 참조해주세요. # # Nova, ceilometer service 및 User 생성 # # $ controller ~(keystone)\u0026gt; openstack user create --domain default --project service --password qwer1234 nova +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | default_project_id | 7c10c02365be496fb47f12bfd40fe4a7 | | domain_id | default | | enabled | True | | id | f26027517d5e4b5b984b5db8d42398c8 | | name | nova | | options | {} | | qwer1234_expires_at | None | +---------------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack user create --domain default --project service --password qwer1234 placement +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | default_project_id | 7c10c02365be496fb47f12bfd40fe4a7 | | domain_id | default | | enabled | True | | id | 2394500b4512456f9d9d5066a5ecb1f7 | | name | placement | | options | {} | | qwer1234_expires_at | None | +---------------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack role add --project service --user nova admin $ controller ~(keystone)\u0026gt; openstack role add --project service --user placement admin $ controller ~(keystone)\u0026gt; openstack service create --name nova --description \u0026#34;OpenStack Compute service\u0026#34; compute +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | OpenStack Compute service | | enabled | True | | id | 28d495eca718439f9dc6ce395e0720dc | | name | nova | | type | compute | +-------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack service create --name placement --description \u0026#34;OpenStack Compute Placement service\u0026#34; placement +-------------+-------------------------------------+ | Field | Value | +-------------+-------------------------------------+ | description | OpenStack Compute Placement service | | enabled | True | | id | 8515d3d046834de9b71b2938aae89898 | | name | placement | | type | placement | +-------------+-------------------------------------+ $ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne compute public http://controller:8774/v2.1/%\\(tenant_id\\)s --------------+-------------------------------------------+ | Field | Value | +--------------+-------------------------------------------+ | enabled | True | | id | f13ca97a20eb46a3a1c1dfab546a00cc | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | 28d495eca718439f9dc6ce395e0720dc | | service_name | nova | | service_type | compute | | url | http://controller:8774/v2.1/%(tenant_id)s | +--------------+-------------------------------------------+ $ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne compute internal http://controller:8774/v2.1/%\\(tenant_id\\)s +--------------+-------------------------------------------+ | Field | Value | +--------------+-------------------------------------------+ | enabled | True | | id | 1bc41c829f2f47e7962cba46f0da8ddc | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | 28d495eca718439f9dc6ce395e0720dc | | service_name | nova | | service_type | compute | | url | http://controller:8774/v2.1/%(tenant_id)s | +--------------+-------------------------------------------+ $ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne compute admin http://controller:8774/v2.1/%\\(tenant_id\\)s +--------------+-------------------------------------------+ | Field | Value | +--------------+-------------------------------------------+ | enabled | True | | id | 8022a415f22c400c92989320a2be3133 | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | 28d495eca718439f9dc6ce395e0720dc | | service_name | nova | | service_type | compute | | url | http://controller:8774/v2.1/%(tenant_id)s | +--------------+-------------------------------------------+ $ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne placement public http://controller:8778 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 5e988f2be72242f0b3923e27e9db009c | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | 8515d3d046834de9b71b2938aae89898 | | service_name | placement | | service_type | placement | | url | http://controller:8778 | +--------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne placement internal http://controller:8778 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | a68cf8b6eeb043c2aa1ec95d7711cb50 | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | 8515d3d046834de9b71b2938aae89898 | | service_name | placement | | service_type | placement | | url | http://controller:8778 | +--------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne placement admin http://controller:8778 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 63e47fcbfd7841dd95bb4d9d9a910ab5 | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | 8515d3d046834de9b71b2938aae89898 | | service_name | placement | | service_type | placement | | url | http://controller:8778 | +--------------+----------------------------------+ # # Nova 유저의 DB를 생성합니다. # # $ controller\u0026gt; mysql -u root -p $ MariaDB\u0026gt; create database nova; $ MariaDB\u0026gt; grant all privileges on nova.* to nova@\u0026#39;localhost\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; grant all privileges on nova.* to nova@\u0026#39;%\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; create database nova_api; $ MariaDB\u0026gt; grant all privileges on nova_api.* to nova@\u0026#39;localhost\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; grant all privileges on nova_api.* to nova@\u0026#39;%\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; create database nova_cell0; $ MariaDB\u0026gt; grant all privileges on nova_cell0.* to nova@\u0026#39;localhost\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; grant all privileges on nova_cell0.* to nova@\u0026#39;%\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; create database placement; $ MariaDB\u0026gt; grant all privileges on placement.* to placement@\u0026#39;localhost\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; grant all privileges on placement.* to placement@\u0026#39;%\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; flush privileges; $ MariaDB\u0026gt; exit; # # Nova 설치 # # $ controller\u0026gt; dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install openstack-nova openstack-placement-api # nova 및 관련 모듈을 설치합니다. $ controller\u0026gt; vi /etc/nova/nova.conf [DEFAULT] my_ip = 10.10.10.10 # my_ip는 반드시 IP로 적어주세요 ! state_path = /var/lib/nova enabled_apis = osapi_compute,metadata log_dir = /var/log/nova transport_url = rabbit://openstack:qwer1234@controller [api] auth_strategy = keystone [glance] api_servers = http://controller:9292 [vnc] enabled = true server_listen = $my_ip server_proxyclient_address = $my_ip [oslo_concurrency] lock_path = $state_path/tmp [api_database] connection = mysql+pymysql://nova:qwer1234@controller/nova_api [database] connection = mysql+pymysql://nova:qwer1234@controller/nova [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = nova password = qwer1234 [placement] auth_url = http://controller:5000 os_region_name = RegionOne auth_type = password project_domain_name = default user_domain_name = default project_name = service username = placement password = qwer1234 [wsgi] api_paste_config = /etc/nova/api-paste.ini $ controller\u0026gt; vi /etc/placement/placement.conf [DEFAULT] debug = false [api] auth_strategy = keystone [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = placement password = qwer1234 [placement_database] connection = mysql+pymysql://placement:qwer1234@controller/placement $ controller\u0026gt; vi /etc/httpd/conf.d/00-placement-api.conf \u0026lt;Directory /usr/bin\u0026gt; Require all granted \u0026lt;/Directory\u0026gt; # 15번 줄에 추가시킵니다. $ controller\u0026gt; su -s /bin/bash placement -c \u0026#34;placement-manage db sync\u0026#34; $ controller\u0026gt; su -s /bin/bash nova -c \u0026#34;nova-manage api_db sync\u0026#34; $ controller\u0026gt; su -s /bin/bash nova -c \u0026#34;nova-manage cell_v2 map_cell0\u0026#34; $ controller\u0026gt; su -s /bin/bash nova -c \u0026#34;nova-manage db sync\u0026#34; $ controller\u0026gt; su -s /bin/bash nova -c \u0026#34;nova-manage cell_v2 create_cell --name cell1\u0026#34; # nova DB에 임포트 시킵니다. $ controller\u0026gt; semanage port -a -t http_port_t -p tcp 8778 $ controller\u0026gt; firewall-cmd --add-port={6080/tcp,6081/tcp,6082/tcp,8774/tcp,8775/tcp,8778/tcp} --permanent $ controller\u0026gt; firewall-cmd --reload $ controller\u0026gt; systemctl restart httpd $ controller\u0026gt; chown placement. /var/log/placement/placement-api.log $ controller\u0026gt; for service in api conductor scheduler novncproxy; do systemctl enable --now openstack-nova-$service done # Selinux 및 방화벽을 설정합니다. $ controller ~(keystone)\u0026gt; openstack compute service list +----+----------------+------------+----------+---------+-------+----------------------------+ | ID | Binary | Host | Zone | Status | State | Updated At | +----+----------------+------------+----------+---------+-------+----------------------------+ | 4 | nova-conductor | controller | internal | enabled | up | 2020-08-06T12:10:34.000000 | | 5 | nova-scheduler | controller | internal | enabled | up | 2020-08-06T12:10:38.000000 | +----+----------------+------------+----------+---------+-------+----------------------------+ # # Conpute node Nova 설치 # # nova 설차 # # # Nova 서비스를 설치하기 전에 가상화를 위한 KVM + QEMU를 설치합니다. 이를 위해서는 Inter VT나 AMD-V가 필요합니다. ( CPU ) $ lsmod | grep kvm kvm_amd 110592 0 ccp 98304 1 kvm_amd kvm 786432 1 kvm_amd irqbypass 16384 1 kvm # $ compute\u0026gt; dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install qemu-kvm libvirt virt-install libguestfs-tools # KVM 관련 모듈을 설치합니다. $ compute\u0026gt; systemctl enable --now libvirtd # libvirtd 서비스를 등록 및 시작합니다. $ compute\u0026gt; nmcli connection add type bridge autoconnect yes con-name br0 ifname br0 # br0의 가상 브릿지를 추가합니다. $ compute\u0026gt; nmcli connection modify br0 ipv4.addresses 10.10.10.30/24 ipv4.method manual # 가상 브리지의 IP를 추가합니다. ( compute node ip ) $ compute\u0026gt; nmcli connection modify br0 ipv4.gateway 10.10.10.10 # 가상 브리지의 GATEWAY를 등록합니다. $ compute\u0026gt; nmcli connection modify br0 ipv4.dns 8.8.8.8 # 가상 브릿지의 DNS를 등록합니다. $ compute\u0026gt; nmcli connection del ens34 # 본래의 네트워크 인터페이스를 삭제합니다. $ compute\u0026gt; nmcli connection add type bridge-slave autoconnect yes con-name ens34 ifname ens34 master br0 # 삭제한 네트워크 인터페이스 대신 브릿지를 매핑시키고 네트워크를 재시작 시킵니다. # 제 compute node의 내부대역 IP는 10.10.10.30/24 ens34입니다 햇갈리지 마세요 ! $ compute\u0026gt; init 6 $ compute\u0026gt; ipfconfig br0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 10.10.10.30 netmask 255.255.255.0 broadcast 10.10.10.255 inet6 fe80::6765:fe91:a94b:5529 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether 00:0c:29:80:33:15 txqueuelen 1000 (Ethernet) RX packets 465 bytes 56335 (55.0 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 348 bytes 66663 (65.1 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 ens34: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 ether 00:0c:29:80:33:15 txqueuelen 1000 (Ethernet) RX packets 471 bytes 63205 (61.7 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 450 bytes 75797 (74.0 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 # 재시작후 네트워크 인터페이스를 확인하면 위와 같이 생성된 것을 확인할 수 있습니다. # # $ compute\u0026gt; dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install openstack-nova-compute # nova 및 관련 모듈을 설치합니다. $ controller\u0026gt; scp /etc/nova/nova.conf compute:/etc/nova/nova.conf # nova의 기본설정파일을 복사합니다. $ compute\u0026gt; vi /etc/nova/nova.conf [default] my_ip = 10.10.10.30 # my_ip는 반드시 IP로 적어주세요 ! [libvirt] virt_type = qemu [vnc] enabled = True server_listen = 0.0.0.0 server_proxyclient_address = $my_ip novncproxy_base_url = http://controller:6080/vnc_auto.html # nova관련 설정을 추가합니다. $ compute\u0026gt; firewall-cmd --add-port=5900-5999/tcp --permanent $ compute\u0026gt; firewall-cmd --reload $ compute\u0026gt; systemctl enable --now libvirtd $ compute\u0026gt; systemctl enable --now openstack-nova-compute # # Nova 설치 확인 # # $ controller\u0026gt; su -s /bin/bash nova -c \u0026#34;nova-manage cell_v2 discover_hosts\u0026#34; # DB에 compute의 대한 설정을 업데이트 합니다. $ controller\u0026gt; nova-manage cell_v2 discover_hosts --verbose # compute 노드가 검색이 안되었을 시 추가적으로 검색합니다. $ controller ~(keystone)\u0026gt; openstack compute service list +----+----------------+------------+----------+---------+-------+----------------------------+ | ID | Binary | Host | Zone | Status | State | Updated At | +----+----------------+------------+----------+---------+-------+----------------------------+ | 4 | nova-conductor | controller | internal | enabled | up | 2020-08-06T21:40:34.000000 | | 5 | nova-scheduler | controller | internal | enabled | up | 2020-08-06T21:40:37.000000 | | 8 | nova-compute | compute | nova | enabled | up | 2020-08-06T21:40:36.000000 | +----+----------------+------------+----------+---------+-------+----------------------------+ # "},{"id":20,"href":"/erp/docs/OpenStack/OpenStack/Service/","title":"Service","section":"OpenStack docs","content":" **** # # 옵셔널 서비스 # 컴퓨트, 오브젝트 스토리지, 이미지, 인증, 네트워크, 블록 스토리지, 대시보드 서비스만으로도 오픈스택을 구축할 수 있음 텔레미터, 오케스트레이션, 데이터베이스 같은 서비스를 제대로 사용한다면 효율적인 클라우드 관리와 운영에 많은 도움을 많을 수 있음 메시징 서비스 Zaqar 공유 파일 시스템 서비스 Manila DNS 서비스 Designate "},{"id":21,"href":"/erp/docs/OpenStack/OpenStackTraining/OpenStack-Ussuri-06/","title":"OpenStack Ussuri : Neutron","section":"OpenStack Training","content":" OpenStack Ussuri : Neutron # # ----------------------- ----------------------- ----------------------- | [ Controller Node ] | | [ Compute Node ] | | [ Network Node ] | | | | Libvirt | | Open vSwitch | | MariaDB RabbitMQ | | Nova compute | | L2 Agent | | Memcached Keystone | | Open vSwitch | | L3 Agent | | httpd Neutron | | L2 Agent | | metadata agent | | Nova-API Compute | ----------------------- ----------------------- | L2 agent L3 agent | | metadata agent | | Neutron Server | ----------------------- # OpenStack Ussuri : Neutron # # Neutron는 OpenStack에서 네트워크 전반을 관리하는 서비스입니다. Neutron에 대한 자세한 설명은 Neutron를 참조해주세요. # # Neutron service 및 User 생성 # # $ controller ~(keystone)\u0026gt; openstack user create --domain default --project service --password qwer1234 neutron +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | default_project_id | 7c10c02365be496fb47f12bfd40fe4a7 | | domain_id | default | | enabled | True | | id | 943fbb4370164c77ae6bf7fa455292f8 | | name | neutron | | options | {} | | password_expires_at | None | +---------------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack role add --project service --user neutron admin $ controller ~(keystone)\u0026gt; openstack service create --name neutron --description \u0026#34;OpenStack Networking service\u0026#34; network +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | OpenStack Networking service | | enabled | True | | id | 055e5f6e38004338b0ae4a86e77932ae | | name | neutron | | type | network | +-------------+----------------------------------+ # neutron service 및 user을 생성합니다. $ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne network public http://controller:9696 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 350c666f597a41e59234b09f534aa72f | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | 055e5f6e38004338b0ae4a86e77932ae | | service_name | neutron | | service_type | network | | url | http://controller:9696 | +--------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne network internal http://controller:9696 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | b9cad959e1634ff797e27f00d50e9578 | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | 055e5f6e38004338b0ae4a86e77932ae | | service_name | neutron | | service_type | network | | url | http://controller:9696 | +--------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne network admin http://controller:9696 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 72fc145deb1d4d508e3691b3bf77708e | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | 055e5f6e38004338b0ae4a86e77932ae | | service_name | neutron | | service_type | network | | url | http://controller:9696 | +--------------+----------------------------------+ # neutron endpoint를 등록합니다. # # neutron 유저의 DB를 생성합니다. # # $ controller\u0026gt; mysql -u root -p $ MariaDB\u0026gt; create database neutron_ml2; $ MariaDB\u0026gt; grant all privileges on neutron_ml2.* to neutron@\u0026#39;localhost\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; grant all privileges on neutron_ml2.* to neutron@\u0026#39;%\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; flush privileges; $ MariaDB\u0026gt; exit; # # Neutron 설치 # # $ controller\u0026gt; dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install openstack-neutron openstack-neutron-ml2 # neutron 및 관련 모듈을 설치합니다. $ controller\u0026gt; vi /etc/neutron/neutron.conf [DEFAULT] core_plugin = ml2 service_plugins = router auth_strategy = keystone state_path = /var/lib/neutron dhcp_agent_notification = True allow_overlapping_ips = True notify_nova_on_port_status_changes = True notify_nova_on_port_data_changes = True transport_url = rabbit://openstack:qwer1234@controller [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = neutron password = qwer1234 [database] connection = mysql+pymysql://neutron:qwer1234@controller/neutron_ml2 [nova] auth_url = http://controller:5000 auth_type = password project_domain_name = default user_domain_name = default region_name = RegionOne project_name = service username = nova password = qwer1234 [oslo_concurrency] lock_path = $state_path/tmp $ controller\u0026gt; vi /etc/neutron/metadata_agent.ini [DEFAULT] nova_metadata_host = controller metadata_proxy_shared_secret = metadata_secret [cache] memcache_servers = controller:11211 $ controller\u0026gt; vi /etc/neutron/plugins/ml2/ml2_conf.ini [ml2] type_drivers = flat,vlan,gre,vxlan tenant_network_types = vxlan mechanism_drivers = openvswitch extension_drivers = port_security [ml2_type_flat] flat_networks = physnet1 [ml2_type_vxlan] vni_ranges = 1:1000 $ controller\u0026gt; vi /etc/nova/nova.conf [default] ... ... use_neutron = True linuxnet_interface_driver = nova.network.linux_net.LinuxOVSInterfaceDriver firewall_driver = nova.virt.firewall.NoopFirewallDriver [neutron] auth_url = http://controller:5000 auth_type = password project_domain_name = default user_domain_name = default region_name = RegionOne project_name = service username = neutron password = qwer1234 service_metadata_proxy = True metadata_proxy_shared_secret = metadata_secret $ controller\u0026gt; setsebool -P neutron_can_network on $ controller\u0026gt; setsebool -P daemons_enable_cluster_mode on $ controller\u0026gt; firewall-cmd --add-port=9696/tcp --permanent $ controller\u0026gt; firewall-cmd --reload # 방화벽 및 SELinux를 설정합니다. $ controller\u0026gt; ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini $ controller\u0026gt; su -s /bin/bash neutron -c \u0026#34;neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugin.ini upgrade head\u0026#34; $ controller\u0026gt; systemctl enable --now neutron-server neutron-metadata-agent $ controller\u0026gt; systemctl restart openstack-nova-api # neutron DB를 임포트 시킨 후, 서비스를 등록 합니다. # # neutron Network Node 설치 # # Neutron 설치 # # $ network\u0026gt; dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install openstack-neutron openstack-neutron-ml2 openstack-neutron-openvswitch libibverbs # neutron 및 관련 모듈을 설치합니다. $ network\u0026gt; vi /etc/neutron/neutron.conf [DEFAULT] core_plugin = ml2 service_plugins = router auth_strategy = keystone state_path = /var/lib/neutron allow_overlapping_ips = True # RabbitMQ connection info transport_url = rabbit://openstack:qwer1234@controller [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = neutron password = qwer1234 [oslo_concurrency] lock_path = $state_path/lock $ network\u0026gt; vi /etc/neutron/dhcp_agent.ini [DEFAULT] interface_driver = openvswitch dhcp_driver = neutron.agent.linux.dhcp.Dnsmasq enable_isolated_metadata = true $ network\u0026gt; vi /etc/neutron/metadata_agent.ini nova_metadata_host = controller metadata_proxy_shared_secret = metadata_secret [cache] memcache_servers = controller:11211 $ network\u0026gt; vi /etc/neutron/plugins/ml2/ml2_conf.ini [ml2] type_drivers = flat,vlan,gre,vxlan tenant_network_types = vxlan mechanism_drivers = openvswitch extension_drivers = port_security [ml2_type_flat] flat_networks = physnet1 [ml2_type_vxlan] vni_ranges = 1:1000 # 끝에 추가합니다. $ network\u0026gt; vi /etc/neutron/plugins/ml2/openvswitch_agent.ini [securitygroup] firewall_driver = openvswitch enable_security_group = true enable_ipset = true [agent] tunnel_types = vxlan prevent_arp_spoofing = True [ovs] local_ip = 10.10.10.20 bridge_mappings = physnet1:br-eth1 # 끝에 추가합니다. # 여기는 IP 를 반드시 적어야 해요 ! $ network\u0026gt; setsebool -P neutron_can_network on $ network\u0026gt; setsebool -P haproxy_connect_any on $ network\u0026gt; setsebool -P daemons_enable_cluster_mode on $ network\u0026gt; vi ovsofctl.te module ovsofctl 1.0; require { type neutron_t; type neutron_exec_t; type neutron_t; type dnsmasq_t; class file execute_no_trans; class capability { dac_override sys_rawio }; } #============= neutron_t ============== allow neutron_t self:capability { dac_override sys_rawio }; allow neutron_t neutron_exec_t:file execute_no_trans; #============= dnsmasq_t ============== allow dnsmasq_t self:capability dac_override; $ network\u0026gt; checkmodule -m -M -o ovsofctl.mod ovsofctl.te $ network\u0026gt; semodule_package --outfile ovsofctl.pp --module ovsofctl.mod $ network\u0026gt; semodule -i ovsofctl.pp $ network\u0026gt; systemctl disable --now firewalld # Selinux 및 방화벽을 설정합니다. $ network\u0026gt; ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini $ network\u0026gt; systemctl enable --now openvswitch $ network\u0026gt; ovs-vsctl add-br br-int $ network\u0026gt; ovs-vsctl add-br br-eth1 $ network\u0026gt; ovs-vsctl add-port br-eth1 ens32 $ network\u0026gt; vi /etc/sysconfig/network-scripts/ifcfg-ens32 TYPE=Ethernet BOOTPROTO=static NAME=ens32 DEVICE=ens32 ONBOOT=yes $ network\u0026gt; vi /var/tmp/network_interface.sh #!/bin/bash ip link set up br-eth1 ip addr add 192.168.10.20/24 dev br-eth1 route add default gw 192.168.10.2 dev br-eth1 echo \u0026#34;nameserver 8.8.8.8\u0026#34; \u0026gt; /etc/resolv.conf $ network\u0026gt; chmod 755 /var/tmp/network_interface.sh $ network\u0026gt; vi /etc/systemd/system/set_interface.service [Unit] Description=Description for sample script goes here After=network.target [Service] Type=simple ExecStart=/var/tmp/network_interface.sh TimeoutStartSec=0 [Install] WantedBy=default.target $ systemctl enable set_interface $ init 6 # network 인터페이스 주의 !!! ( ex : ens32 ) $ network\u0026gt; for service in dhcp-agent l3-agent metadata-agent openvswitch-agent; do systemctl enable --now neutron-$service done # neutron 서비스를 등록합니다. # neutron compute Node 설치 # Neutron 설치 # # $ compute\u0026gt; dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install openstack-neutron openstack-neutron-ml2 openstack-neutron-openvswitch # neutron 및 관련 모듈을 설치합니다. $ compute\u0026gt; vi /etc/neutron/neutron.conf [DEFAULT] core_plugin = ml2 service_plugins = router auth_strategy = keystone state_path = /var/lib/neutron allow_overlapping_ips = True transport_url = rabbit://openstack:qwer1234@controller [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = neutron password = qwer1234 [oslo_concurrency] lock_path = $state_path/lock $ compute\u0026gt; vi /etc/neutron/plugins/ml2/ml2_conf.ini [ml2] type_drivers = flat,vlan,gre,vxlan tenant_network_types = vxlan mechanism_drivers = openvswitch extension_drivers = port_security [ml2_type_flat] flat_networks = physnet1 [ml2_type_vxlan] vni_ranges = 1:1000 # 끝에 추가합니다. $ compute\u0026gt; vi /etc/neutron/plugins/ml2/openvswitch_agent.ini [securitygroup] firewall_driver = openvswitch enable_security_group = true enable_ipset = true [agent] tunnel_types = vxlan prevent_arp_spoofing = True [ovs] local_ip = 10.10.10.20 # 끝에 추가합니다. # 여기는 반드시 IP로 적어야 해요 ! $ compute\u0026gt; vi /etc/nova/nova.conf [default] ... ... use_neutron = True linuxnet_interface_driver = nova.network.linux_net.LinuxOVSInterfaceDriver firewall_driver = nova.virt.firewall.NoopFirewallDriver vif_plugging_is_fatal = True vif_plugging_timeout = 300 [neutron] auth_url = http://controller:5000 auth_type = password project_domain_name = default user_domain_name = default region_name = RegionOne project_name = service username = neutron password = qwer1234 service_metadata_proxy = True metadata_proxy_shared_secret = metadata_secret $ compute\u0026gt; setsebool -P neutron_can_network on $ compute\u0026gt; setsebool -P daemons_enable_cluster_mode on $ compute\u0026gt; vi ovsofctl.te module ovsofctl 1.0; require { type neutron_t; type neutron_exec_t; type neutron_t; type dnsmasq_t; class file execute_no_trans; class capability { dac_override sys_rawio }; } #============= neutron_t ============== allow neutron_t self:capability { dac_override sys_rawio }; allow neutron_t neutron_exec_t:file execute_no_trans; #============= dnsmasq_t ============== allow dnsmasq_t self:capability dac_override; $ network\u0026gt; checkmodule -m -M -o ovsofctl.mod ovsofctl.te $ network\u0026gt; semodule_package --outfile ovsofctl.pp --module ovsofctl.mod $ network\u0026gt; semodule -i ovsofctl.pp $ systemctl disable --now firewalld # Selinux 및 방화벽을 설정합니다. $ compute\u0026gt; ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini $ compute\u0026gt; systemctl enable --now openvswitch $ compute\u0026gt; ovs-vsctl add-br br-int $ compute\u0026gt; systemctl restart openstack-nova-compute $ compute\u0026gt; systemctl enable --now neutron-openvswitch-agent # neutron 서비스를 등록합니다. # # 확인 # # $ controller ~(keystone)\u0026gt; openstack router create router +-------------------------+----------------------------------------------------- -------------------------------------------------------------------------------- --------------------+ | Field | Value | +-------------------------+----------------------------------------------------- -------------------------------------------------------------------------------- --------------------+ | admin_state_up | UP | | availability_zone_hints | | | availability_zones | | | created_at | 2020-08-07T00:05:40Z | | description | | | distributed | False | | external_gateway_info | null | | flavor_id | None | | ha | False | | id | f40d6130-a01c-486a-b088-3f27c9f57607 | | location | cloud=\u0026#39;\u0026#39;, project.domain_id=, project.domain_name=\u0026#39;d efault\u0026#39;, project.id=\u0026#39;c76211c24a1f460ca67274d655d46725\u0026#39;, project.name=\u0026#39;admin\u0026#39;, re gion_name=\u0026#39;\u0026#39;, zone= | | name | router | | project_id | c76211c24a1f460ca67274d655d46725 | | revision_number | 1 | | routes | | | status | ACTIVE | | tags | | | updated_at | 2020-08-07T00:05:40Z | +-------------------------+----------------------------------------------------- -------------------------------------------------------------------------------- --------------------+ $ controller ~(keystone)\u0026gt; openstack network create int --provider-network-type vxlan +---------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +---------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ | admin_state_up | UP | | availability_zone_hints | | | availability_zones | | | created_at | 2020-08-07T00:05:58Z | | description | | | dns_domain | None | | id | 0edec63e-cc62-4e93-8962-d0ad2df27bc8 | | ipv4_address_scope | None | | ipv6_address_scope | None | | is_default | False | | is_vlan_transparent | None | | location | cloud=\u0026#39;\u0026#39;, project.domain_id=, project.domain_name=\u0026#39;default\u0026#39;, project.id=\u0026#39;c76211c24a1f460ca67274d655d46725\u0026#39;, project.name=\u0026#39;admin\u0026#39;, region_name=\u0026#39;\u0026#39;, zone= | | mtu | 1450 | | name | int | | port_security_enabled | True | | project_id | c76211c24a1f460ca67274d655d46725 | | provider:network_type | vxlan | | provider:physical_network | None | | provider:segmentation_id | 1 | | qos_policy_id | None | | revision_number | 1 | | router:external | Internal | | segments | None | | shared | False | | status | ACTIVE | | subnets | | | tags | | | updated_at | 2020-08-07T00:05:58Z | +---------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ $ controller ~(keystone)\u0026gt; openstack subnet create int-sub --network int \\ --subnet-range 1.1.1.0/24 --gateway 1.1.1.1 \\ --dns-nameserver 8.8.8.8 +----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ | allocation_pools | 1.1.1.2-1.1.1.254 | | cidr | 1.1.1.0/24 | | created_at | 2020-08-07T00:06:25Z | | description | | | dns_nameservers | 8.8.8.8 | | dns_publish_fixed_ip | None | | enable_dhcp | True | | gateway_ip | 1.1.1.1 | | host_routes | | | id | 800bc5af-45e9-4719-8969-4c154bc111d6 | | ip_version | 4 | | ipv6_address_mode | None | | ipv6_ra_mode | None | | location | cloud=\u0026#39;\u0026#39;, project.domain_id=, project.domain_name=\u0026#39;default\u0026#39;, project.id=\u0026#39;c76211c24a1f460ca67274d655d46725\u0026#39;, project.name=\u0026#39;admin\u0026#39;, region_name=\u0026#39;\u0026#39;, zone= | | name | int-sub | | network_id | 0edec63e-cc62-4e93-8962-d0ad2df27bc8 | | prefix_length | None | | project_id | c76211c24a1f460ca67274d655d46725 | | revision_number | 0 | | segment_id | None | | service_types | | | subnetpool_id | None | | tags | | | updated_at | 2020-08-07T00:06:25Z | +----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ $ controller ~(keystone)\u0026gt; openstack router add subnet router int-sub $ controller ~(keystone)\u0026gt; openstack network create \\ --provider-physical-network physnet1 \\ --provider-network-type flat --external ext +---------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +---------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ | admin_state_up | UP | | availability_zone_hints | | | availability_zones | | | created_at | 2020-08-07T00:06:47Z | | description | | | dns_domain | None | | id | 68e5adb0-a8c4-473b-88a9-fdaaf6f12ec2 | | ipv4_address_scope | None | | ipv6_address_scope | None | | is_default | False | | is_vlan_transparent | None | | location | cloud=\u0026#39;\u0026#39;, project.domain_id=, project.domain_name=\u0026#39;default\u0026#39;, project.id=\u0026#39;c76211c24a1f460ca67274d655d46725\u0026#39;, project.name=\u0026#39;admin\u0026#39;, region_name=\u0026#39;\u0026#39;, zone= | | mtu | 1500 | | name | ext | | port_security_enabled | True | | project_id | c76211c24a1f460ca67274d655d46725 | | provider:network_type | flat | | provider:physical_network | physnet1 | | provider:segmentation_id | None | | qos_policy_id | None | | revision_number | 1 | | router:external | External | | segments | None | | shared | False | | status | ACTIVE | | subnets | | | tags | | | updated_at | 2020-08-07T00:06:47Z | +---------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ $ controller ~(keystone)\u0026gt; openstack subnet create ext-sub \\ --network ext --subnet-range 192.168.10.0/24 \\ --allocation-pool start=192.168.10.150,end=192.168.10.200 \\ --gateway 192.168.10.2 --dns-nameserver 8.8.8.8 +----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ | allocation_pools | 192.168.10.150-192.168.10.200 | | cidr | 192.168.10.0/24 | | created_at | 2020-08-07T00:07:21Z | | description | | | dns_nameservers | 8.8.8.8 | | dns_publish_fixed_ip | None | | enable_dhcp | True | | gateway_ip | 192.168.10.2 | | host_routes | | | id | 31a92331-f102-4c4e-8c02-f97baa9eab28 | | ip_version | 4 | | ipv6_address_mode | None | | ipv6_ra_mode | None | | location | cloud=\u0026#39;\u0026#39;, project.domain_id=, project.domain_name=\u0026#39;default\u0026#39;, project.id=\u0026#39;c76211c24a1f460ca67274d655d46725\u0026#39;, project.name=\u0026#39;admin\u0026#39;, region_name=\u0026#39;\u0026#39;, zone= | | name | ext-sub | | network_id | 68e5adb0-a8c4-473b-88a9-fdaaf6f12ec2 | | prefix_length | None | | project_id | c76211c24a1f460ca67274d655d46725 | | revision_number | 0 | | segment_id | None | | service_types | | | subnetpool_id | None | | tags | | | updated_at | 2020-08-07T00:07:21Z | +----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ $ controller ~(keystone)\u0026gt; openstack router set router --external-gateway ext $ controller ~(keystone)\u0026gt; openstack network rbac list +--------------------------------------+-------------+--------------------------------------+ | ID | Object Type | Object ID | +--------------------------------------+-------------+--------------------------------------+ | 4e8ebe0b-60f0-485c-8696-74378068c844 | network | 68e5adb0-a8c4-473b-88a9-fdaaf6f12ec2 | +--------------------------------------+-------------+--------------------------------------+ $ controller ~(keystone)\u0026gt; wget http://cloud-images.ubuntu.com/releases/18.04/release/ubuntu-18.04-server-cloudimg-amd64.img -P /var/kvm/images $ controller ~(keystone)\u0026gt; openstack image create \u0026#34;Ubuntu1804\u0026#34; --file /var/kvm/images/ubuntu-18.04-server-cloudimg-amd64.img --disk-format qcow2 --container-format bare --public # Ubuntu18.04 이미지를 다운로드 후, 등록합니다. $ controller ~(keystone)\u0026gt; openstack security group create all-port +-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ | created_at | 2020-08-07T00:10:31Z | | description | all-port | | id | 97224218-b304-4076-9645-d68092a9366a | | location | cloud=\u0026#39;\u0026#39;, project.domain_id=, project.domain_name=\u0026#39;default\u0026#39;, project.id=\u0026#39;c76211c24a1f460ca67274d655d46725\u0026#39;, project.name=\u0026#39;admin\u0026#39;, region_name=\u0026#39;\u0026#39;, zone= | | name | all-port | | project_id | c76211c24a1f460ca67274d655d46725 | | revision_number | 1 | | rules | created_at=\u0026#39;2020-08-07T00:10:32Z\u0026#39;, direction=\u0026#39;egress\u0026#39;, ethertype=\u0026#39;IPv6\u0026#39;, id=\u0026#39;333de7e9-5c1b-4b2f-bb0e-2da1b878abb6\u0026#39;, updated_at=\u0026#39;2020-08-07T00:10:32Z\u0026#39; | | | created_at=\u0026#39;2020-08-07T00:10:32Z\u0026#39;, direction=\u0026#39;egress\u0026#39;, ethertype=\u0026#39;IPv4\u0026#39;, id=\u0026#39;644e18e1-4f4e-42ad-bef8-937e47254a27\u0026#39;, updated_at=\u0026#39;2020-08-07T00:10:32Z\u0026#39; | | stateful | True | | tags | [] | | updated_at | 2020-08-07T00:10:32Z | +-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ $ controller ~(keystone)\u0026gt; openstack security group rule create --protocol icmp --ingress all-port +-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ | created_at | 2020-08-07T00:13:31Z | | description | | | direction | ingress | | ether_type | IPv4 | | id | 27688481-047b-4fc0-948c-de109e46d7f5 | | location | cloud=\u0026#39;\u0026#39;, project.domain_id=, project.domain_name=\u0026#39;default\u0026#39;, project.id=\u0026#39;c76211c24a1f460ca67274d655d46725\u0026#39;, project.name=\u0026#39;admin\u0026#39;, region_name=\u0026#39;\u0026#39;, zone= | | name | None | | port_range_max | None | | port_range_min | None | | project_id | c76211c24a1f460ca67274d655d46725 | | protocol | icmp | | remote_group_id | None | | remote_ip_prefix | 0.0.0.0/0 | | revision_number | 0 | | security_group_id | 97224218-b304-4076-9645-d68092a9366a | | tags | [] | | updated_at | 2020-08-07T00:13:31Z | +-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ $ controller ~(keystone)\u0026gt; openstack security group rule create --protocol tcp --dst-port 22:22 all-port +-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ | created_at | 2020-08-07T00:13:36Z | | description | | | direction | ingress | | ether_type | IPv4 | | id | da2afd20-818a-4bfe-9017-c837b2bf30ec | | location | cloud=\u0026#39;\u0026#39;, project.domain_id=, project.domain_name=\u0026#39;default\u0026#39;, project.id=\u0026#39;c76211c24a1f460ca67274d655d46725\u0026#39;, project.name=\u0026#39;admin\u0026#39;, region_name=\u0026#39;\u0026#39;, zone= | | name | None | | port_range_max | 22 | | port_range_min | 22 | | project_id | c76211c24a1f460ca67274d655d46725 | | protocol | tcp | | remote_group_id | None | | remote_ip_prefix | 0.0.0.0/0 | | revision_number | 0 | | security_group_id | 97224218-b304-4076-9645-d68092a9366a | | tags | [] | | updated_at | 2020-08-07T00:13:36Z | +-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ $ controller ~(keystone)\u0026gt; ssh-keygen -q -N \u0026#34;\u0026#34; $ controller ~(keystone)\u0026gt; openstack keypair create --public-key ~/.ssh/id_rsa.pub MyKey +-------------+-------------------------------------------------+ | Field | Value | +-------------+-------------------------------------------------+ | fingerprint | a3:8f:44:f6:e1:4e:da:a0:90:f1:5d:dc:6a:8b:ad:76 | | name | MyKey | | user_id | 57ce8f772e374a7c9282f2674fda1ba7 | +-------------+-------------------------------------------------+ $ controller ~(keystone)\u0026gt; openstack flavor create --ram 1024 --disk 10 --vcpus 1 m1.small +----------------------------+--------------------------------------+ | Field | Value | +----------------------------+--------------------------------------+ | OS-FLV-DISABLED:disabled | False | | OS-FLV-EXT-DATA:ephemeral | 0 | | disk | 10 | | id | dabfebd4-cd05-4cec-9567-78b8c9e3d6b6 | | name | m1.small | | os-flavor-access:is_public | True | | properties | | | ram | 1024 | | rxtx_factor | 1.0 | | swap | | | vcpus | 1 | +----------------------------+--------------------------------------+ $ controller ~(keystone)\u0026gt; openstack server create --image Ubuntu1804 --flavor m1.small --key Mykey --network int --security-group all-port Ubuntu $ controller ~(keystone)\u0026gt; openstack floating ip create ext +---------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +---------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | created_at | 2020-08-07T00:16:15Z | | description | | | dns_domain | None | | dns_name | None | | fixed_ip_address | None | | floating_ip_address | 192.168.10.191 | | floating_network_id | 68e5adb0-a8c4-473b-88a9-fdaaf6f12ec2 | | id | 409a4724-1e13-4150-a2e1-6b3a205c4ff6 | | location | Munch({\u0026#39;cloud\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;region_name\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;zone\u0026#39;: None, \u0026#39;project\u0026#39;: Munch({\u0026#39;id\u0026#39;: \u0026#39;c76211c24a1f460ca67274d655d46725\u0026#39;, \u0026#39;name\u0026#39;: \u0026#39;admin\u0026#39;, \u0026#39;domain_id\u0026#39;: None, \u0026#39;domain_name\u0026#39;: \u0026#39;default\u0026#39;})}) | | name | 192.168.10.191 | | port_details | None | | port_id | None | | project_id | c76211c24a1f460ca67274d655d46725 | | qos_policy_id | None | | revision_number | 0 | | router_id | None | | status | DOWN | | subnet_id | None | | tags | [] | | updated_at | 2020-08-07T00:16:15Z | +---------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ $ controller ~(keystone)\u0026gt; openstack server add floating ip Ubuntu 192.168.10.191 $ controller ~(keystone)\u0026gt; # "},{"id":22,"href":"/erp/docs/OpenStack/OpenStackTraining/OpenStack-Ussuri-07/","title":"OpenStack Ussuri : Cinder","section":"OpenStack Training","content":" OpenStack Ussuri : Cinder # # ----------------------- ----------------------- ----------------------- | [ Controller Node ] | | [ Compute Node ] | | [ Network Node ] | | | | Libvirt | | Open vSwitch | | MariaDB RabbitMQ | | Nova compute | | L2 Agent | | Memcached Keystone | | Open vSwitch | | L3 Agent | | httpd Cinder API | | L2 Agent | | metadata agent | | Nova-API Compute | | Cinder-LVM | ----------------------- | L2 agent L3 agent | | NFS | | metadata agent | ----------------------- | Neutron Server | ----------------------- # OpenStack Ussuri : Cinder # # Cinder는 OpenStack에서 전체적인 볼륨, 디스크를 관리하는 서비스입니다. Cinder 서비스는 다른 Storage Node들과 함께 사용하도록 NFS 서버 또한 구축하여 백업 서비스를 활성하할 수 있게 구성핟록 하겠습니다. Cinder에 대한 자세한 설명은 Cinder를 참조해주세요. # # Cinder service 및 User 생성 # # $ controller ~(keystone)\u0026gt; openstack user create --domain default --project service --password qwer1234 cinder +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | default_project_id | 7c10c02365be496fb47f12bfd40fe4a7 | | domain_id | default | | enabled | True | | id | 1f9dbcbb529a45c28b5bb8b035ea277a | | name | cinder | | options | {} | | password_expires_at | None | +---------------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack role add --project service --user cinder admin $ controller ~(keystone)\u0026gt; openstack service create --name cinderv3 --description \u0026#34;OpenStack Block Storage\u0026#34; volumev3 +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | OpenStack Block Storage | | enabled | True | | id | 225ceadb699d4e79adf30769cd872fef | | name | cinderv3 | | type | volumev3 | +-------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne volumev3 public http://controller:8776/v3/%\\(tenant_id\\)s +--------------+-----------------------------------------+ | Field | Value | +--------------+-----------------------------------------+ | enabled | True | | id | 6bf917232caa43eab3b83959fb19cb45 | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | 225ceadb699d4e79adf30769cd872fef | | service_name | cinderv3 | | service_type | volumev3 | | url | http://controller:8776/v3/%(tenant_id)s | +--------------+-----------------------------------------+ $ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne volumev3 internal http://controller:8776/v3/%\\(tenant_id\\)s +--------------+-----------------------------------------+ | Field | Value | +--------------+-----------------------------------------+ | enabled | True | | id | c5987fc3d9eb4fb79a2e8cf73a274936 | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | 225ceadb699d4e79adf30769cd872fef | | service_name | cinderv3 | | service_type | volumev3 | | url | http://controller:8776/v3/%(tenant_id)s | +--------------+-----------------------------------------+ $ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne volumev3 admin http://controller:8776/v3/%\\(tenant_id\\)s +--------------+-----------------------------------------+ | Field | Value | +--------------+-----------------------------------------+ | enabled | True | | id | eff2398584944c0fa7575d1991d725fe | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | 225ceadb699d4e79adf30769cd872fef | | service_name | cinderv3 | | service_type | volumev3 | | url | http://controller:8776/v3/%(tenant_id)s | +--------------+-----------------------------------------+ # Cinder의 Endpoint를 생성합니다. # # Cinder 유저의 DB를 생성합니다. # # $ controller\u0026gt; mysql -u root -p $ MariaDB\u0026gt; create database cinder; $ MariaDB\u0026gt; grant all privileges on cinder.* to cinder@\u0026#39;localhost\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; grant all privileges on cinder.* to cinder@\u0026#39;%\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; flush privileges; $ MariaDB\u0026gt; exit; # # Cinder 설치 # # $ controller\u0026gt; dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install openstack-cinder # cinder 및 관련 모듈을 설치합니다. $ controller\u0026gt; vi /etc/cinder/cinder.conf [DEFAULT] my_ip = controller log_dir = /var/log/cinder state_path = /var/lib/cinder auth_strategy = keystone transport_url = rabbit://openstack:qwer1234@controller enable_v3_api = True [database] connection = mysql+pymysql://cinder:qwer1234@controller/cinder [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = cinder password = qwer1234 [oslo_concurrency] lock_path = $state_path/tmp $ controller\u0026gt; su -s /bin/bash cinder -c \u0026#34;cinder-manage db sync\u0026#34; $ controller\u0026gt; systemctl enable --now openstack-cinder-api openstack-cinder-scheduler # cinder DB를 임포트 시키고, 서비스를 등록합니다. $ controller\u0026gt; echo \u0026#34;export OS_VOLUME_API_VERSION=3\u0026#34; \u0026gt;\u0026gt; ~/admin_key $ controller\u0026gt; source ~/admin_key # key파일을 수정합니다. $ controller\u0026gt; firewall-cmd --add-port=8776/tcp --permanent $ controller\u0026gt; firewall-cmd --reload # 방화벽을 설정합니다. # # Cinder compute node 설치 # # $ compute\u0026gt; dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install openstack-cinder targetcli # cinder 및 관련 모듈을 설치합니다. $ compute\u0026gt; fdisk ... # LVM의 타입으로 파티션을 추가합니다. # cinder 이름으로 vg를 생성합니다. $ controller\u0026gt; scp /etc/cinder/cinder.conf compute:/etc/cinder/cinder.conf $ compute\u0026gt; vi /etc/cinder/cinder.conf [default] my_ip = compute ... ... glance_api_servers = http://controller:9292 enabled_backends = lvm [lvm] target_helper = lioadm target_protocol = iscsi target_ip_address = compute volume_group = cinder volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver volumes_dir = $state_path/volumes $ compute\u0026gt; vi /etc/nova/nova.conf [cinder] os_region_name = RegionOne $ compute\u0026gt; systemctl restart openstack-nova-compute $ compute\u0026gt; systemctl enable --now openstack-cinder-volume $ compute\u0026gt; vi iscsiadm.te module iscsiadm 1.0; require { type iscsid_t; class capability dac_override; } #============= iscsid_t ============== allow iscsid_t self:capability dac_override; $ compute\u0026gt; checkmodule -m -M -o iscsiadm.mod iscsiadm.te $ compute\u0026gt; semodule_package --outfile iscsiadm.pp --module iscsiadm.mod $ compute\u0026gt; semodule -i iscsiadm.pp $ compute\u0026gt; firewall-cmd --add-service=iscsi-target --permanent $ compute\u0026gt; firewall-cmd --reload # SELinux 및 방화벽을 설정합니다. # # 확인 # $ controller ~/(keystone)\u0026gt; openstack volume service list +------------------+-------------+------+---------+-------+----------------------------+ | Binary | Host | Zone | Status | State | Updated At | +------------------+-------------+------+---------+-------+----------------------------+ | cinder-scheduler | controller | nova | enabled | up | 2020-08-07T01:29:22.000000 | | cinder-volume | compute@lvm | nova | enabled | up | 2020-08-07T01:29:22.000000 | +------------------+-------------+------+---------+-------+----------------------------+ $ controller ~/(keystone)\u0026gt; openstack volume create --size 1 test +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | attachments | [] | | availability_zone | nova | | bootable | false | | consistencygroup_id | None | | created_at | 2020-08-07T01:46:06.000000 | | description | None | | encrypted | False | | id | aa07bf85-424d-478c-ae52-648ddc588465 | | migration_status | None | | multiattach | False | | name | test | | properties | | | replication_status | None | | size | 1 | | snapshot_id | None | | source_volid | None | | status | creating | | type | __DEFAULT__ | | updated_at | None | | user_id | 57ce8f772e374a7c9282f2674fda1ba7 | +---------------------+--------------------------------------+ $ controller ~/(keystone)\u0026gt; openstack volume list +--------------------------------------+------+-----------+------+-------------+ | ID | Name | Status | Size | Attached to | +--------------------------------------+------+-----------+------+-------------+ | aa07bf85-424d-478c-ae52-648ddc588465 | test | available | 1 | | +--------------------------------------+------+-----------+------+-------------+ # # 오류가 있어 수정 중입니다 ! # # Cinder 백업 서비스 구성 # NFS 구성참조 # $ compute\u0026gt; dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install nfs-utils # nfs-utils을 설치합니다. $ compute\u0026gt; vi /etc/exports /nfs 10.10.10.0/24(rw,no_root_squash) # ro: 마운트 된 볼륨의 데이터를 읽기만 가능 # rw: 마운트 된 볼륨에 쓰기 또한 가능 # no_root_squash: 루트 자격을 가진 사용자만 쓰기 가능 # noaccess: 디렉터리 접근 불가 $ compute\u0026gt; systemctl enable --now rpcbind nfs-server # NFC-server 서비스를 등록 및 시작합니다. $ vi /etc/cinder/cinder.conf [default] ... ... enabled_backends = lvm,nfs [nfs] volume_driver = cinder.volume.drivers.nfs.NfsDriver volume_backend_name = NFS nfs_shares_config = /etc/cinder/nfs_shares nfs_mount_point_base = $state_path/mnt_nfs # cinder.conf 파일의 nfs를 추가합니다. $ compute\u0026gt; vi /etc/cinder/nfs_shares compute:/nfs # 공유될 디렉토리를 지정합니다. $ compute\u0026gt; chmod 640 /etc/cinder/nfs_shares $ compute\u0026gt; chgrp cinder /etc/cinder/nfs_shares $ compute\u0026gt; systemctl restart openstack-cinder-volume $ compute\u0026gt; chown -R cinder. /var/lib/cinder/mnt_nfs # cinder nfs 파일의 권한을 변경하고 cinder 서비스를 재시작합니다. $ compute\u0026gt; firewall-cmd --add-service=nfs --permanent $ compute\u0026gt; firewall-cmd --reload # 방화벽을 설정합니다. $ compute\u0026gt; vi iscsiadm.te module iscsiadm 1.0; require { type iscsid_t; class capability dac_override; } #============= iscsid_t ============== allow iscsid_t self:capability dac_override; $ compute\u0026gt; checkmodule -m -M -o iscsiadm.mod iscsiadm.te $ compute\u0026gt; semodule_package --outfile iscsiadm.pp --module iscsiadm.mod $ compute\u0026gt; semodule -i iscsiadm.pp $ compute\u0026gt; systemctl restart openstack-nova-compute # SELinux를 설정하고 compute 서비스를 재시작합니다. # # Cinder 백업 서비스 구성 # # $ compute\u0026gt; vi /etc/cinder/cinder.conf [default] ... ... backup_driver = cinder.backup.drivers.nfs.NFSBackupDriver backup_mount_point_base = $state_path/backup_nfs backup_share = compute:/var/lib/cinder-backup # ciner 백업 서비스를 활성화하기 이해 cinder.conf 파일의 설정을 추가합니다. $ compute\u0026gt; systemctl enable --now openstack-cinder-backup $ compute\u0026gt; chown -R cinder. /var/lib/cinder/backup_nfs $ cinder backup 서비스를 활성화합니다. # # 확인 # # $ controller ~(keystone)\u0026gt; openstack volume service list +------------------+-------------+------+---------+-------+----------------------------+ | Binary | Host | Zone | Status | State | Updated At | +------------------+-------------+------+---------+-------+----------------------------+ | cinder-scheduler | controller | nova | enabled | up | 2020-08-12T04:31:53.000000 | | cinder-volume | compute@lvm | nova | enabled | up | 2020-08-12T04:31:46.000000 | | cinder-volume | compute@nfs | nova | enabled | up | 2020-08-12T04:31:46.000000 | +------------------+-------------+------+---------+-------+----------------------------+ $ controller ~(keystone)\u0026gt; $ controller ~(keystone)\u0026gt; $ controller ~(keystone)\u0026gt; $ controller ~(keystone)\u0026gt; "},{"id":23,"href":"/erp/docs/OpenStack/OpenStackTraining/OpenStack-Ussuri-08/","title":"OpenStack Ussuri : Horizon","section":"OpenStack Training","content":" OpenStack : Horizon # # OpenStack : Horizon # # Horizon은 openstack에서 GUI 환경을 제공해주는 서비스입니다. Horizon에 대한 자세한 설명은 Horizon을 참조해주세요. # $ controller\u0026gt; dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install openstack-dashboard $ controller\u0026gt; vi /etc/openstack-dashboard/local_settings ALLOWED_HOSTS = [\u0026#39;*\u0026#39;,\u0026#39;\u0026#39;] # 모든 host의 접속이 가능하게 설정합니다. CACHES = { \u0026#39;default\u0026#39;: { \u0026#39;BACKEND\u0026#39;: \u0026#39;django.core.cache.backends.memcached.MemcachedCache\u0026#39;, \u0026#39;LOCATION\u0026#39;: \u0026#39;controller:11211\u0026#39;, }, } SESSION_ENGINE = \u0026#34;django.contrib.sessions.backends.cache\u0026#34; OPENSTACK_HOST = \u0026#34;controller\u0026#34; OPENSTACK_KEYSTONE_URL = \u0026#34;http://controller:5000/v3\u0026#34; # openstack host와 SESSION 서버의 host를 지정합니다. TIME_ZONE = \u0026#34;Asia/Seoul\u0026#34; # 시간을 지정합니다. WEBROOT = \u0026#39;/dashboard/\u0026#39; LOGIN_URL = \u0026#39;/dashboard/auth/login/\u0026#39; LOGOUT_URL = \u0026#39;/dashboard/auth/logout/\u0026#39; LOGIN_REDIRECT_URL = \u0026#39;/dashboard/\u0026#39; OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = \u0026#39;Default\u0026#39; OPENSTACK_API_VERSIONS = { \u0026#34;identity\u0026#34;: 3, \u0026#34;volume\u0026#34;: 3, \u0026#34;compute\u0026#34;: 2, } # 끝에 추가합니다. $ controller\u0026gt; vi /etc/httpd/conf.d/openstack-dashboard.conf .... .... WSGIApplicationGroup %{GLOBAL} # 상단에 추가합니다. $ controller\u0026gt; systemctl restart httpd # httpd를 재 시작합니다. $ controller\u0026gt; setsebool -P httpd_can_network_connect on $ controller\u0026gt; firewall-cmd --add-service={http,https} --permanent $ controller\u0026gt; firewall-cmd --reload # SELinux 및 방화벽을 설정합니다. # # 확인 # # 접속확인 http://[ controller의 IP ]/dashboard/ "},{"id":24,"href":"/erp/docs/OpenStack/OpenStackTraining/OpenStack-Ussuri-09/","title":"OpenStack Ussuri : Swift","section":"OpenStack Training","content":" # OpenStack Ussuri : Swift # # ----------------------- ----------------------- ----------------------- | [ Controller Node ] | | [ Compute Node ] | | [ Network Node ] | | | | Libvirt | | Open vSwitch | | MariaDB RabbitMQ | | Nova compute | | L2 Agent | | Memcached Keystone | | Open vSwitch | | L3 Agent | | httpd Cinder API | | L2 Agent | | metadata agent | | Nova-API Compute | | Cinder-LVM | | Swift-proxy | | L2 agent L3 agent | | NFS | ----------------------- | metadata agent | ----------------------- | Neutron Server | ----------------------- --------------------------------- | [ Storage Node 1, 2, 3 ] | | | | Swift-account-auditor | | Swift-account-replicator | | Swift-account | | Swift-container-auditor | | Swift-container-replicator | | Swift-container-updater | | Swift-container | | Swift-object-auditor | | Swift-object-replicator | | Swift-object-updater | | Swift-swift-object | --------------------------------- # OpenStack Ussuri : Swift # # Swift는 우리가 흔히 사용하는 네이버 클라우드, 구글 드라이브와 같은 오브젝트 스토리지 서비스 입니다. Swift 설치는 network, Storage 순으로 이루어집니다. Swift*에 대한 설명은 Swift을 참조해주세요. # # $ controller ~(keystone)\u0026gt; openstack user create --domain default --project service --password qwer1234 swift +--------------------------------------------------------+ | Field | Value | +---------------------+----------------------------------+ | default_project_id | b470c69e28db47cdbfc81e06cc67f627 | | domain_id | default | | enabled | True | | id | dd2f0225406249b195e4feff91eca393 | | name | swift | | options | {} | | password_expires_at | None | +---------------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack role add --project service --user swift admin $ controller ~(keystone)\u0026gt; openstack service create --name swift --description \u0026#34;OpenStack Object Storage\u0026#34; object-store +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | OpenStack Object Storage | | enabled | True | | id | d9d7bc4b99774d3ba701e2eae93edfe2 | | name | swift | | type | object-store | +-------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne object-store public http://network:8080/v1/AUTH_%\\(tenant_id\\)s +--------------+------------------------------------+ | Field | Value | +--------------+------------------------------------+ | enabled | True | | id | a70e1ac16a9144529ea49132cd7dd39e | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | d9d7bc4b99774d3ba701e2eae93edfe2 | | service_name | swift | | service_type | object-store | | url | http://network:8080/v1/AUTH_%(tenant_id)s | +--------------+------------------------------------+ $ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne object-store internal http://network:8080/v1/AUTH_%\\(tenant_id\\)s +--------------+------------------------------------+ | Field | Value | +--------------+------------------------------------+ | enabled | True | | id | 6b5ea7b028f94035aef5601cf35d3a29 | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | d9d7bc4b99774d3ba701e2eae93edfe2 | | service_name | swift | | service_type | object-store | | url | http://network:8080/v1/AUTH_%(tenant_id)s | +--------------+------------------------------------+ $ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne object-store admin http://network:8080/v1 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 08c18a5313f642d59de980f51666f830 | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | d9d7bc4b99774d3ba701e2eae93edfe2 | | service_name | swift | | service_type | object-store | | url | http://network:8080/v1 | +--------------+----------------------------------+ # # Network Node Swift-Proxy 설치 # # $ network\u0026gt; dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install openstack-swift-proxy python3-memcached openssh-clients # swift-proxy 및 관련 모듈을 설치합니다. $ network\u0026gt; vi /etc/swift/proxy-server.conf [filter:cache] use = egg:swift#memcache memcache_servers = controller:11211 [filter:authtoken] paste.filter_factory = keystonemiddleware.auth_token:filter_factory # admin_tenant_name = %SERVICE_TENANT_NAME% # admin_user = %SERVICE_USER% # admin_password = %SERVICE_PASSWORD% # auth_host = 127.0.0.1 # auth_port = 35357 # auth_protocol = http # signing_dir = /tmp/keystone-signing-swift # 주석처리 후, 하단의 아래의 항모들을 추가합니다.합니다. www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = swift password = qwer1234 delay_auth_decision = true $ network\u0026gt; vi /etc/swift/swift.conf [swift-hash] swift_hash_path_suffix = swift_shared_path swift_hash_path_prefix = swift_shared_path # 파일 안에 내용들을 삭제 후, 생성합니다. $ network\u0026gt; swift-ring-builder /etc/swift/account.builder create 12 3 1 $ network\u0026gt; swift-ring-builder /etc/swift/container.builder create 12 3 1 $ network\u0026gt; swift-ring-builder /etc/swift/object.builder create 12 3 1 $ network\u0026gt; swift-ring-builder /etc/swift/account.builder add r0z0-10.10.10.50:6202/device 100 $ network\u0026gt; swift-ring-builder /etc/swift/container.builder add r0z0-10.10.10.50:6201/device 100 $ network\u0026gt; swift-ring-builder /etc/swift/object.builder add r0z0-10.10.10.50:6200/device 100 $ network\u0026gt; swift-ring-builder /etc/swift/account.builder add r1z1-10.10.10.51:6202/device 100 $ network\u0026gt; swift-ring-builder /etc/swift/container.builder add r1z1-10.10.10.51:6201/device 100 $ network\u0026gt; swift-ring-builder /etc/swift/object.builder add r1z1-10.10.10.51:6200/device 100 $ network\u0026gt; swift-ring-builder /etc/swift/account.builder add r2z2-10.10.10.52:6202/device 100 $ network\u0026gt; swift-ring-builder /etc/swift/container.builder add r2z2-10.10.10.52:6201/device 100 $ network\u0026gt; swift-ring-builder /etc/swift/object.builder add r2z2-10.10.10.52:6200/device 100 $ network\u0026gt; swift-ring-builder /etc/swift/account.builder rebalance $ network\u0026gt; swift-ring-builder /etc/swift/container.builder rebalance $ network\u0026gt; swift-ring-builder /etc/swift/object.builder rebalance $ network\u0026gt; chown swift. /etc/swift/*.gz $ network\u0026gt; systemctl enable --now openstack-swift-proxy $ network\u0026gt; firewall-cmd --add-port=8080/tcp --permanent $ network\u0026gt; firewall-cmd --reload # # Swift Stoage Node 설치 # # $ storage all\u0026gt; dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install openstack-swift-account openstack-swift-container openstack-swift-object openstack-selinux xfsprogs rsync rsync-daemon openssh-clients # swift 밒 관련 모듈을 설치합니다. $ storage all\u0026gt; mkfs.xfs -i size=1024 -s size=4096 /dev/sdb1 $ storage all\u0026gt; mkdir -p /srv/node/device $ storage all\u0026gt; mount -o noatime,nodiratime /dev/sdb1 /srv/node/device $ storage all\u0026gt; chown -R swift. /srv/node # 하드 디스크를 임포트 후, XFS로 포맷을 진행합니다. $ storage all\u0026gt; vi /etc/fstab /dev/sdb1 /srv/node/device xfs noatime,nodiratime 0 0 # 설정을 fstab의 등록합니다. $ network\u0026gt; scp /etc/swift/*.gz storage1:/etc/swift/ $ network\u0026gt; scp /etc/swift/*.gz storage2:/etc/swift/ $ network\u0026gt; scp /etc/swift/*.gz storage3:/etc/swift/ # 설정을 복사합니다. $ storage all\u0026gt; chown swift. /etc/swift/*.gz $ storage all\u0026gt; vi /etc/swift/swift.conf [swift-hash] swift_hash_path_suffix = swift_shared_path swift_hash_path_prefix = swift_shared_path $ storage all\u0026gt; vi /etc/swift/account-server.conf bind_ip = 0.0.0.0 bind_port = 6202 $ storage all\u0026gt; vi /etc/swift/container-server.conf bind_ip = 0.0.0.0 bind_port = 6201 $ storage all\u0026gt; vi /etc/swift/object-server.conf bind_ip = 0.0.0.0 bind_port = 6200 $ storage all\u0026gt; vi /etc/rsyncd.conf pid file = /var/run/rsyncd.pid log file = /var/log/rsyncd.log uid = swift gid = swift pid file = /var/run/rsyncd.pid log file = /var/log/rsyncd.log uid = swift gid = swift address = storage1 or storage2 or storage3 [account] path = /srv/node read only = false write only = no list = yes incoming chmod = 0644 outgoing chmod = 0644 max connections = 25 lock file = /var/lock/account.lock [container] path = /srv/node read only = false write only = no list = yes incoming chmod = 0644 outgoing chmod = 0644 max connections = 25 lock file = /var/lock/container.lock [object] path = /srv/node read only = false write only = no list = yes incoming chmod = 0644 outgoing chmod = 0644 max connections = 25 lock file = /var/lock/object.lock [swift_server] path = /etc/swift read only = true write only = no list = yes incoming chmod = 0644 outgoing chmod = 0644 max connections = 5 lock file = /var/lock/swift_server.lock $ storage all\u0026gt; semanage fcontext -a -t swift_data_t /srv/node/device $ storage all\u0026gt; restorecon /srv/node/device $ storage all\u0026gt; firewall-cmd --add-port={873/tcp,6200/tcp,6201/tcp,6202/tcp} --permanent $ storage all\u0026gt; firewall-cmd --reload # SELinux 및 방화벽을 설정합니다. $ storage all\u0026gt; systemctl enable --now rsyncd \\ openstack-swift-account-auditor \\ openstack-swift-account-replicator \\ openstack-swift-account \\ openstack-swift-container-auditor \\ openstack-swift-container-replicator \\ openstack-swift-container-updater \\ openstack-swift-container \\ openstack-swift-object-auditor \\ openstack-swift-object-replicator \\ openstack-swift-object-updater \\ openstack-swift-object # swift 서비스를 등록 및 시작합니다. # # 확인 # # $ controller ~(keystone)\u0026gt; dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install python3-openstackclient python3-keystoneclient python3-swiftclient # swift 사용을 위해 관련 모듈을 설치합니다. $ controller ~(keystone)\u0026gt; openstack project create --domain default --description \u0026#34;Swift Service Project\u0026#34; swiftservice +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | Swift Service Project | | domain_id | default | | enabled | True | | id | ab658f35464e49b7a3df626e09feab91 | | is_domain | False | | name | swiftservice | | options | {} | | parent_id | default | | tags | [] | +-------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack role create SwiftOperator +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | None | | domain_id | None | | id | 3818d26e54244c1ba5d0481e9ad44e6e | | name | SwiftOperator | | options | {} | +-------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack user create --domain default --project swiftservice --password qwer1234 swiftuser01 +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | default_project_id | ab658f35464e49b7a3df626e09feab91 | | domain_id | default | | enabled | True | | id | 2ac2c69fd55a4bef95b2a8b728f131a7 | | name | swiftuser01 | | options | {} | | password_expires_at | None | +---------------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack role add --project swiftservice --user swiftuser01 SwiftOperator $ controller ~(keystone)\u0026gt; vi ~/swift export OS_PROJECT_DOMAIN_NAME=default export OS_USER_DOMAIN_NAME=default export OS_PROJECT_NAME=swiftservice export OS_USERNAME=swiftuser01 export OS_PASSWORD=qwer1234 export OS_AUTH_URL=http://controller:5000/v3 export OS_IDENTITY_API_VERSION=3 export PS1=\u0026#39;[\\u@\\h \\W(swift)]\\$ \u0026#39; $ controller ~(keystone)\u0026gt; chmod 600 ~/swift $ controller ~(keystone)\u0026gt; source ~/swift $ controller ~(keystone)\u0026gt; echo \u0026#34;source ~/swift \u0026#34; \u0026gt;\u0026gt; ~/.bash_profile $ controller ~(swift)\u0026gt; swift stat Account: AUTH_ab658f35464e49b7a3df626e09feab91 Containers: 0 Objects: 0 Bytes: 0 Content-Type: text/plain; charset=utf-8 X-Timestamp: 1597360203.35834 X-Put-Timestamp: 1597360203.35834 Vary: Accept X-Trans-Id: tx09982b0a02ac4b7eac244-005f35c849 X-Openstack-Request-Id: tx09982b0a02ac4b7eac244-005f35c849 $ controller ~(swift)\u0026gt; openstack container create test +---------------------------------------+-----------+------------------------------------+ | account | container | x-trans-id | +---------------------------------------+-----------+------------------------------------+ | AUTH_ab658f35464e49b7a3df626e09feab91 | test | txce00712612794927965f7-005f35c864 | +---------------------------------------+-----------+------------------------------------+ $ controller ~(swift)\u0026gt; openstack container list +------+ | Name | +------+ | test | +------+ $ controller ~(swift)\u0026gt; openstack object create testfile.txt test $ controller ~(swift)\u0026gt; openstack object list test $ controller ~(swift)\u0026gt; rm testfile.txt $ controller ~(swift)\u0026gt; openstack object save test testfile.txt $ controller ~(swift)\u0026gt; ll testfile.txt $ controller ~(swift)\u0026gt; openstack object delete test testfile.txt $ controller ~(swift)\u0026gt; openstack object list test # "},{"id":25,"href":"/erp/docs/OpenStack/OpenStackTraining/OpenStack-Ussuri-10/","title":"OpenStack Ussuri : Heat","section":"OpenStack Training","content":" OpenStack Ussuri : Heat # # ----------------------- ----------------------- ----------------------- | [ Controller Node ] | | [ Compute Node ] | | [ Network Node ] | | | | Libvirt | | Open vSwitch | | MariaDB RabbitMQ | | Nova compute | | L2 Agent | | Memcached Keystone | | Open vSwitch | | L3 Agent | | httpd Cinder API | | L2 Agent | | metadata agent | | Nova-API Compute | | Cinder-LVM | | Swift-proxy | | L2 agent L3 agent | | NFS | | Heat API | | metadata agent | ----------------------- | API-CFN | | Neutron Server | | Heat Engine | ----------------------- ----------------------- --------------------------------- | [ Storage Node 1, 2, 3 ] | | | | Swift-account-auditor | | Swift-account-replicator | | Swift-account | | Swift-container-auditor | | Swift-container-replicator | | Swift-container-updater | | Swift-container | | Swift-object-auditor | | Swift-object-replicator | | Swift-object-updater | | Swift-swift-object | --------------------------------- # OpenStack Ussuri : Heat # # 클라우딩 컴퓨팅이 꽃인 Orchestaration 기능을 수행하는 Heat 서비스를 설치해보도록 하겠습니다. Heat 설치는 controller, network 노드 순으로 이루어집니다. 단 Heat는 controller에서는 API의 Endpoint만을 제공하며, 대부분의 설정은 network node에서 이루어집니다. Heat*에 대한 설명은 Heat을 참조해주세요. # # Heat service 및 User 생성 # $ controller ~(keystone)\u0026gt; openstack user create --domain default --project service --password qwer1234 heat +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | default_project_id | b470c69e28db47cdbfc81e06cc67f627 | | domain_id | default | | enabled | True | | id | 148bafa480d84f87ba939968edb2585f | | name | heat | | options | {} | | password_expires_at | None | +---------------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack role add --project service --user heat admin $ controller ~(keystone)\u0026gt; openstack role create heat_stack_owner +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | None | | domain_id | None | | id | d46789e4326e4055aa8f6fead7c777bb | | name | heat_stack_owner | | options | {} | +-------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack role create heat_stack_user +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | None | | domain_id | None | | id | ff45744ddbe247919034cea7c3f309e7 | | name | heat_stack_user | | options | {} | +-------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack role add --project admin --user admin heat_stack_owner $ controller ~(keystone)\u0026gt; openstack service create --name heat --description \u0026#34;Openstack Orchestration\u0026#34; orchestration +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | Openstack Orchestration | | enabled | True | | id | 6cd5b7c7a3234b39998073587c2d9f9a | | name | heat | | type | orchestration | +-------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack service create --name heat-cfn --description \u0026#34;Openstack Orchestration\u0026#34; cloudformation +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | Openstack Orchestration | | enabled | True | | id | 2fb2087bf8da472d8c51e9fee39c93ad | | name | heat-cfn | | type | cloudformation | +-------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne orchestration public http://network:8004/v1/AUTH_%\\(tenant_id\\)s +--------------+-------------------------------------------+ | Field | Value | +--------------+-------------------------------------------+ | enabled | True | | id | 50481bc9998b454a9f70682132ecb026 | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | 6cd5b7c7a3234b39998073587c2d9f9a | | service_name | heat | | service_type | orchestration | | url | http://network:8004/v1/AUTH_%(tenant_id)s | +--------------+-------------------------------------------+ $ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne orchestration internal http://network:8004/v1/AUTH_%\\(tenant_id\\)s +--------------+-------------------------------------------+ | Field | Value | +--------------+-------------------------------------------+ | enabled | True | | id | 1015f4c570a747349109b76b7295876c | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | 6cd5b7c7a3234b39998073587c2d9f9a | | service_name | heat | | service_type | orchestration | | url | http://network:8004/v1/AUTH_%(tenant_id)s | +--------------+-------------------------------------------+ $ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne orchestration admin http://network:8004/v1/AUTH_%\\(tenant_id\\)s +--------------+-------------------------------------------+ | Field | Value | +--------------+-------------------------------------------+ | enabled | True | | id | ed21251a3f274ba6bb35061cef6cac1d | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | 6cd5b7c7a3234b39998073587c2d9f9a | | service_name | heat | | service_type | orchestration | | url | http://network:8004/v1/AUTH_%(tenant_id)s | +--------------+-------------------------------------------+ $ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne cloudformation public http://network:8000/v1 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | fb2b67b2a13d43e1a55f775857908a5f | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | 2fb2087bf8da472d8c51e9fee39c93ad | | service_name | heat-cfn | | service_type | cloudformation | | url | http://network:8000/v1 | +--------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne cloudformation internal http://network:8000/v1 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | a8f4517ecf4d4370beecee9e17183c6b | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | 2fb2087bf8da472d8c51e9fee39c93ad | | service_name | heat-cfn | | service_type | cloudformation | | url | http://network:8000/v1 | +--------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne cloudformation admin http://network:8000/v1 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 66db714e538545879b7121f7150e72fc | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | 2fb2087bf8da472d8c51e9fee39c93ad | | service_name | heat-cfn | | service_type | cloudformation | | url | http://network:8000/v1 | +--------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack domain create --description \u0026#34;Stack projects and users\u0026#34; heat +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | Stack projects and users | | enabled | True | | id | 36fa9838b2fa43f6a6bbc95f0cdfd0a7 | | name | heat | | options | {} | | tags | [] | +-------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack user create --domain heat --password qwer1234 heat_domain_admin +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | domain_id | 36fa9838b2fa43f6a6bbc95f0cdfd0a7 | | enabled | True | | id | c77bd90604254f8097aed49ea17f6fb3 | | name | heat_domain_admin | | options | {} | | password_expires_at | None | +---------------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack role add --domain heat --user heat_domain_admin admin # # # # Heat 유저의 DB를 생성합니다. # # $ controller\u0026gt; mysql -u root -p $ MariaDB\u0026gt; create database heat; $ MariaDB\u0026gt; grant all privileges on heat.* to heat@\u0026#39;localhost\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; grant all privileges on heat.* to heat@\u0026#39;%\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; flush privileges; $ MariaDB\u0026gt; exit; # # Network Node Heat 설치 # # $ Network\u0026gt; dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install openstack-heat-api openstack-heat-api-cfn openstack-heat-engine python3-heatclient # Heat 및 관련 모듈을 설치합니다. $ Network\u0026gt; vi /etc/heat/heat.conf [DEFAULT] deferred_auth_method = trusts trusts_delegated_roles = heat_stack_owner heat_metadata_server_url = http://network:8000 heat_waitcondition_server_url = http://network:8000/v1/waitcondition heat_watch_server_url = http://network:8003 heat_stack_user_role = heat_stack_user stack_user_domain_name = heat stack_domain_admin = heat_domain_admin stack_domain_admin_password = qwer1234 transport_url = rabbit://openstack:qwer1234@controller [database] connection = mysql+pymysql://heat:qwer1234@controller/heat [clients_keystone] auth_uri = http://controller:5000 [ec2authtoken] auth_uri = http://controller:5000 [heat_api] bind_host = 0.0.0.0 bind_port = 8004 [heat_api_cfn] bind_host = 0.0.0.0 bind_port = 8000 [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = heat password = qwer1234 [trustee] auth_plugin = password auth_url = http://controller:5000 username = heat password = qwer1234 user_domain_name = default $ network\u0026gt; chgrp heat /etc/heat/heat.conf $ network\u0026gt; chmod 640 /etc/heat/heat.conf $ network\u0026gt; su -s /bin/bash heat -c \u0026#34;heat-manage db_sync\u0026#34; $ network\u0026gt; systemctl enable --now openstack-heat-api openstack-heat-api-cfn openstack-heat-engine # DB를 import 시키고, haet 서비스를 등록 및 시작합니다. $ network\u0026gt; firewall-cmd --add-port={8000/tcp,8004/tcp} --permanent $ network\u0026gt; firewall-cmd --reload # 방화벽을 설정합니다. # # 확인 # # $ controller ~(keystone)\u0026gt; vi sample-stack.yml heat_template_version: 2018-08-31 description: Heat Sample Template parameters: ImageID: type: string description: Image used to boot a server NetID: type: string description: Network ID for the server resources: server1: type: OS::Nova::Server properties: name: \u0026#34;Heat_Deployed_Server\u0026#34; image: { get_param: ImageID } flavor: \u0026#34;m1.tiny\u0026#34; networks: - network: { get_param: NetID } outputs: server1_private_ip: description: IP address of the server in the private network value: { get_attr: [ server1, first_address ] } $ controller ~(keystone)\u0026gt; openstack stack create -t sample-stack.yml --parameter \u0026#34;ImageID=cirros;NetID=Int_net\u0026#34; Sample-Stack # controller ~(keystone)\u0026gt; openstack stack list +--------------------------------------+--------------+----------------------------------+-----------------+----------------------+--------------+ | ID | Stack Name | Project | Stack Status | Creation Time | Updated Time | +--------------------------------------+--------------+----------------------------------+-----------------+----------------------+--------------+ | 4cb88c32-24f9-41cf-a44d-e18593c5eb2f | Sample-Stack | edd7025c02574d3aa2d3ab6e56208320 | CREATE_COMPLETE | 2020-08-13T09:39:16Z | None | +--------------------------------------+---- # controller ~(keystone)\u0026gt; openstack server list +--------------------------------------+----------------------+--------+-----------------------+--------+---------+ | ID | Name | Status | Networks | Image | Flavor | +--------------------------------------+----------------------+--------+-----------------------+--------+---------+ | ab20d06c-955a-404b-9525-11e3e4b09484 | Heat_Deployed_Server | ACTIVE | int_net=192.168.100.6 | cirros | m1.tiny | +--------------------------------------+----------------------+--------+-----------------------+--------+---------+ # "},{"id":26,"href":"/erp/docs/OpenStack/OpenStackTraining/Openstack-stein/","title":"Openstack Stain Manual 설치","section":"OpenStack Training","content":" Openstack Stain Manual 설치 # # 1. 시스템 및 네트워크 구성 # 여기서는 Nat 네트워크를 외부, host1 대역을 내부로 사용하여 Openstack을 구축해보도록 하겠습니다. # 운영체제 및 네트워크 구성 Hypervisor : Vmware Workstation 15 OS : CentOS7 # 노드 구성 OS Hostname Network Interface Network Interface2 CPU RAM DISK CentOS7 controller Nat ( 192.168.10.100 ) HOST1 ( 10.10.10.10 ) 2cpu 4thread 8 RAm 30G CentOS7 natwork Nat ( 192.168.10.101 ) HOST1 ( 10.10.10.20 ) 1cpu 2thread 2 RAm 20G CentOS7 compute Nat ( 192.168.10.102 ) HOST1 ( 10.10.10.30 ) 1cpu 4thread 4 RAm 100G # 기본적인 업데이트 및 설정을 모든 노드에 진행합니다. $ yum -y update # 업데이트 $ vi /etc/hosts 10.10.10.10 controller 10.10.10.20 network 10.10.10.30 compute # known host 등록 # 설정이 완료되면 기본 구성을 모든 노드에 진행합니다. $ yum -y install chrony # 시간 동기화를 위한 chrony 설치 $ vi /etc/chrony.conf #server 0.centos.pool.ntp.org iburst #server 1.centos.pool.ntp.org iburst #server 2.centos.pool.ntp.org iburst #server 3.centos.pool.ntp.org iburst server ntp1.jst.mfeed.ad.jp iburst server ntp2.jst.mfeed.ad.jp iburst server ntp3.jst.mfeed.ad.jp iburst allow 10.10.10.0/24 # 시간동기화 $ firewall-cmd --add-service=ntp --permanent $ firewall-cmd --reload # ntp 방화벽 허용 및 리로딩 $ init 6 # 시스템 재시작 $ chronyc sources # 확인 # # 2. Openstack 기본 패키지 구성 # Openstack의 기본 패키지 구성은 먼저 contorller 노드만을 통해 진행됨을 유의해주시길 바랍니다. controller 노드에는 다음의 패키지가 설치됩니다. MariaDB: OpenStack 서비스 및 VM 관련 설정들을 보관하기 위해 사용 RabbitMQ: OpenStack 서비스 간 상호 메시지를 주고 받기 위하나 메시지 큐로 사용 Memcached: 범용 분산 메모리 캐시 시스템으로, 자주 외부 데이터에 접근해야 하는 경우에 발생하는 오버헤드를 줄이기 위해 메모리르르 캐싱하고 읽어들이는 역할을 담당, OpenStack 서비스에서는 주로 인증 메커니즘에서 토큰 캐싱을 위해 사용됩니다. # Openstack 패키지 설치 및 레포지토리 구성 $ yum -y install centos-release-openstack-stein $ sed -i -e \u0026#34;s/enabled=1/enabled=0/g\u0026#34; /etc/yum.repos.d/CentOS-OpenStack-stein.repo # stein 패캐지를 등록합니다. # MariaDB를 설치합니다. $ yum --enablerepo=centos-openstack-stein -y install mariadb-server $ vi /etc/my.cnf [mysqld] character-set-server=utf8 # charset을 utf-8으로 변경합니다 $ systemctl start mariadb $ systemctl enable mariadb # mariadb을 시작 및 자동시작을 등록합니다. $ mysql_secure_installation # 패스워드 설정을 진행합니다. $ firewall-cmd --add-service=mysql --permanent $ firewall-cmd --reload # RabbitMQ 및 Memcached를 설치합니다. $ yum --enablerepo=centos-openstack-stein -y install rabbitmq-server $ yum --enablerepo=centos-openstack-stein -y install memcached $ vi /etc/my.cnf.d/mariadb-server.cnf [mysqld] ... character-set-server=utf8 max_connections=500 # Mariadb의 위에 내용을 추가합니다. $ vi /etc/sysconfig/memcached OPTIONS=\u0026#34;-l 0.0.0.0,::\u0026#34; # mamcached를 모든 리스닝 상태로 전환시킵니다. $ systemctl restart mariadb rabbitmq-server memcached $ systemctl enable mariadb rabbitmq-server memcached # Mariadb와 함께 RabbitMQ 및 Memcached를 시작 및 자동시작을 등록합니다. $ rabbitmqctl add_user [ id ] [ pw ] # rabbitmq 유저를 생성합니다. 여기서는 openstack/qwer1234를 사용하도록 하겠습니다. $ rabbitmqctl set_permissions [ id ] \u0026#34;.*\u0026#34; \u0026#34;.*\u0026#34; \u0026#34;.*\u0026#34; # 생성한 사용자에게 모든 권한을 부여합니다. $ firewall-cmd --add-port={11211/tcp,5672/tcp} --permanent $ firewall-cmd --reload # # 3. Keystone ( 인증 서비스 ) 구성 # Keystone 또한 controller의 설치를 진행합니다. keystone에 대한 설명은 keystone을 참조해주세요. # keyston DB 생성 $ mysql -u root -p MariaDB [(none)]\u0026gt; create database keystone; MariaDB [(none)]\u0026gt; grant all privileges on keystone.* to keystone@\u0026#39;localhost\u0026#39; identified by \u0026#39;pw\u0026#39;; MariaDB [(none)]\u0026gt; grant all privileges on keystone.* to keystone@\u0026#39;%\u0026#39; identified by \u0026#39;pw\u0026#39;; MariaDB [(none)]\u0026gt; flush privileges; # keystone 구성을 위한 db를 생성합니다. ( 저는 편의를 위해 모든 pw로 qwer1234 설정하였습니다. ) # keystone 패키지 설치 및 수정 $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-keystone openstack-utils python-openstackclient httpd mod_wsgi # keystone 및 관련 패키지를 설치합니다. $ vi /etc/keystone/keystone.conf [cache] ... memcache_servers = controller:11211 [database] ... connection = mysql+pymysql://keystone:qwer1234@controller/keystone [token] ... provider = fernet # keystone 구성을 위해 설정파일 수정합니다. # hosts에 등록한 IP 혹은 controller의 IP를 기입하셔도 무관합니다. $ su -s /bin/bash keystone -c \u0026#34;keystone-manage db_sync\u0026#34; # 설정 값을 토대로 db의 설정을 저정합니다. $ keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone $ keystone-manage credential_setup --keystone-user keystone --keystone-group keystone # 토큰 및 자격 증명 암호화를 위해 사용되는 키 저장소를 생성합니다. $ export controller=10.10.10.10 $ keystone-manage bootstrap --bootstrap-password qwer1234 \\ --bootstrap-admin-url http://$controller:5000/v3/ \\ --bootstrap-internal-url http://$controller:5000/v3/ \\ --bootstrap-public-url http://$controller:5000/v3/ \\ --bootstrap-region-id RegionOne # controlelr의 IP로 keystone을 부트스트랩합니다. $ setsebool -P httpd_use_openstack on $ setsebool -P httpd_can_network_connect on $ setsebool -P httpd_can_network_connect_db on $ firewall-cmd --add-port=5000/tcp --permanent $ firewall-cmd --reload # Selinux와 방화벽으르 설정합니다. $ ln -s /usr/share/keystone/wsgi-keystone.conf /etc/httpd/conf.d/ $ systemctl start httpd $ systemctl enable httpd # keystone 설정 활성화 및 httpd 를 시작합니다. # 정상 동작 확인을 위한 토큰 파일 생성 $ vi ~/admin export OS_PROJECT_DOMAIN_NAME=default export OS_USER_DOMAIN_NAME=default export OS_PROJECT_NAME=admin export OS_USERNAME=admin export OS_PASSWORD=qwer1234 export OS_AUTH_URL=http://controller:5000/v3 export OS_IDENTITY_API_VERSION=3 export OS_IMAGE_API_VERSION=2 $ chmod 600 ~/admin $ source ~/admin # project 생성 $ cd ~ $ . admin $ openstack project create --domain default --description \u0026#34;Service Project\u0026#34; service +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | Service Project | | domain_id | default | | enabled | True | | id | 3f0b3ef5b8c94a0a9cca8e34ea2fdbd6 | | is_domain | False | | name | service | | parent_id | default | | tags | [] | +-------------+----------------------------------+ # project 생성 $ openstack project list +----------------------------------+---------+ | ID | Name | +----------------------------------+---------+ | 3f0b3ef5b8c94a0a9cca8e34ea2fdbd6 | service | | ec1a4336cfa64d04bbc8f908b26a6cda | admin | +----------------------------------+---------+ # 이것으로 keystone에 대한 설치가 끝났습니다. 혹시 오류가 발생할 경우 /var/log/keystone/ 혹은 /var/log/httpd/에서 error 로그, keystone 로그를 검색하여 오류를 찾아내시면 보다 쉽게 문제를 해결하실 수 있습니다. # # 4. Glance ( 이미지 서비스 ) 구성 # Glance 또한 controller에서만 설치를 진행합니다. 에 대한 설명은 Glance을 참조해주세요. # glance 사용자 추가 $ source ~/admin # 전에 생성했던 토큰 값을 적용합니다. $ openstack user create --domain default --project service --password qwer1234 glance # glance 게정을 추가합니다. $ openstack role add --project service --user glance admin # glance에 admin의 권한을 부여합니다. $ openstack service create --name glance --description \u0026#34;OpenStack Image service\u0026#34; image # glance 서비스 엔트리를 생성합니다. $ export controller=10.10.10.10 $ openstack endpoint create --region RegionOne image public http://$controller:9292 $ openstack endpoint create --region RegionOne image internal http://$controller:9292 $ openstack endpoint create --region RegionOne image admin http://$controller:9292 # glance 서비스의 endpoint를 추가합니다 ( public, internal, admin ) $ openstack user list +----------------------------------+--------+ | ID | Name | +----------------------------------+--------+ | bd36365f2459468a9c480cb48bab3ac0 | glance | | e19db9d5ec2c4c30b7a85d18b8b0e589 | admin | +----------------------------------+--------+ $ openstack endpoint list +----------------------------------+-----------+--------------+--------------+---------+-----------+-----------------------------+ | ID | Region | Service Name | Service Type | Enabled | Interface | URL | +----------------------------------+-----------+--------------+--------------+---------+-----------+-----------------------------+ | 00b38774cef048ee9950eda6938accc3 | RegionOne | keystone | identity | True | public | http://10.10.10.10:5000/v3/ | | 4591b06391374fe888380fa23b8f5121 | RegionOne | glance | image | True | admin | http://10.10.10.10:9292 | | 53dd31fecf2d44949c141149a13c673b | RegionOne | keystone | identity | True | admin | http://10.10.10.10:5000/v3/ | | 555f3d900f7e416bb783120f7ce74fe8 | RegionOne | glance | image | True | internal | http://10.10.10.10:9292 | | 5b3ac620bb7d4d9aabdf0f33229ee346 | RegionOne | glance | image | True | public | http://10.10.10.10:9292 | | bdd7df7c8cba46f6ada2c12155a9f1d6 | RegionOne | keystone | identity | True | internal | http://10.10.10.10:5000/v3/ | +----------------------------------+-----------+--------------+--------------+---------+-----------+-----------------------------+ Glance DB 생성 $ mysql -u root -p MariaDB [(none)]\u0026gt; create database glance; MariaDB [(none)]\u0026gt; grant all privileges on glance.* to glance@\u0026#39;localhost\u0026#39; identified by \u0026#39;pw\u0026#39;; MariaDB [(none)]\u0026gt; grant all privileges on glance.* to glance@\u0026#39;%\u0026#39; identified by \u0026#39;pw\u0026#39;; MariaDB [(none)]\u0026gt; flush privileges; # 구성을 위한 db를 생성합니다. ( 저는 편의를 위해 모든 pw로 qwer1234 설정하였습니다. ) # glance 패키지 설치 및 수정 $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-glance # glance 패키지를 설치합니다. $ vi /etc/glance/glance-api.conf [DEFAULT] bind_host = 0.0.0.0 [glance_store] stores = file,http default_store = file filesystem_store_datadir = /var/lib/glance/images/ # 이미지 경로 지정 [database] # database 연동 connection = mysql+pymysql://glance:qwer1234@controller/glance [keystone_authtoken] # keystone 인증 www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = glance password = qwer1234 [paste_deploy] flavor = keystone # glance.conf를 수정합니다. $ su -s /bin/bash glance -c \u0026#34;glance-manage db_sync\u0026#34; # glance db를 동기화 시킵니다. $ systemctl start openstack-glance-api $ systemctl enable openstack-glance-api # glance를 시작 및 실행시 자동시작을 등록합니다. $ setsebool -P glance_api_can_network on $ firewall-cmd --add-port=9292/tcp --permanent $ firewall-cmd --reload # Selinux 및 firewall을 설정합니다. # 확인을 위한 이미지 생성 $ wget http://download.cirros-cloud.net/0.5.1/cirros-0.5.1-x86_64-disk.img # 확인을 위해 cirros 이미지를 다운 받습니다. $ openstack image create \u0026#34;Cirros\u0026#34; --file cirros-0.5.1-x86_64-disk.img --disk-format qcow2 # image 등록 $ openstack image list +--------------------------------------+--------+--------+ | ID | Name | Status | +--------------------------------------+--------+--------+ | 38e15009-022b-49ce-bcdf-b220eb3c5b12 | Cirros | active | +--------------------------------------+--------+--------+ # 확인 # # 5. Nova ( 컴퓨트 서비스 ) 구성 # Nova 서비스는 controller 노드와 compute노드에 구성됩니다. 설치는 contoller \u0026gt; compute 순으로 진행하도록 하겠습니다. Nova에 대한 설명은 Nova을 참조해주세요. # Nova, Placement 추가 및 등록 $ source ~/admin $ openstack user create --domain default --project service --password qwer1234 nova $ openstack role add --project service --user nova admin $ openstack user create --domain default --project service --password qwer1234 placement $ openstack role add --project service --user placement admin # nova 유저와 placement유저를 생성합니다. $ openstack service create --name nova --description \u0026#34;OpenStack Compute Service\u0026#34; compute # nova 서버 엔트리 저장 $ openstack service create --name placement --description \u0026#34;OpenStack Compute Placement Service\u0026#34; placement # placement 서버 엔트리 저장 $ openstack user list # 확인 +----------------------------------+-----------+ | ID | Name | +----------------------------------+-----------+ | 18bdf3e68a754aa182f93196a918ba65 | nova | | 18ff8b52493a408d9933596ed20cca9c | glance | | bfd0cf6d358e49bf88f183a463c689a2 | placement | | e19db9d5ec2c4c30b7a85d18b8b0e589 | admin | +----------------------------------+-----------+ $ export controller=10.10.10.10 $ openstack endpoint create --region RegionOne compute public http://$controller:8774/v2.1/%\\(tenant_id\\)s $ openstack endpoint create --region RegionOne compute internal http://$controller:8774/v2.1/%\\(tenant_id\\)s $ openstack endpoint create --region RegionOne compute admin http://$controller:8774/v2.1/%\\(tenant_id\\)s # nova 서비스의 endpoint를 추가합니다 $ openstack endpoint create --region RegionOne placement public http://$controller:8778 $ openstack endpoint create --region RegionOne placement internal http://$controller:8778 $ openstack endpoint create --region RegionOne placement admin http://$controller:8778 $ placement의 endpoint를 추가합니다. $ openstack endpoint list # 확인 -------+-----------+--------------------------------------------+ | ID | Region | Service Name | Service Type | Enabled | Interface | URL | +----------------------------------+-----------+--------------+--------------+---------+-----------+--------------------------------------------+ | 00b38774cef048ee9950eda6938accc3 | RegionOne | keystone | identity | True | public | http://10.10.10.10:5000/v3/ | | 04ca5fb6701348089777d68a68ca7cd2 | RegionOne | placement | placement | True | public | http://10.10.10.10:8778 | | 53ad55ce8897463b86ea616a8ba64d95 | RegionOne | glance | image | True | public | http://10.10.10.10:9292 | | 53dd31fecf2d44949c141149a13c673b | RegionOne | keystone | identity | True | admin | http://10.10.10.10:5000/v3/ | | 595a2045543b42c2bb6f23e2dd30a3bb | RegionOne | glance | image | True | internal | http://10.10.10.10:9292 | | 6820b49138d54b63ac34cd52f1be08f6 | RegionOne | placement | placement | True | internal | http://10.10.10.10:8778 | | 6ad740445fca4a0fb684d913909fe129 | RegionOne | nova | compute | True | admin | http://10.10.10.10:8774/v2.1/%(tenant_id)s | | 9863826e093943cf97a05dfc6e3c159a | RegionOne | nova | compute | True | internal | http://10.10.10.10:8774/v2.1/%(tenant_id)s | | b9f9701a57ec40e487ce493a63903cae | RegionOne | placement | placement | True | admin | http://10.10.10.10:8778 | | bd787b85b3124f0ab15854998624cb19 | RegionOne | nova | compute | True | public | http://10.10.10.10:8774/v2.1/%(tenant_id)s | | bdd7df7c8cba46f6ada2c12155a9f1d6 | RegionOne | keystone | identity | True | internal | http://10.10.10.10:5000/v3/ | | d394eaf13ac840b3b2e69e074c2c1c20 | RegionOne | glance | image | True | admin | http://10.10.10.10:9292 | +----------------------------------+-----------+--------------+--------------+---------+-----------+--------------------------------------------+ # Nova DB 생성 $ mysql -u root -p MariaDB [(none)]\u0026gt; create database nova; MariaDB [(none)]\u0026gt; grant all privileges on nova.* to nova@\u0026#39;localhost\u0026#39; identified by \u0026#39;pw\u0026#39;; MariaDB [(none)]\u0026gt; grant all privileges on nova.* to nova@\u0026#39;%\u0026#39; identified by \u0026#39;pw\u0026#39;; MariaDB [(none)]\u0026gt; create database nova_api; MariaDB [(none)]\u0026gt; grant all privileges on nova_api.* to nova@\u0026#39;localhost\u0026#39; identified by \u0026#39;pw\u0026#39;; MariaDB [(none)]\u0026gt; grant all privileges on nova_api.* to nova@\u0026#39;%\u0026#39; identified by \u0026#39;pw\u0026#39;; MariaDB [(none)]\u0026gt; create database nova_placement; MariaDB [(none)]\u0026gt; grant all privileges on nova_placement.* to nova@\u0026#39;localhost\u0026#39; identified by \u0026#39;pw\u0026#39;; MariaDB [(none)]\u0026gt; grant all privileges on nova_placement.* to nova@\u0026#39;%\u0026#39; identified by \u0026#39;pw\u0026#39;; MariaDB [(none)]\u0026gt; create database nova_cell0; MariaDB [(none)]\u0026gt; grant all privileges on nova_cell0.* to nova@\u0026#39;localhost\u0026#39; identified by \u0026#39;pw\u0026#39;; MariaDB [(none)]\u0026gt; grant all privileges on nova_cell0.* to nova@\u0026#39;%\u0026#39; identified by \u0026#39;pw\u0026#39;; MariaDB [(none)]\u0026gt; flush privileges; # nova 구성을 위한 db를 생성합니다. ( 저는 편의를 위해 모든 pw로 qwer1234 설정하였습니다. ) # nova 서비스를 설치 및 수정합니다. $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-nova # nova 패키지를 설치합니다. $ vi /etc/nova/nova.conf [DEFAULT] my_ip = 10.10.10.10 state_path = /var/lib/nova enabled_apis = osapi_compute,metadata log_dir = /var/log/nova [api] auth_strategy = keystone [glance] api_servers = http://controller:9292 [oslo_concurrency] lock_path = $state_path/tmp [api_database] connection = mysql+pymysql://nova:qwer1234@controller/nova_api [database] connection = mysql+pymysql://nova:qwer1234@controller/nova [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = nova password = qwer1234 [placement] auth_url = http://controller:5000 os_region_name = RegionOne auth_type = password project_domain_name = default user_domain_name = default project_name = service username = placement password = qwer1234 [placement_database] connection = mysql+pymysql://nova:qwer1234@controller/nova_placement [wsgi] api_paste_config = /etc/nova/api-paste.ini # nova의 설정 파일을 수정합니다. # Selinux 및 firewalld을 설정합니다. $ yum --enablerepo=centos-openstack-stein -y install openstack-selinux $ semanage port -a -t http_port_t -p tcp 8778 $ firewall-cmd --add-port={6080/tcp,6081/tcp,6082/tcp,8774/tcp,8775/tcp,8778/tcp} --permanent $ firewall-cmd --reload # nova 서비스를 DB에 저장합니다. $ su -s /bin/bash nova -c \u0026#34;nova-manage api_db sync\u0026#34; $ su -s /bin/bash nova -c \u0026#34;nova-manage cell_v2 map_cell0\u0026#34; $ su -s /bin/bash nova -c \u0026#34;nova-manage db sync\u0026#34; $ su -s /bin/bash nova -c \u0026#34;nova-manage cell_v2 create_cell --name cell1\u0026#34; # nova 서비스를 시작 및 자동시작을 설정합니다. $ systemctl restart httpd $ chown nova. /var/log/nova/nova-placement-api.log $ for service in api consoleauth conductor scheduler novncproxy; do systemctl start openstack-nova-$service systemctl enable openstack-nova-$service done 이상으로 controller 노드에서의 구성을 마치겠습니다. 하단부터의 패키지 설치는 compute노드에서 진행해주세요 # # Stein 레포지터리를 활성화합니다. $ yum -y install centos-release-openstack-stein $ sed -i -e \u0026#34;s/enabled=1/enabled=0/g\u0026#34; /etc/yum.repos.d/CentOS-OpenStack-stein.repo # stein 패캐지를 등록합니다. # KVM 하이퍼바이저를 구성합니다. $ yum -y install qemu-kvm libvirt virt-install bridge-utils # KVM 구성에 필요한 가상화 및 네트워크 도구들을 설치합니다. $ lsmod | grep kvm # 확인 $ systemctl start libvirtd $ systenctk ebable libvirtd # compute 노드에 nova 서비스를 설치 및 수정합니다. $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-nova # nova 패키지를 설치합니다. $ vi /etc/nova/nova.conf [DEFAULT] my_ip = 10.10.10.30 state_path = /var/lib/nova enabled_apis = osapi_compute,metadata log_dir = /var/log/nova transport_url = rabbit://openstack:qwer1234@controller [api] auth_strategy = keystone [vnc] enabled = True server_listen = 0.0.0.0 server_proxyclient_address = 192.168.10.102 novncproxy_base_url = http://192.168.10.102/vnc_auto.html # vnc 화면으르 활성화 합니다. 추후 오픈스택 대시보드 혹은 vnc 클라이언트 프로그램으로 접속할 때 사용합니다. [glance] api_servers = http://controller:9292 [oslo_concurrency] lock_path = $state_path/tmp [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = nova password = qwer1234 [placement] auth_url = http://controller:5000 os_region_name = RegionOne auth_type = password project_domain_name = default user_domain_name = default project_name = service username = placement password = qwer1234 [wsgi] api_paste_config = /etc/nova/api-paste.ini # nova의 설정 파일을 수정합니다. # Selinux 및 firewall 설정 $ yum --enablerepo=centos-openstack-stein -y install openstack-selinux $ firewall-cmd --add-port=5900-5999/tcp --permanent $ firewall-cmd --reload # nova 서비스 시작 $ systemctl start openstack-nova-compute $ systemctl enable openstack-nova-compute \u0026amp; controller# openstack compute service list # 확인 +----+------------------+------------+----------+---------+-------+----------------------------+ | ID | Binary | Host | Zone | Status | State | Updated At | +----+------------------+------------+----------+---------+-------+----------------------------+ | 4 | nova-consoleauth | controller | internal | enabled | up | 2020-07-19T02:47:16.000000 | | 5 | nova-conductor | controller | internal | enabled | up | 2020-07-19T02:47:12.000000 | | 8 | nova-scheduler | controller | internal | enabled | up | 2020-07-19T02:47:12.000000 | | 9 | nova-compute | compute | nova | enabled | up | 2020-07-19T02:47:08.000000 | +----+------------------+------------+----------+---------+-------+----------------------------+ # # 6. Neutron ( 네트워크 서비스 ) 구성 # Neutron 서비스르르 구성하는 과정에서는 모든 노드에 설치가 진행됩니다. 기본적으로 openvswithch를 중심으로 진행하며, 경우에 따라서는 linuxbridge로 서비스를 대체하는 것이 가능합니다. 설치 과정은 controller, compute, network 노드 순으로 진행하겠습니다. Neutron에 대한 설명은 Neutron을 참조해주세요. # Neutron 사용자 추가 $ openstack user create --domain default --project service --password qwer1234 neutron $ openstack role add --project service --user neutron admin $ openstack service create --name neutron --description \u0026#34;OpenStack Networking service\u0026#34; network # Netutron 사용자를 추가 및 서비스를 등록합니다. $ export controller=10.10.10.10 $ openstack endpoint create --region RegionOne network public http://$controller:9696 $ openstack endpoint create --region RegionOne network internal http://$controller:9696 $ openstack endpoint create --region RegionOne network admin http://$controller:9696 # neutron의 endpoint를 생성합니다. # Neutron DB 생성 $ mysql -u root -p MariaDB [(none)]\u0026gt; create database neutron_ml2; MariaDB [(none)]\u0026gt; grant all privileges on neutron_ml2.* to neutron@\u0026#39;localhost\u0026#39; identified by \u0026#39;pw\u0026#39;; MariaDB [(none)]\u0026gt; grant all privileges on neutron_ml2.* to neutron@\u0026#39;%\u0026#39; identified by \u0026#39;pw\u0026#39;; MariaDB [(none)]\u0026gt; flush privileges; # neutron 구성을 위한 db를 생성합니다. ( 저는 편의를 위해 모든 pw로 qwer1234 설정하였습니다. ) # Neutron 설치 및 설정 $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-neutron openstack-neutron-ml2 # neutron 패키지 설치 $ vi /etc/neutron/neutron.conf [DEFAULT] core_plugin = ml2 service_plugins = router auth_strategy = keystone stae_path = /var/lib/neutron dhcp_agent_notification = True allow_overlapping_ips = True notify_nova_on_port_status_changes = True notify_nova_on_port_data_changes = True transport_url = rabbit://openstack:qwer1234@controller [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = neutron password = qwer1234 [database] connection = mysql+pymysql://neutron:qwer1234@controller/neutron_ml2 [nova] auth_url = http://controller:5000 auth_type = password project_domain_name = default user_domain_name = default region_name = RegionOne project_name = service username = nova password = qwer1234 [oslo_concurrency] lock_path = $state_path/tmp $ vi /etc/neutron/metadata_agent.ini [DEFAULT] nova_metadata_host = controller metadata_proxy_shared_secret = metadata_secret memcache_servers = controller:11211 # metadata_agent.ini 파일을 수정합니다. $ vi /etc/neutron/plugins/ml2/ml2_conf.ini [ml2] type_drivers = flat,vlan,gre,vxlan tenant_network_types = mechanism_drivers = openvswitch extension_drivers = port_security # ml2_conf.ini 파일에 설정을 수정합니다. # 이어 nova.conf 파일에 설정을 추가합니다. $ vi /etc/nova/nova.conf [DEFAULT] ... use_neutron = True linuxnet_interface_driver = nova.network.linux_net.LinuxOVSInterfaceDriver firewall_driver = nova.virt.firewall.NoopFirewallDriver [neutron] auth_url = http://controller:5000 auth_type = password project_domain_name = default user_domain_name = default region_name = RegionOne project_name = service username = neutron password = qwer1234 service_metadata_proxy = True metadata_proxy_shared_secret = metadata_secret # Selinux 및 방화벽 설정 $ yum --enablerepo=centos-openstack-stein -y install openstack-selinux $ setsebool -P neutron_can_network on $ setsebool -P daemons_enable_cluster_mode on $ firewall-cmd --add-port=9696/tcp --permanent $ firewall-cmd --reload # Selinux 및 방화벽을 설정합니다. # Neutron DB를 생성 및 서비스를 시작합니다. $ ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini $ su -s /bin/bash neutron -c \u0026#34;neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugin.ini upgrade head\u0026#34; # Neutron DB를 생성합니다. $ systemctl start neutron-server neutron-metadata-agent $ systemctl enable neutron-server neutron-metadata-agent $ systemctl restart openstack-nova-api # # 이제 다음으로는 network 노드에 구현해보도록 하겠습니다. $ yum -y install centos-release-openstack-stein $ sed -i -e \u0026#34;s/enabled=1/enabled=0/g\u0026#34; /etc/yum.repos.d/CentOS-OpenStack-stein.repo # stein 패캐지를 등록합니다. $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-neutron openstack-neutron-ml2 openstack-neutron-openvswitch libibverbs # neutron 패키지를 설치합니다. # neutron 설정합니다. $ vi /etc/neutron/neutron.conf [DEFAULT] core_plugin = ml2 service_plugins = router auth_strategy = keystone stae_path = /var/lib/neutron allow_overlapping_ips = True transport_url = rabbit://openstack:qwer1234@controller [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = neutron password = qwer1234 [oslo_concurrency] lock_path = $state_path/tmp $ vi /etc/neutron/l3_agent.ini [DEFAULT] ... interface_driver = openvswitch # l3_agent.ini 파일을 수정합니다. $ vi /etc/neutron/dhcp_agent.ini [DEFAULT] ... interface_driver = openvswitch dhcp_driver = neutron.agent.linux.dhcp.Dnsmasq enable_isolated_metadata = true # dhcp_agent.ini 파일을 수정합니다. $ vi /etc/neutron/metadata_agent.ini [DEFAULT] nova_metadata_host = controller metadata_proxy_shared_secret = metadata_secret # metadata_agent.ini 파일을 수정합니다. $ vi /etc/neutron/plugins/ml2/ml2_conf.ini [ml2] type_drivers = flat,vlan,gre,vxlan tenant_network_types = mechanism_drivers = openvswitch extension_drivers = port_security # ml2_conf.ini 파일에 설정을 수정합니다. $ vi /etc/neutron/plugins/ml2/openvswitch_agent.ini [securitygroup] firewall_driver = openvswitch enable_security_group = true enable_ipset = true # openvswitch_agent.ini 파일의 하단에 추가합니다. # Selinux 및 방화벽 설정 $ yum --enablerepo=centos-openstack-stein -y install openstack-selinux $ setsebool -P neutron_can_network on $ setsebool -P haproxy_connect_any on $ setsebool -P daemons_enable_cluster_mode on $ vi my-ovsofctl.te # create new module my-ovsofctl 1.0; require { type neutron_t; class capability sys_rawio; } #============= neutron_t ============== allow neutron_t self:capability sys_rawio; $ checkmodule -m -M -o my-ovsofctl.mod my-ovsofctl.te $ semodule_package --outfile my-ovsofctl.pp --module my-ovsofctl.mod $ semodule -i my-ovsofctl.pp # Selinux 및 방화벽을 추가설정합니다. # 시스템을 재시작 합니다. $ ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini $ systemctl start openvswitch $ systemctl enable openvswitch $ ovs-vsctl add-br br-int $ systemctl restart openstack-nova-compute $ systemctl start neutron-openvswitch-agent $ systemctl enable neutron-openvswitch-agent # 이어서 compute 노드에서의 설정을 진행하겠습니다. $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-neutron openstack-neutron-ml2 openstack-neutron-openvswitch # neutron 패키지를 설치합니다. # neutron 설정합니다. $ vi /etc/neutron/neutron.conf [DEFAULT] core_plugin = ml2 service_plugins = router auth_strategy = keystone stae_path = /var/lib/neutron allow_overlapping_ips = True transport_url = rabbit://openstack:qwer1234@controller [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = neutron password = qwer1234 [oslo_concurrency] lock_path = $state_path/tmp $ vi /etc/neutron/plugins/ml2/ml2_conf.ini [ml2] type_drivers = flat,vlan,gre,vxlan tenant_network_types = mechanism_drivers = openvswitch extension_drivers = port_security # ml2_conf.ini 파일에 설정을 수정합니다. $ vi /etc/neutron/plugins/ml2/openvswitch_agent.ini [securitygroup] firewall_driver = openvswitch enable_security_group = true enable_ipset = true # openvswitch_agent.ini 파일의 하단에 추가합니다. # 이어서 Nova.conf 파일을 수정합니다. $ vi /etc/nova/nova.conf [DEFAULT] ... use_neutron = True linuxnet_interface_driver = nova.network.linux_net.LinuxOVSInterfaceDriver firewall_driver = nova.virt.firewall.NoopFirewallDriver vif_plugging_is_fatal = True vif_plugging_timeout = 300 [neutron] auth_url = http://controller:5000 auth_type = password project_domain_name = default user_domain_name = default region_name = RegionOne project_name = service username = neutron password = qwer1234 service_metadata_proxy = True metadata_proxy_shared_secret = metadata_secret # Selinux 및 방화벽 설정 $ yum --enablerepo=centos-openstack-stein -y install openstack-selinux $ setsebool -P neutron_can_network on $ setsebool -P haproxy_connect_any on $ setsebool -P daemons_enable_cluster_mode on $ vi my-ovsofctl.te # create new module my-ovsofctl 1.0; require { type neutron_t; class capability sys_rawio; } #============= neutron_t ============== allow neutron_t self:capability sys_rawio; $ checkmodule -m -M -o my-ovsofctl.mod my-ovsofctl.te $ semodule_package --outfile my-ovsofctl.pp --module my-ovsofctl.mod $ semodule -i my-ovsofctl.pp # Selinux 및 방화벽을 추가설정합니다. # 시스템을 재시작 합니다. $ ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini $ systemctl start openvswitch $ systemctl enable openvswitch $ ovs-vsctl add-br br-int $ for service in dhcp-agent l3-agent metadata-agent openvswitch-agent; do systemctl start neutron-$service systemctl enable neutron-$service done # # 이제 이어 compute 노드에서 neutron 서비스를 설치하도록 하겠습니다. $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-neutron openstack-neutron-ml2 openstack-neutron-openvswitch # neutron 서비스를 설치합니다. $ vi /etc/neutron/neutron.conf [DEFAULT] core_plugin = ml2 service_plugins = router auth_strategy = keystone stae_path = /var/lib/neutron allow_overlapping_ips = True transport_url = rabbit://openstack:qwer1234@controller [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = neutron password = qwer1234 [oslo_concurrency] lock_path = $state_path/tmp $ vi /etc/neutron/plugins/ml2/ml2_conf.ini [ml2] type_drivers = flat,vlan,gre,vxlan tenant_network_types = mechanism_drivers = openvswitch extension_drivers = port_security $ vi /etc/neutron/plugins/ml2/openvswitch_agent.ini [securitygroup] firewall_driver = openvswitch enable_security_group = true enable_ipset = true # 이어서 nova.conf 파일을 수정합니다. $ vi /etc/nova/nova.conf [DEFAULT] ... use_neutron = True linuxnet_interface_driver = nova.network.linux_net.LinuxOVSInterfaceDriver firewall_driver = nova.virt.firewall.NoopFirewallDriver vif_plugging_is_fatal = True vif_plugging_timeout = 300 [neutron] auth_url = http://controller:5000 auth_type = password project_domain_name = default user_domain_name = default region_name = RegionOne project_name = service username = neutron password = qwer1234 service_metadata_proxy = True metadata_proxy_shared_secret = metadata_secret # Selinux 및 방화벽 설정 $ yum --enablerepo=centos-openstack-stein -y install openstack-selinux $ setsebool -P neutron_can_network on $ setsebool -P haproxy_connect_any on $ setsebool -P daemons_enable_cluster_mode on $ vi my-ovsofctl.te # create new module my-ovsofctl 1.0; require { type neutron_t; class capability sys_rawio; } #============= neutron_t ============== allow neutron_t self:capability sys_rawio; $ checkmodule -m -M -o my-ovsofctl.mod my-ovsofctl.te $ semodule_package --outfile my-ovsofctl.pp --module my-ovsofctl.mod $ semodule -i my-ovsofctl.pp # Selinux 및 방화벽을 추가설정합니다. # 서비스를 재시작 및 등록합니다. $ ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini $ systemctl start openvswitch $ systemctl enable openvswitch $ ovs-vsctl add-br br-int $ systemctl restart openstack-nova-compute $ systemctl start neutron-openvswitch-agent $ systemctl enable neutron-openvswitch-agent # # 이제 다음으로는 본격적으로 neutron 네트워크를 구현해보도록 하겠습니다. 먼저 controller 노드에서 ml2_conf 파일을 수정 및 추가합니다. 위에서 tenant 타입을 비워둔 이유는, 타입에 따라 사용하는 네트워크 구조가 달라지기 때문입니다. 여기서는 vxlan을 사용해 구성해보도록 하겠습니다. $ vi /etc/neutron/plugins/ml2/ml2_conf.ini [ml2] tenant_network_types = vxlan [ml2_type_flat] flat_networks = physnet1 [ml2_type_vxlan] vni_ranges = 1:1000 # ml2.conf 파일을 수정합니다 $ systemctl restart neutron-server # neutron 서비스를 재시작 합니다. # 이제 Network 노드에서의 설치를 진행해보도록 하겠습니다. # $ ovs-vsctl add-br br-eth1 $ ovs-vsctl add-port br-eth1 ens33 # 네트워크 브릿지를 생성하고, 네트워크 노드의 외부대역의 인터페이스 번호를 바인딩합니다. # neutron 서비스 사용을 위한 설정을 진행합니다. $ vi /etc/neutron/plugins/ml2/ml2_conf.ini [ml2] type_drivers = flat,vlan,gre,vxlan tenant_network_types = vxlan mechanism_drivers = openvswitch extension_drivers = port_security [ml2_type_flat] flat_networks = physnet1 [ml2_type_vxlan] vni_ranges = 1:1000 # ml2_conf.ini 파일에 설정을 추가 설정합니다. $ vi /etc/neutron/plugins/ml2/openvswitch_agent.ini [agent] tunnel_type = vxlan prevent_arp_spoofing = True [ovs] local_ip = 10.10.10.20 bridge_mappings = physnet1:br-eth1 # openvswitch_agent.ini 파일의 하단에 추가합니다. $ for service in dhcp-agent l3-agent metadata-agent openvswitch-agent; do systemctl restart neutron-$service done # neutron 서비스를 재시작합니다. $ systemctl stop firewalld $ systemctl disable firewalld # 방화벽을 해제합니다. # 바인딩 오류를 해결하기 위해 설정을 진행합니다. $ vi /etc/sysconfig/network-scripts/ifcfg-ens33 TYPE=Ethernet BOOTPROTO=static DEFROUTE=yes NAME=ens33 DEVICE=ens33 ONBOOT=yes $ vi /var/tmp/create_interface.sh #!/bin/bash ip link set up br-eth1 ip addr add 192.168.10.101/24 dev br-eth1 route add default gw 192.168.10.2 dev br-eth1 echo \u0026#34;nameserver 8.8.8.8\u0026#34; \u0026gt; /etc/resolv.conf $ chmod 755 /var/tmp/create_interface.sh $ vi /etc/systemd/system/set_interface.service [Unit] Description=Description for sample script goes here After=network.target [Service] Type=simple ExecStart=/var/tmp/create_interface.sh TimeoutStartSec=0 [Install] WantedBy=default.target $ systemctl enable set_interface $ init 6 # 이어 compute 노드에서의 설정을 진행합니다. $ vi /etc/neutron/plugins/ml2/ml2_conf.ini [ml2] type_drivers = flat,vlan,gre,vxlan tenant_network_types = vxlan mechanism_drivers = openvswitch extension_drivers = port_security [ml2_type_flat] flat_networks = physnet1 [ml2_type_vxlan] vni_ranges = 1:1000 # ml2_conf.ini 파일에 설정을 추가 설정합니다. $ vi /etc/neutron/plugins/ml2/openvswitch_agent.ini [agent] tunnel_type = vxlan prevent_arp_spoofing = True [ovs] local_ip = 10.10.10.30 # openvswitch_agent.ini 파일의 하단에 추가합니다. $ for service in dhcp-agent l3-agent metadata-agent openvswitch-agent; do systemctl restart neutron-$service done # neutron 서비스를 재시작합니다. $ systemctl stop firewalld $ systemctl disable firewalld # 방화벽을 해제합니다. # 확인 $ openstack network agent list +--------------------------------------+--------------------+------------+-------------------+-------+-------+---------------------------+ | ID | Agent Type | Host | Availability Zone | Alive | State | Binary | +--------------------------------------+--------------------+------------+-------------------+-------+-------+---------------------------+ | 261bbd8f-ece9-4818-91c3-be75b928fa54 | Open vSwitch agent | network | None | :-) | UP | neutron-openvswitch-agent | | 26376b7b-e4d0-413c-85b9-521994c41bf6 | Open vSwitch agent | compute | None | :-) | UP | neutron-openvswitch-agent | | 8b520189-c500-47ec-b330-b84bc0a3b622 | Metadata agent | controller | None | :-) | UP | neutron-metadata-agent | | ba443e32-a931-465f-acff-05621dac0424 | Metadata agent | network | None | :-) | UP | neutron-metadata-agent | | be878ec2-b8c9-4923-8d01-111d7c11c8f1 | L3 agent | network | nova | :-) | UP | neutron-l3-agent | | cb74c09d-7ec5-4457-a384-8303235adc97 | DHCP agent | network | nova | :-) | UP | neutron-dhcp-agent | +--------------------------------------+--------------------+------------+-------------------+-------+-------+---------------------------+ $ openstack router create router01 $ openstack network create int --provider-network-type vxlan $ openstack subnet create int_sub --network int \\ --subnet-range 1.1.1.0/24 --gateway 1.1.1.2 \\ --dns-nameserver 8.8.8.8 # 라우터와 내부대역을 생성합니다. $ openstack router add subnet router01 int_sub # 라우터와 내부대벽을 연결시킵니다. $ openstack network create \\ --provider-physical-network physnet1 \\ --provider-network-type flat --external ext $ openstack subnet create subnet2 \\ --network ext_net --subnet-range 192.168.10.0/24 \\ --allocation-pool start=192.168.10.150,end=192.168.10.200 \\ --gateway 192.168.10.2 --dns-nameserver 8.8.8.8 # 외부대역을 생성합니다. 외부대역의 IP는 바인딩한 br-eth1의 IP 대역과 동일해야합니다. $ openstack router set router01 --external-gateway ext # 생성한 라우터의 게이트웨이를 생성한 외부대역에 바운딩시킵니다. $ openstack network list +--------------------------------------+------+--------------------------------------+ | ID | Name | Subnets | +--------------------------------------+------+--------------------------------------+ | 2875f833-2d46-4740-bdd4-09c75c53e2b1 | int | 698d35ae-8d7c-436f-be1b-fcf4319eb5fe | | 4a25933d-ed21-4a5c-a87b-4e782e93c14c | ext | 47b0ee11-b628-4260-9185-71d1dab401ea | +--------------------------------------+------+--------------------------------------+ $ openstack subnet list +--------------------------------------+---------+--------------------------------------+-----------------+ | ID | Name | Network | Subnet | +--------------------------------------+---------+--------------------------------------+-----------------+ | 47b0ee11-b628-4260-9185-71d1dab401ea | ext-sub | 4a25933d-ed21-4a5c-a87b-4e782e93c14c | 192.168.10.0/24 | | 698d35ae-8d7c-436f-be1b-fcf4319eb5fe | int-sub | 2875f833-2d46-4740-bdd4-09c75c53e2b1 | 1.1.1.0/24 | +--------------------------------------+---------+--------------------------------------+-----------------+ $ wget http://cloud-images.ubuntu.com/releases/18.04/release/ubuntu-18.04-server-cloudimg-amd64.img -P /var/kvm/images $ openstack image create \u0026#34;Ubuntu1804\u0026#34; --file /var/kvm/images/ubuntu-18.04-server-cloudimg-amd64.img --disk-format qcow2 --container-format bare --public # 이미지를 다운로드 및 등록합니다. $ openstack flavor create --ram 1024 --disk 10 --vcpus 1 m1.small # flavor를 생성합니다. $ ssh-keygen -q -N \u0026#34;\u0026#34; $ openstack keypair create --public-key ~/.ssh/id_rsa.pub mykey # keypair를 생성합니다. $ openstack floating ip create ext # floating ip를 생성합니다. $ openstack create server --image Ubuntu1804 --flavor m1.small --key mykey --network int Ubuntu $ openstack server add floating ip Ubuntu 192.168.10.170 # 인스턴스를 생성하고 floating ip를 추가합니다. $ openstack server list +--------------------------------------+--------+--------+-------------------------------+------------+----------+ | ID | Name | Status | Networks | Image | Flavor | +--------------------------------------+--------+--------+-------------------------------+------------+----------+ | 75fa0186-ab63-4aaa-a27c-3f2126e5d31d | Ubuntu | ACTIVE | int=1.1.1.248, 192.168.10.170 | Ubuntu1804 | m1.small | +--------------------------------------+--------+--------+-------------------------------+------------+----------+ $ openstack security group create open $ openstack security group rule create --protocol icmp --ingress open $ openstack security group rule create --protocol tcp --dst-port 22:22 open $ openstack security group rule create --protocol tcp --dst-port 80:80 open $ openstack server add security group Ubuntu open # 보안그룹을 생성하고 적용시킵니다. $ ssh ubuntu@192.168.10.170 $ ping 8.8.8.8 $ sudo apt -y install apache2 $ sudo service apache2 start # 본체 Host에서 접속해서 확인 이것으로 기본적인 openstack-stein 버전의 설치를 완료하였습니다. # # 7. Horizon ( 대시보드 서비스 ) 구성 # Horizon은 controller 노드에서 설치가 진행됩니다. 에 대한 설명은 Horizone을 참조해주세요. # Horizon 패키지 설치 $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-dashboard # Horizon 패키지를 설치합니다. # 대시보드를 설정합니다. $ vi /etc/openstack-dashboard/local_settings ALLOWED_HOSTS = [\u0026#39;*\u0026#39;] # 수정 OPENSTACK_API_VERSIONS = { \u0026#34;identity\u0026#34;: 3, \u0026#34;image\u0026#34;: 3, \u0026#34;volume\u0026#34;: 3, \u0026#34;compute\u0026#34;: 2, } # 주석 제거 및 수정 OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True # 주석 해제 및 수정 OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = \u0026#39;Default\u0026#39; # 주석 제거 CACHES = { \u0026#39;default\u0026#39;: { \u0026#39;BACKEND\u0026#39;: \u0026#39;django.core.cache.backends.memcached.MemcachedCache\u0026#39;, \u0026#39;LOCATION\u0026#39;: \u0026#39;127.0.0.1:11211\u0026#39;, }, } # 주석제거 OPENSTACK_HOST = \u0026#34;controller\u0026#34; # IP 변경 OPENSTACK_KEYSTONE_DEFAULT_ROLE = \u0026#34;member\u0026#34; # 수정 $ vi /etc/httpd/conf.d/openstack-dashboard.conf WSGIDaemonProcess dashboard WSGIProcessGroup dashboard WSGISocketPrefix run/wsgi WSGIApplicationGroup %{GLOBAL} # 추가 # Selinux 및 방화벽 설정 $ setsebool -P httpd_can_network_connect on $ firewall-cmd --add-service={http,https} --permanent $ firewall-cmd --reload $ systemctl restart httpd # ** DB 생성** $ mysql -u root -p MariaDB [(none)]\u0026gt; create database keystone; MariaDB [(none)]\u0026gt; grant all privileges on .* to @\u0026#39;localhost\u0026#39; identified by \u0026#39;pw\u0026#39;; MariaDB [(none)]\u0026gt; grant all privileges on .* to @\u0026#39;%\u0026#39; identified by \u0026#39;pw\u0026#39;; MariaDB [(none)]\u0026gt; flush privileges; # 구성을 위한 db를 생성합니다. ( 저는 편의를 위해 모든 pw로 qwer1234 설정하였습니다. ) # # 8. Cinder ( 오브젝트 스토리지 및 블록 스토리지 구성 ) # Cinder는 기본적으로 독립적으로 storage 노드를 구성하거나 혹은 compute 노드에 추가하여 사용합니다. 여기서는 compute 노드에 포함하여 구성하도록 하겠습니다/ 구성 순서는 controller \u0026gt; compute 노드 순으로 진행하겠습니다. Cinder에 대한 설명은 Cinder을 참조해주세요. # Cinder 서비스 등록 $ source ~/admin $ openstack user create --domain default --project service --password qwer1234 cinder $ openstack role add --project service --user cinder admin $ openstack service create --name cinderv3 --description \u0026#34;OpenStack Block service\u0026#34; volumev3 # cinder 사용자를 추가 및 서비스를 등록합니다. $ export controller=10.10.10.10 $ openstack endpoint create --region RegionOne volumev3 public http://$controller:8776/v3/%\\(tenant_id\\)s $ openstack endpoint create --region RegionOne volumev3 internal http://$controller:8776/v3/%\\(tenant_id\\)s $ openstack endpoint create --region RegionOne volumev3 admin http://$controller:8776/v3/%\\(tenant_id\\)s # cinder의 endpoint를 생성합니다. # Cinder DB 생성 $ mysql -u root -p MariaDB [(none)]\u0026gt; create database cinder; MariaDB [(none)]\u0026gt; grant all privileges on cinder.* to cinder@\u0026#39;localhost\u0026#39; identified by \u0026#39;pw\u0026#39;; MariaDB [(none)]\u0026gt; grant all privileges on cinder.* to cinder@\u0026#39;%\u0026#39; identified by \u0026#39;pw\u0026#39;; MariaDB [(none)]\u0026gt; flush privileges; # cinder 구성을 위한 db를 생성합니다. ( 저는 편의를 위해 모든 pw로 qwer1234 설정하였습니다. ) # cinder 패키지 설치 및 수정 $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-cinder $ vi /etc/cinder/cinder.conf [DEFAULT] my_ip = 10.10.10.10 log_dir = /var/log/cinder state_path = /var/lib/cinder auth_strategy = keystone transport_url = rabbit://openstack:qwer1234@controller enable_v3_api = True [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = cinder password = qwer1234 [database] connection = mysql+pymysql://cinder:qwer1234@controller/cinder [oslo_concurrency] lock_path = $state_path/tmp # cinder.conf 파일을 수정합니다. $ su -s /bin/bash cinder -c \u0026#34;cinder-manage db sync\u0026#34; # cinder db를 동기화시킵니다. $ systemctl start openstack-cinder-api openstack-cinder-scheduler $ systemctl enable openstack-cinder-api openstack-cinder-scheduler # cinder 시작 및 자동시작을 등록합니다. $ echo \u0026#34;export OS_VOLUME_API_VERSION=3\u0026#34; \u0026gt;\u0026gt; ~/admin $ source ~/admin # 볼륨 버전을 API 3로 지정합니다. $ firewall-cmd --add-port=8776/tcp --permanent $ firewall-cmd --reload # # 이어서 compute 노드에 설치를 진행하겠습니다. cinder 패키지 설치 및 수정 $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-cinder python2-crypto targetcli $ vi /etc/cinder/cinder.conf [DEFAULT] my_ip = 10.10.10.30 log_dir = /var/log/cinder state_path = /var/lib/cinder auth_strategy = keystone transport_url = rabbit://openstack:qwer1234@controller glance_api_servers = http://controller:9292 enable_v3_api = True [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = cinder password = qwer1234 [database] connection = mysql+pymysql://cinder:qwer1234@controller/cinder [oslo_concurrency] lock_path = $state_path/tmp # cinder.conf 파일을 수정합니다. $ systemctl start openstack-cinder-volume $ systemctl enable openstack-cinder-volume # cinder 서비스를 시작 및 자동시작을 등록합니다. $ controller $openstack volume service list # 확인 +------------------+------------+------+---------+-------+----------------------------+ | Binary | Host | Zone | Status | State | Updated At | +------------------+------------+------+---------+-------+----------------------------+ | cinder-scheduler | controller | nova | enabled | up | 2020-07-20T04:02:31.000000 | +------------------+------------+------+---------+-------+----------------------------+ # # 8-2. LVM으로 블록 스토리지 백엔드 구성 # # compute 노드에 cinder 서비스를 설치한 것에 이어 LVM 백엔드를 설정해보도록 하겠습니다. VG 생성 참조 $ fdisk /dev/sd[ n ] # 만약 디스크 파티션이 없으시면 새로 생성 후 등록합니다. # 저는 간단하게 100G 하드를 추가한 후, cinder 이름으로 vg를 생성하였습니다. $ vi /etc/cinder/cinder.conf [DEFAULT] ... enabled_backends = lvm [lvm] target_helper = lioadm target_protocol = iscsi target_ip_address = 10.10.10.30 volume_group = cinder volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver volume_dir = $state_path/volumes # cinder.conf의 상단에 내용을 추가설정합니다. $ firewall-cmd --add-service=iscsi-target --permanent $ firewall-cmd --reload # 방화벽 설정을 추가합니다. $ systemctl restart openstack-cinder-volume # 서비스를 재시작합니다. # 이어서 compute 노드의 nova.conf 파일을 수정합니다. $ vi /etc/nova/nova.conf [cinder] os_region_name = RegionOne # nova.conf의 하단에 상단의 내용을 추가합니다. $ systemctl restart openstack-nova-compute # nova 서비스를 재시작합니다. $ controller $ openstack volume service list # 생성을 확인합니다. +------------------+-------------+------+---------+-------+----------------------------+ | Binary | Host | Zone | Status | State | Updated At | +------------------+-------------+------+---------+-------+----------------------------+ | cinder-scheduler | controller | nova | enabled | up | 2020-07-20T04:54:52.000000 | | cinder-volume | compute@lvm | nova | enabled | up | 2020-07-20T04:54:52.000000 | +------------------+-------------+------+---------+-------+----------------------------+ $ controller $ openstack volume cretae --size 1 test # 확인용 1G volume을 생성합니다. +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | attachments | [] | | availability_zone | nova | | bootable | false | | consistencygroup_id | None | | created_at | 2020-07-20T05:00:21.000000 | | description | None | | encrypted | False | | id | f09ee80f-3ec8-4eaf-a4a5-af13cccbd5ae | | migration_status | None | | multiattach | False | | name | test | | properties | | | replication_status | None | | size | 1 | | snapshot_id | None | | source_volid | None | | status | creating | | type | None | | updated_at | None | | user_id | 296ce49d1dc94931b62a726fb64712e9 | +---------------------+--------------------------------------+ $ openstack volume list # 생성한 volume을 확인합니다. +--------------------------------------+------+-----------+------+-------------+ | ID | Name | Status | Size | Attached to | +--------------------------------------+------+-----------+------+-------------+ | f09ee80f-3ec8-4eaf-a4a5-af13cccbd5ae | test | available | 1 | | +--------------------------------------+------+-----------+------+-------------+ # # 8-3. LBaaS 설치 # 로드밸런싱을 위해서는 LBaaS를 사용해야 합니다. LBaaS에 대해서는 LBaaS를 참조해주세요. $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-neutron-lbaas net-tools # LBaaS 서비스를 설치합니다. $ vi /etc/neutron/neutron.conf service_plugins = router,lbaasv2 # lbaasv2 서비스를 추가합니다. $ vi /etc/neutron/neutron_lbaas.conf [service_providers] service_provider = LOADBALANCERV2:Haproxy:neutron_lbaas.drivers.haproxy.plugin_driver.HaproxyOnHostPluginDriver:default $ vi /etc/neutron/lbaas_agent.ini [DEFAULT] interface_driver = openvswitch $ su -s /bin/bash neutron -c \u0026#34;neutron-db-manage --subproject neutron-lbaas --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugin.ini upgrade head\u0026#34; $ systemctl restart neutron-server # network 노드와 compute 노드는 동일하게 진행합니다. $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-neutron-lbaas haproxy net-tools $ vi /etc/neutron/neutron.conf service_plugins = router,lbaasv2 $ vi /etc/neutron/neutron_lbaas.conf [service_providers] service_provider = LOADBALANCERV2:Haproxy:neutron_lbaas.drivers.haproxy.plugin_driver.HaproxyOnHostPluginDriver:default $ vi /etc/neutron/lbaas_agent.ini [DEFAULT] interface_driver = openvswitch $ systemctl start neutron-lbaasv2-agent $ systemctl enable neutron-lbaasv2-agent # # 8-4. LFS, LVM 기반 다중 스토리지 노드 구성 # # # 9. Swift ( 오브젝트 스토리지 서비스 ) 구성 # Swift란 오브젝트 스토리지 서비스로, 흔히 우리가 생각하는 네이버 클라우드와 거의 동일한 맥락이라 할 수 있습니다. swift는 기본적으로 controller에 설치하나 여기서는 비교적 자원소모가 적은 network 노드에 proxy-sever를, compute 노드를 storage로 사용하여 설치하여 진행하겠습니다. swift에 대한 설명은 swift을 참조해주세요.** # swift 서비스 생성 controlloer 노드에는 swift 관련 패키지를 설치하지는 않지만 서비스의 관리를 위해 유저, 엔드포인트, url을 생성합니다. $ openstack user create --domain default --project service --password qwer1234 swift $ openstack role add --project service --user swift admin $ openstack service create --name swift --description \u0026#34;OpenStack Object Storage\u0026#34; object-store # swfit 유저를 생성하고 관리자의 권한을 부여합니다. $ export swift_proxy=10.10.10.20 $ openstack endpoint create --region RegionOne object-store public http://$swift_proxy:8080/v1/AUTH_%\\(tenant_id\\)s $ openstack endpoint create --region RegionOne object-store internal http://$swift_proxy:8080/v1/AUTH_%\\(tenant_id\\)s $ openstack endpoint create --region RegionOne object-store admin http://$swift_proxy:8080/v1/AUTH_%\\(tenant_id\\)s # swift의 endpoint를 등록합니다. 여기서 proxy 서버는 네트워크 노드를 등록합니다. # # 이어서 network 노드에서의 swift 설치 및 설정을 진행하겠습니다. $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-swift-proxy python-memcached openssh-clients # swift 서비스에 필요한 패키지를 설치합니다. $ vi /etc/swift/proxy-server.conf [filter:cache] use = egg:swift#memcache #memcache_servers = 127.0.0.1:11211 memcache_servers = controller:11211 [filter:authtoken] paste.filter_factory = keystonemiddleware.auth_token:filter_factory #admin_tenant_name = %SERVICE_TENANT_NAME% #admin_user = %SERVICE_USER% #admin_password = %SERVICE_PASSWORD% #admin_host = 127.0.0.1 #admin_port = 35357 #admin_protocol = http #admin_ /tmp/keystone-signing-swift # paste.filter_factory를 제외한 기존 정보는 주석처리 www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = swift password = qwer1234 delay_auth_decision = true # 위에 내용을 대신 주석 추가 # proxy-server.conf 파일을 수정합니다. # memcache_servers의 IP는 controller 노드의 IP로 수정합니다. $ vi /etc/swift/swift.conf [swift-hash] #swift_hash_path_suffix = %SWIFT_HASH_PATH_SUFFIX% swift_hash_path_suffix = swift_shared_path swift_hash_path_prefix = swift_shared_path # swift 서비스의 사용을 위해 account, container, object를 생성합니다. $ swift-ring-builder /etc/swift/account.builder create 12 1 1 $ swift-ring-builder /etc/swift/container.builder create 12 1 1 $ swift-ring-builder /etc/swift/object.builder create 12 1 1 # account, container, object를 생성합니다. # 12 = 한 클러스터 스토리지에서 생성 가능한 파티션의 수 # 1 = 오브젝트 복수 수 ( 스토리지의 개수 ) # 1 = 데이터 이동, 복제, 파티션 이동 등이 진행될 때 잠기는 최소 시간, 데이터 손실을 방지하기 위한 기능 $ swift-ring-builder /etc/swift/account.builder add r0z0-10.10.10.30:6202/device0 100 $ swift-ring-builder /etc/swift/container.builder add r0z0-10.10.10.30:6201/device0 100 $ swift-ring-builder /etc/swift/object.builder add r0z0-10.10.10.30:6200/device0 100 $ swift-ring-builder /etc/swift/account.builder rebalance $ swift-ring-builder /etc/swift/container.builder rebalance $ swift-ring-builder /etc/swift/object.builder rebalance # compute 노드의 builder에 region과 zone을 추가 후 반영시킵니다. # r = region, z = zone $ chown swift. /etc/swift/*.gz # swift 관련 파일의 소유권을 변경합니다. $ systemctl start openstack-swift-proxy $ systemctl enable openstack-swift-proxy # 프록시 서비스를 시작합니다. $ firewall-cmd --add-port=8080/tcp --permanent $ firewall-cmd --reload # 방화벽을 사용 중이라면 방화벽을 등록합니다. # # 이제 이어 storage를 구성하기 위해 compute 노드에서의 설치를 진행해보도록 하겠습니다. compute 노드는 이미 cinder 서비스가 동작하고 있어 기본적인 네트워크, 시간 설정, 레포지터리 지정 등은 구성이 마친 상태의 노드입니다. 만약 다른 노드에 구성하시거나 swift 서비스를 다중 노드로 구성하시는 경우 위와 같은 설정을 먼저 진행해주시길 바랍니다. 여기서는 swift 서비스를 위해 100G의 버츄얼 디스크( dev/sdc )를 추가하여 진행하였습니다. $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-swift-account openstack-swift-container openstack-swift-object xfsprogs rsync openssh-clients # swift 서비스를 설치합니다. $ scp root@network:/etc/swift/*.gz /etc/swift/ $ chown swift. /etc/swift/*.gz # network 노드에서의 설정파일을 복사옵니다. $ vi /etc/swift/swift.conf [swift-hash] #swift_hash_path_suffix = %SWIFT_HASH_PATH_SUFFIX% swift_hash_path_suffix = swift_shared_path swift_hash_path_prefix = swift_shared_path $ swift.conf 파일을 설정합니다. $ vi /etc/swift/account-server.conf bind-ip = 0.0.0.0 bind_port = 6202 $ vi /etc/swift/container-server.conf bind-ip = 0.0.0.0 bind_port = 6201 $ vi /etc/swift/object-server.conf bind-ip = 0.0.0.0 bind_port = 6200 $ vi /etc/rsyncd.conf pid file = /var/run/rsymcd.pid log file = /var/log/rsymcd.log uid = swift gid = swift address = compute [account] path = /srv/node read only = false write only = no list = yes incoming chmod = 0644 outgoing chmod = 0644 max connections = 25 lock file = /var/lock/account.lock [container] path = /srv/node read only = false write only = no list = yes incoming chmod = 0644 outgoing chmod = 0644 max connections = 25 lock file = /var/lock/container.lock [object] path = /srv/node read only = false write only = no list = yes incoming chmod = 0644 outgoing chmod = 0644 max connections = 25 lock file = /var/lock/object.lock [swift_server] path = /etc/swift read only = true write only = no list = yes incoming chmod = 0644 outgoing chmod = 0644 max connections = 5 lock file = /var/lock/swift_server.lock # swift 서비스관련 파일을 수정합니다. # compute 노드에서 disk 설정을 진행합니다. $ mkfs.xfs -i size=1024 -s size=4096 /dev/sdb1 meta-data=/dev/sdc1 isize=1024 agcount=4, agsize=6553536 blks = sectsz=4096 attr=2, projid32bit=1 = crc=1 finobt=0, sparse=0 data = bsize=4096 blocks=26214144, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0 ftype=1 log =internal log bsize=4096 blocks=12799, version=2 = sectsz=4096 sunit=1 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 # 디스크의 xfs의 유형으로 포맷시킵니다. $ mkdir -p /srv/node/device0 $ mount -o noatime,nodiratime,nobarrier /dev/sdc1 /srv/node/device0 $ chown -R swift. /srv/node # device0 디렉토리를 생성하고 해당 디렉토리에 sdb1 볼륨을 마운트시킨 후, swift로 소유권을 변경시킵니다. $ vi /etc/fstab /dev/sdc1 /srv/node/device0 xfs noatime,nodiratime,nobarrier 0 0 # 재부팅할 경우를 대비하여 생성한 볼륨을 fstab에 등록합니다. # # selinux 및 방화벽 관련 서비스를 설정합니다. $ semanage fcontext -a -t swift_data_t /srv/node/device0 $ restorecon /srv/node/device0 $ firewall-cmd --add-port={873/tcp,6200/tcp,6201/tcp,6202/tcp} --permanent $ firewall-cmd --reload # swift 관련 서비스를 재시작합니다. $ systemctl restart rsyncd openstack-swift-account-auditor openstack-swift-account-replicator openstack-swift-account openstack-swift-container-auditor openstack-swift-container-replicator openstack-swift-container-updater openstack-swift-container openstack-swift-object-auditor openstack-swift-object-replicator openstack-swift-object-updater openstack-swift-object $ systemctl enable rsyncd openstack-swift-account-auditor openstack-swift-account-replicator openstack-swift-account openstack-swift-container-auditor openstack-swift-container-replicator openstack-swift-container-updater openstack-swift-container openstack-swift-object-auditor openstack-swift-object-replicator openstack-swift-object-updater openstack-swift-object # 확인을 위해 controller 노드에 httpd를 재시작합니다. $ systemctl restart httpd # 대시보드 접속 후 프로젝트에서 오브젝트 스토리지가 메뉴에 있는 지를 확인합니다. $ openstack container create test +---------------------------------------+-----------+------------------------------------+ | account | container | x-trans-id | +---------------------------------------+-----------+------------------------------------+ | AUTH_2ac06290d2d943d5a768fe3daa53b118 | test | tx22f3dd125f134a189602c-005f24cef1 | +---------------------------------------+-----------+------------------------------------+ $ echo Hello \u0026gt; test.txt $ swift upload test test.txt $ swift list test $ swift list test test.txt # # 10. Heat ( Orchestration ) 설치 # # 클라우딩 컴퓨팅이 꽃인 Orchestaration 기능을 수행하는 Heat 서비스를 설치해보도록 하겠습니다. Heat 설치는 controller, network 노드 순으로 우리어집니다. Heat*에 대한 설명은 Heat을 참조해주세요. # Heat 서비스 생성 $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-heat-common python-heatclient # heat 서비스 관련 패키지를 다운로드 합니다. $ openstack user create --domain default --project service --password qwer1234 heat $ openstack role add --project service --user heat admin $ openstack role create heat_stack_owner $ openstack role create heat_stack_user $ openstack role add --project admin --user admin heat_stack_owner $ openstack service create --name heat --description \u0026#34;Openstack Orchestration\u0026#34; orchestration $ openstack service create --name heat-cfn --description \u0026#34;Openstack Orchestration\u0026#34; cloudformation # heat 유저를 생성하고 관리자의 권한을 부여합니다. $ export heat_api=10.10.10.20 $ openstack endpoint create --region RegionOne orchestration public http://$heat_api:8004/v1/AUTH_%\\(tenant_id\\)s $ openstack endpoint create --region RegionOne orchestration internal http://$heat_api:8004/v1/AUTH_%\\(tenant_id\\)s $ openstack endpoint create --region RegionOne orchestration admin http://$heat_api:8004/v1/AUTH_%\\(tenant_id\\)s $ openstack endpoint create --region RegionOne cloudformation public http://$heat_api:8000/v1 $ openstack endpoint create --region RegionOne cloudformation internal http://$heat_api:8000/v1 $ openstack endpoint create --region RegionOne cloudformation admin http://$heat_api:8000/v1 # heat 서비스의 endpoint를 등록합니다. 여기서 proxy 서버는 네트워크 노드를 등록합니다. $ openstack domain create --description \u0026#34;Stack projects and users\u0026#34; heat $ openstack user create --domain heat --password qwer1234 heat_domain_admin $ openstack role add --domain heat --user heat_domain_admin admin # heat domain을 생성하고 heat 유저에게 권한을 부여합니다. # heat의 DB를 생성합니다. $ mysql -u root -p MariaDB [(none)]\u0026gt; create database heat; MariaDB [(none)]\u0026gt; grant all privileges on heat.* to heat@\u0026#39;localhost\u0026#39; identified by \u0026#39;pw\u0026#39;; MariaDB [(none)]\u0026gt; grant all privileges on heat.* to keystone@\u0026#39;%\u0026#39; identified by \u0026#39;pw\u0026#39;; MariaDB [(none)]\u0026gt; flush privileges; # heat DB를 생성합니다. 여기서 pw는 qwer1234으로 모두 통일하였습니다. # 이어서 network 노드에서 heat 서비스를 설치해보겠습니다. $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-heat-api openstack-heat-api-cfn openstack-heat-engine python-heatclient # heat 서비스를 위한 패키지를 설치합니다. $ vi /etc/heat/heat.conf [DEFAULT] deferred_auth_method = trusts trusts_delegated_roles = heat_stack_owner # Heat installed server heat_metadata_server_url = http://network:8000 heat_waitcondition_server_url = http://network:8000/v1/waitcondition heat_watch_server_url = http://network:8003 heat_stack_user_role = heat_stack_user # Heat domain name stack_user_domain_name = heat # Heat domain admin name stack_domain_admin = heat_domain_admin # Heat domain admin\u0026#39;s password stack_domain_admin_password = qwer1234 # RabbitMQ connection info transport_url = rabbit://openstack:qwer1234@controller # MariaDB connection info [database] connection = mysql+pymysql://heat:qwer1234@controller/heat # Keystone auth info [clients_keystone] auth_uri = http://controller:5000 # Keystone auth info [ec2authtoken] auth_uri = http://controller:5000 [heat_api] bind_host = 0.0.0.0 bind_port = 8004 [heat_api_cfn] bind_host = 0.0.0.0 bind_port = 8000 # Keystone auth info [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = heat password = qwer1234 [trustee] auth_plugin = password auth_url = http://controller:5000 username = heat password = qwer1234 user_domain_name = default # heat.conf 파일을 수정합니다. $ su -s /bin/bash heat -c \u0026#34;heat-manage db_sync\u0026#34; $ systemctl start openstack-heat-api openstack-heat-api-cfn openstack-heat-engine $ systemctl enable openstack-heat-api openstack-heat-api-cfn openstack-heat-engine # DB의 데이터를 삽입하고, 서비스슬 등록합니다. # 방화벽을 사용중이면 방화벽을 설정합니다. $ firewall-cmd --add-port={8000/tcp,8004/tcp} --permanent $ firewall-cmd --reload # # $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-designate-api openstack-designate-central openstack-designate-worker openstack-designate-producer openstack-designate-mdns python-designateclient bind bind-utils # 서비스 관련 패키지를 설치합니다. $ rndc-confgen -a -k designate -c /etc/designate.key -r /dev/urandom $ chown named:designate /etc/designate.key $ chmod 640 /etc/designate.key # key를 생성합니다. $ vi /etc/named.conf # create new options { listen-on port 53 { any; }; listen-on-v6 port 53 { none; }; directory \u0026#34;/var/named\u0026#34;; dump-file \u0026#34;/var/named/data/cache_dump.db\u0026#34;; statistics-file \u0026#34;/var/named/data/named_stats.txt\u0026#34;; memstatistics-file \u0026#34;/var/named/data/named_mem_stats.txt\u0026#34;; # replace query range to your own environment allow-query { localhost; 10.10.10.0/24; }; allow-new-zones yes; request-ixfr no; recursion no; bindkeys-file \u0026#34;/etc/named.iscdlv.key\u0026#34;; managed-keys-directory \u0026#34;/var/named/dynamic\u0026#34;; pid-file \u0026#34;/run/named/named.pid\u0026#34;; session-keyfile \u0026#34;/run/named/session.key\u0026#34;; }; include \u0026#34;/etc/designate.key\u0026#34;; controls { inet 0.0.0.0 port 953 allow { localhost; } keys { \u0026#34;designate\u0026#34;; }; }; logging { channel default_debug { file \u0026#34;data/named.run\u0026#34;; severity dynamic; }; }; zone \u0026#34;.\u0026#34; IN { type hint; file \u0026#34;named.ca\u0026#34;; }; $ chown -R named. /var/named $ systemctl start named $ systemctl enable naemd $ vi /etc/designate/designate.conf [DEFAULT] log_dir = /var/log/designate transport_url = rabbit://openstack:qwer1234@controller root_helper = sudo designate-rootwrap /etc/designate/rootwrap.conf [database] connection = mysql+pymysql://heat:qwer1234@controller/heat [service:api] listen = 0.0.0.0:9001 auth_strategy = keystone api_base_uri = http://controller:9001 enable_api_v2 = True enabled_extensions_v2 = quotas, reports [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = heat password = qwer1234 [service:worker] enabled = True notify = True [storage:sqlalchemy] connection = mysql+pymysql://heat:qwer1234@controller/heat $ su -s /bin/sh -c \u0026#34;designate-manage database sync\u0026#34; designate $ systemctl start designate-central designate-api $ systemctl enable designate-central designate-api $ vi /etc/designate/pools.yaml # create new (replace hostname and IP address to your own environment) - name: default description: Default Pool attributes: {} ns_records: - hostname: network.srv.world. priority: 1 nameservers: - host: 10.10.10.20 port: 53 targets: - type: bind9 description: BIND9 Server masters: - host: 10.10.10.20 port: 5354 options: host: 10.10.10.20 port: 53 rndc_host: 10.10.10.20 rndc_port: 953 rndc_key_file: /etc/designate.key $ su -s /bin/sh -c \u0026#34;designate-manage pool update\u0026#34; designate Updating Pools Configuration $ systemctl start designate-worker designate-producer designate-mdns $ systemctl enable designate-worker designate-producer designate-mdns # 이어서 selinux와 방화벽을 설정합니다. $ setsebool -P named_write_master_zones on $ firewall-cmd --add-service=dns --permanent $ firewall-cmd --add-port={5354/tcp,9001/tcp} --permanent $ firewall-cmd --reload controller\u0026gt; $openstack dns service list # 확인 # # # 11. Openstack 대시보드 메인 로고 및 링크 변경 # # # 12. Neutron 기반 Service Functon Chaining ( SFC ) 기능 구성 # # # "},{"id":27,"href":"/erp/docs/OpenStack/OpenStackTraining/OpenStack-Ussuri-11/","title":"OpenStack Ussuri : Gnocch","section":"OpenStack Training","content":" OpenStack Ussuri : Gnocch # # ----------------------- ----------------------- ----------------------- | [ Controller Node ] | | [ Compute Node ] | | [ Network Node ] | | | | Libvirt | | Open vSwitch | | MariaDB RabbitMQ | | Nova compute | | L2 Agent | | Memcached Keystone | | Open vSwitch | | L3 Agent | | httpd Cinder API | | L2 Agent | | metadata agent | | Nova-API Compute | | Cinder-LVM | | Swift-proxy | | L2 agent L3 agent | | NFS | | Heat API | | metadata agent | ----------------------- | API-CFN | | Neutron Server | | Heat Engine | | Gnocchi | ----------------------- ----------------------- --------------------------------- | [ Storage Node 1, 2, 3 ] | | | | Swift-account-auditor | | Swift-account-replicator | | Swift-account | | Swift-container-auditor | | Swift-container-replicator | | Swift-container-updater | | Swift-container | | Swift-object-auditor | | Swift-object-replicator | | Swift-object-updater | | Swift-swift-object | --------------------------------- # OpenStack Ussuri : Gnnoch # # Gnnoch # # Gnocchi service 및 User 생성 # $ controller ~(keystone)\u0026gt; openstack user create --domain default --project service --password qwer1234 gnocchi +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | default_project_id | b470c69e28db47cdbfc81e06cc67f627 | | domain_id | default | | enabled | True | | id | 3217be4917454641994660bd1f3ea007 | | name | gnocchi | | options | {} | | password_expires_at | None | +---------------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack role add --project service --user gnocchi admin $ controller ~(keystone)\u0026gt; openstack service create --name gnocchi --description \u0026#34;Metric Service\u0026#34; metric -------------------------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | Metric Service | | enabled | True | | id | 6ac9ec31386f4291b582bd5b504ac485 | | name | gnocchi | | type | metric | +-------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne metric public http://controller:8041 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 12f4410ed82240b0b1340d48b0627612 | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | 6ac9ec31386f4291b582bd5b504ac485 | | service_name | gnocchi | | service_type | metric | | url | http://controller:8041 | +-------------------------------------------------+ $ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne metric internal http://controller:8041 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 70f43453f93b407e94d2dd11ddce7260 | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | 6ac9ec31386f4291b582bd5b504ac485 | | service_name | gnocchi | | service_type | metric | | url | http://controller:8041 | +--------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne metric admin http://controller:8041 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | bb9a955359fd4af18599913465f46958 | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | 6ac9ec31386f4291b582bd5b504ac485 | | service_name | gnocchi | | service_type | metric | | url | http://controller:8041 | +--------------+----------------------------------+ # Gnocchi 유저의 DB를 생성합니다. # # $ controller\u0026gt; mysql -u root -p $ MariaDB\u0026gt; create database gnocchi; $ MariaDB\u0026gt; grant all privileges on gnocchi.* to gnocchi@\u0026#39;localhost\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; grant all privileges on gnocchi.* to gnocchi@\u0026#39;%\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; flush privileges; $ MariaDB\u0026gt; exit; # # controller node Gnoochi 설치 # # $ controller\u0026gt; dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install openstack-gnocchi-api openstack-gnocchi-metricd python3-gnocchiclient # gnoochi 및 관련 모듈을 설치합니다. $ controller\u0026gt; vi /etc/gnocchi/gnocchi.conf [DEFAULT] log_dir = /var/log/gnocchi [api] auth_mode = keystone [database] backend = sqlalchemy [indexer] url = mysql+pymysql://gnocchi:qwer1234@controller/gnocchi [storage] driver = file file_basepath = /var/lib/gnocchi [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = gnocchi password = qwer1234 service_token_roles_required = true $ controller\u0026gt; vi /etc/httpd/conf.d/10-gnocchi_wsgi.conf Listen 8041 \u0026lt;VirtualHost *:8041\u0026gt; \u0026lt;Directory /usr/bin\u0026gt; AllowOverride None Require all granted \u0026lt;/Directory\u0026gt; CustomLog /var/log/httpd/gnocchi_wsgi_access.log combined ErrorLog /var/log/httpd/gnocchi_wsgi_error.log SetEnvIf X-Forwarded-Proto https HTTPS=1 WSGIApplicationGroup %{GLOBAL} WSGIDaemonProcess gnocchi display-name=gnocchi_wsgi user=gnocchi group=gnocchi processes=6 threads=6 WSGIProcessGroup gnocchi WSGIScriptAlias / /usr/bin/gnocchi-api \u0026lt;/VirtualHost\u0026gt; # 새로 생성합니다. $ controller\u0026gt; su -s /bin/bash gnocchi -c \u0026#34;gnocchi-upgrade\u0026#34; $ controller\u0026gt; systemctl enable --now openstack-gnocchi-metricd $ controller\u0026gt; systemctl restart httpd # gnocchi DB를 import 시킨 후 서비스를 등록합니다. $ controller\u0026gt; semanage port -a -t http_port_t -p tcp 8041 $ controller\u0026gt; firewall-cmd --add-port=8041/tcp --permanent $ controller\u0026gt; firewall-cmd --reload # SELinux 및 방화벽을 설정합니다. "},{"id":28,"href":"/erp/docs/OpenStack/OpenStackTraining/OpenStack-Ussuri-12/","title":"OpenStack Ussuri : Trove","section":"OpenStack Training","content":" ! 아직 수정 중 문제있음 # OpenStack Ussuri : Trove # # ----------------------- ----------------------- ----------------------- | [ Controller Node ] | | [ Compute Node ] | | [ Network Node ] | | | | Libvirt | | Open vSwitch | | MariaDB RabbitMQ | | Nova compute | | L2 Agent | | Memcached Keystone | | Open vSwitch | | L3 Agent | | httpd Cinder API | | L2 Agent | | metadata agent | | Nova-API Compute | | Cinder-LVM | | Swift-proxy | | L2 agent L3 agent | | NFS | | Heat API | | metadata agent | ----------------------- | API-CFN | | Neutron Server | | Heat Engine | | Gnocchi Trove API | ----------------------- ----------------------- --------------------------------- | [ Storage Node 1, 2, 3 ] | | | | Swift-account-auditor | | Swift-account-replicator | | Swift-account | | Swift-container-auditor | | Swift-container-replicator | | Swift-container-updater | | Swift-container | | Swift-object-auditor | | Swift-object-replicator | | Swift-object-updater | | Swift-swift-object | --------------------------------- # OpenStack Ussuri : Trove # # Trove는 관리형 데이터베이스 서비스 입니다. Trove*에 대한 설명은 Heat을 참조해주세요. # # Trove service 및 User 생성 # $ controller ~(keystone)\u0026gt; openstack user create --domain default --project service --password qwer1234 trove +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | default_project_id | b470c69e28db47cdbfc81e06cc67f627 | | domain_id | default | | enabled | True | | id | 3cdb0abe0a3a429ba08f98d8db786b6d | | name | trove | | options | {} | | password_expires_at | None | +---------------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack role add --project service --user trove admin $ controller ~(keystone)\u0026gt; openstack service create --name trove --description \u0026#34;Database\u0026#34; database +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | Database | | enabled | True | | id | 701b7dca93e74509aaf811eafc29cc03 | | name | trove | | type | database | +-------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne database public http://controller:8779/v1.0/%\\(tenant_id\\)s +--------------+-------------------------------------------+ | Field | Value | +--------------+-------------------------------------------+ | enabled | True | | id | 58fa28821c444c869be463b550f48651 | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | 701b7dca93e74509aaf811eafc29cc03 | | service_name | trove | | service_type | database | | url | http://controller:8779/v1.0/%(tenant_id)s | +--------------+-------------------------------------------+ $ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne database internal http://controller:8779/v1.0/%\\(tenant_id\\)s +--------------+-------------------------------------------+ | Field | Value | +--------------+-------------------------------------------+ | enabled | True | | id | 2809c9cb57884ceabf2902c6b6e62ced | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | 701b7dca93e74509aaf811eafc29cc03 | | service_name | trove | | service_type | database | | url | http://controller:8779/v1.0/%(tenant_id)s | +--------------+-------------------------------------------+ $ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne database admin http://controller:8779/v1.0/%\\(tenant_id\\)s +--------------+-------------------------------------------+ | Field | Value | +--------------+-------------------------------------------+ | enabled | True | | id | d86280b59bf04be28e71a57ff8b36a0b | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | 701b7dca93e74509aaf811eafc29cc03 | | service_name | trove | | service_type | database | | url | http://controller:8779/v1.0/%(tenant_id)s | +--------------+-------------------------------------------+ # Trove 유저의 DB를 생성합니다. # # $ controller\u0026gt; mysql -u root -p $ MariaDB\u0026gt; create database trove; $ MariaDB\u0026gt; grant all privileges on trove.* to trove@\u0026#39;localhost\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; grant all privileges on trove.* to trove@\u0026#39;%\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; flush privileges; $ MariaDB\u0026gt; exit; # # Trove 설치 # # $ controller ~(keystone)\u0026gt; dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install openstack-trove python-troveclient # Trove 서비스 및 관련 모듈을 설치합니다. $ controller ~(keystone)\u0026gt; vi /etc/trove/trove.conf [DEFAULT] network_driver = trove.network.neutron.NeutronDriver management_networks = ef7541ad-9599-4285-878a-e0ab62032b03 management_security_groups = d0d797f7-11d4-436e-89a3-ac8bca829f81 cinder_volume_type = lvmdriver-1 nova_keypair = trove-mgmt default_datastore = mysql taskmanager_manager = trove.taskmanager.manager.Manager trove_api_workers = 5 transport_url = rabbit://openstack:qwer1234@controller control_exchange = trove rpc_backend = rabbit reboot_time_out = 300 usage_timeout = 900 agent_call_high_timeout = 1200 use_syslog = False debug = True [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = trove password = qwer1234 service_token_roles_required = true [service_credentials] auth_url = http://controller/identity/v3 region_name = RegionOne project_name = service password = qwer1234 project_domain_name = Default user_domain_name = Default username = trove [database] connection = mysql+pymysql://trove:qwer1234@controller/trove [mariadb] tcp_ports = 3306,4444,4567,4568 [mysql] tcp_ports = 3306 [postgresql] tcp_ports = 5432 $ controller ~(keystone)\u0026gt; vi /etc/trove/trove-guestagent.conf [DEFAULT] log_file = trove-guestagent.log log_dir = /var/log/trove/ ignore_users = os_admin control_exchange = trove transport_url = rabbit://openstack:qwer1234@controller rpc_backend = rabbit command_process_timeout = 60 use_syslog = False debug = True [service_credentials] auth_url = http://controller/identity/v3 region_name = RegionOne project_name = service password = qwer1234 project_domain_name = Default user_domain_name = Default username = trove $ controller ~(keystone)\u0026gt; su -s /bin/sh -c \u0026#34;trove-manage db_sync\u0026#34; trove $ controller ~(keystone)\u0026gt; systemctl enable --now openstack-trove-api.service openstack-trove-taskmanager.service openstack-trove-conductor.service $ controller ~(keystone)\u0026gt; # "},{"id":29,"href":"/erp/docs/OpenStack/OpenStackTraining/OpenStack-Ussuri-13/","title":"OpenStack Ussuri : Designate","section":"OpenStack Training","content":" OpenStack Ussuri : Designate # # ----------------------- ----------------------- ----------------------- | [ Controller Node ] | | [ Compute Node ] | | [ Network Node ] | | | | Libvirt | | Open vSwitch | | MariaDB RabbitMQ | | Nova compute | | L2 Agent | | Memcached Keystone | | Open vSwitch | | L3 Agent | | httpd Cinder API | | L2 Agent | | metadata agent | | Nova-API Compute | | Cinder-LVM | | Swift-proxy | | L2 agent L3 agent | | NFS | | Heat API | | metadata agent | ----------------------- | API-CFN | | Neutron Server | | Heat Engine | | Gnocchi Trove API | | Designate Services | ----------------------- ----------------------- # OpenStack Ussuri : Designate # # Designate는 OpenStack 서비스에서 DNS 서비스를 배포, 관리를 담당합니다. Desigante는 Network node의 설치를 진행하고, controller node의 API를 이용하겠습니다. Designate의 보다 자세한 설명은 Designate를 참조해주세요. # # Designate service 및 User 생성 # $ controller ~(keystone)\u0026gt; openstack user create --domain default --project service --password qwer1234 designate +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | default_project_id | b470c69e28db47cdbfc81e06cc67f627 | | domain_id | default | | enabled | True | | id | 7563701765d24b4884c0b324b7997530 | | name | designate | | options | {} | | password_expires_at | None | +---------------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack role add --project service --user designate admin $ controller ~(keystone)\u0026gt; openstack service create --name designate --description \u0026#34;OpenStack DNS Service\u0026#34; dns +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | OpenStack DNS Service | | enabled | True | | id | 0e7dacc11b5b48c099d3fe110f8b8197 | | name | designate | | type | dns | +-------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne dns public http://network:9001/ +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 20b26900a14d44209ade2fab0a0f3bbc | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | 0e7dacc11b5b48c099d3fe110f8b8197 | | service_name | designate | | service_type | dns | | url | http://network:9001/ | +--------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne dns internal http://network:9001/ +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 856883757b604b93a1273ecc4775f549 | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | 0e7dacc11b5b48c099d3fe110f8b8197 | | service_name | designate | | service_type | dns | | url | http://network:9001/ | +--------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne dns admin http://network:9001/ +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | c6fef7cbb6a848228fa8ef4067ebcc49 | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | 0e7dacc11b5b48c099d3fe110f8b8197 | | service_name | designate | | service_type | dns | | url | http://network:9001/ | +--------------+----------------------------------+ # Designate 유저의 DB를 생성합니다. # # $ controller\u0026gt; mysql -u root -p $ MariaDB\u0026gt; create database designate; $ MariaDB\u0026gt; grant all privileges on designate.* to designate@\u0026#39;localhost\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; grant all privileges on designate.* to designate@\u0026#39;%\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; flush privileges; $ MariaDB\u0026gt; exit; # # Network Node Desigante 설치 # # $ network\u0026gt; dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install openstack-designate-api openstack-designate-central openstack-designate-worker openstack-designate-producer openstack-designate-mdns python3-designateclient bind bind-utils # designate 및 관련 모듈을 설치합니다. $ network\u0026gt; rndc-confgen -a -k designate -c /etc/designate.key -r /dev/urandom $ network\u0026gt; chown named:designate /etc/designate.key $ network\u0026gt; chmod 640 /etc/designate.key # 역할기반 키를 생성하고 권한을 설정합니다. $ network\u0026gt; cp /etc/named.conf /etc/named.conf.backup $ network\u0026gt; vi /etc/named.conf options { listen-on port 53 { any; }; listen-on-v6 port 53 { none; }; directory \u0026#34;/var/named\u0026#34;; dump-file \u0026#34;/var/named/data/cache_dump.db\u0026#34;; statistics-file \u0026#34;/var/named/data/named_stats.txt\u0026#34;; memstatistics-file \u0026#34;/var/named/data/named_mem_stats.txt\u0026#34;; # replace query range to your environment allow-query { localhost; 10.10.10.0/24; }; allow-new-zones yes; request-ixfr no; recursion no; bindkeys-file \u0026#34;/etc/named.iscdlv.key\u0026#34;; managed-keys-directory \u0026#34;/var/named/dynamic\u0026#34;; pid-file \u0026#34;/run/named/named.pid\u0026#34;; session-keyfile \u0026#34;/run/named/session.key\u0026#34;; }; include \u0026#34;/etc/designate.key\u0026#34;; controls { inet 0.0.0.0 port 953 allow { localhost; } keys { \u0026#34;designate\u0026#34;; }; }; logging { channel default_debug { file \u0026#34;data/named.run\u0026#34;; severity dynamic; }; }; zone \u0026#34;.\u0026#34; IN { type hint; file \u0026#34;named.ca\u0026#34;; }; $ network\u0026gt; chmod 640 /etc/named.conf $ network\u0026gt; chgrp named /etc/named.conf $ network\u0026gt; chown -R named. /var/named $ network\u0026gt; systemctl enable --now named # named dns 서비스를 시작합니다. $ network\u0026gt; vi /etc/designate/designate.conf [DEFAULT] log_dir = /var/log/designate transport_url = rabbit://openstack:qwer1234@controller root_helper = sudo designate-rootwrap /etc/designate/rootwrap.conf [database] connection = mysql+pymysql://designate:qwer1234@controller/designate [service:api] listen = 0.0.0.0:9001 auth_strategy = keystone api_base_uri = http://network:9001 enable_api_v2 = True enabled_extensions_v2 = quotas, reports [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = designate password = qwer1234 [service:worker] enabled = True notify = True [storage:sqlalchemy] connection = mysql+pymysql://designate:qwer1234@controller/designate $ network\u0026gt; su -s /bin/bash -c \u0026#34;designate-manage database sync\u0026#34; designate $ network\u0026gt; systemctl enable --now designate-central designate-api # designate api db를 임포트 시키고 서비스를 시작 및 등록합니다. $ network\u0026gt; vi /etc/designate/pools.yaml - name: default description: Default Pool attributes: {} ns_records: - hostname: network priority: 1 nameservers: - host: network port: 53 targets: - type: bind9 description: BIND9 Server masters: - host: network port: 5354 options: host: network port: 53 rndc_host: network rndc_port: 953 rndc_key_file: /etc/designate.key $ network\u0026gt; chmod 640 /etc/designate/pools.yaml $ network\u0026gt; chgrp designate /etc/designate/pools.yaml $ network\u0026gt; su -s /bin/bash -c \u0026#34;designate-manage pool update\u0026#34; designate $ network\u0026gt; systemctl enable --now designate-worker designate-producer designate-mdns # designate pool db를 임포트 시키고 서비스를 시작 및 등록합니다. $ network\u0026gt; setsebool -P named_write_master_zones on $ network\u0026gt; firewall-cmd --add-service=dns --permanent $ network\u0026gt; firewall-cmd --add-port={5354/tcp,9001/tcp} --permanent $ network\u0026gt; firewall-cmd --reload # SELinux 및 방화벽을 설정합니다. # # 확인 # # $ controller ~(keystone)\u0026gt; openstack dns service list +--------------------------------------+----------+--------------+--------+-------+--------------+ | id | hostname | service_name | status | stats | capabilities | +--------------------------------------+----------+--------------+--------+-------+--------------+ | 43f62f8d-20bc-43b9-8c64-758ac0a2a074 | network | central | UP | - | - | | ab90b2dc-381d-4ca8-ae66-fad57a9f9c11 | network | api | UP | - | - | | 2dbc8027-0c6a-4c4a-b7a0-92a2b19517a7 | network | worker | UP | - | - | | 502ce893-3256-432f-9c6f-2353078ee585 | network | producer | UP | - | - | | 078b306a-e3bb-461d-9c97-71679c9f8830 | network | mdns | UP | - | - | +--------------------------------------+----------+--------------+--------+-------+--------------+ $ controller ~(keystone)\u0026gt; openstack zone create --email dnsmaster@server.education server.education. $ controller ~(keystone)\u0026gt; openstack zone list $ controller ~(keystone)\u0026gt; openstack recordset create --record \u0026#39;192.168.100.10\u0026#39; --type A server.education. node01 $ controller ~(keystone)\u0026gt; openstack recordset list server.education. $ controller ~(keystone)\u0026gt; dig -p 5354 @network.srv.world node01.server.education. $ controller ~(keystone)\u0026gt; openstack zone create --email dnsmaster@server.education 100.168.192.in-addr.arpa. $ controller ~(keystone)\u0026gt; openstack zone list $ controller ~(keystone)\u0026gt; openstack recordset create --record \u0026#39;node01.server.education.\u0026#39; --type PTR 100.168.192.in-addr.arpa. 10 $ controller ~(keystone)\u0026gt; openstack recordset list 100.168.192.in-addr.arpa. $ controller ~(keystone)\u0026gt; dig -p 5354 @network.srv.world -x 192.168.100.10 $ controller ~(keystone)\u0026gt; openstack recordset list server.education. $ controller ~(keystone)\u0026gt; openstack recordset delete server.education. node01.server.education. $ controller ~(keystone)\u0026gt; openstack recordset list server.education. $ controller ~(keystone)\u0026gt; openstack zone list $ controller ~(keystone)\u0026gt; openstack zone delete server.education. $ controller ~(keystone)\u0026gt; openstack zone list # "},{"id":30,"href":"/erp/docs/OpenStack/OpenStackTraining/OpenStack-Ussuri-14/","title":"OpenStack Ussuri : Barbican","section":"OpenStack Training","content":" OpenStack Ussuri : Barbican # # ----------------------- ----------------------- ----------------------- | [ Controller Node ] | | [ Compute Node ] | | [ Network Node ] | | | | Libvirt | | Open vSwitch | | MariaDB RabbitMQ | | Nova compute | | L2 Agent | | Memcached Keystone | | Open vSwitch | | L3 Agent | | httpd Cinder API | | L2 Agent | | metadata agent | | Nova-API Compute | | Cinder-LVM | | Swift-proxy | | L2 agent L3 agent | | NFS | | Heat API | | metadata agent | ----------------------- | API-CFN | | Neutron Server | | Heat Engine | | Gnocchi Trove API | | Designate Services | | Barbican API | ----------------------- ----------------------- # OpenStack Ussuri : Barbican # # Barbican은 키 관리 서비스 입니다. 비밀 데이터의 안전한 저장, 프로비저닝 및 관리를 제공합니다. 여기에는 대칭 키, 비대칭 키, 인증서 및 원시 바이너리 데이터와 같은 키 자료가 포함됩니다. 자세한 설명은 Barbican을 참조해주세요. # # Barbican service 및 User 생성 # $ controller ~(keystone)\u0026gt; openstack user create --domain default --project service --password qwer1234 barbican +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | default_project_id | b470c69e28db47cdbfc81e06cc67f627 | | domain_id | default | | enabled | True | | id | bc85b317bd7c4cc1a4d5aee81c383421 | | name | barbican | | options | {} | | password_expires_at | None | +---------------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack role add --project service --user barbican admin $ controller ~(keystone)\u0026gt; openstack service create --name barbican --description \u0026#34;OpenStack Key Manager\u0026#34; key-manager -------------------------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | OpenStack Key Manager | | enabled | True | | id | ec2cbdda740a4887b5737fe885b4b86e | | name | barbican | | type | key-manager | +-------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne key-manager public http://controller:9311 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 3254f8ccb5894560ab3dea0268dddd03 | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | ec2cbdda740a4887b5737fe885b4b86e | | service_name | barbican | | service_type | key-manager | | url | http://controller:9311 | +--------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne key-manager internal http://controller:9311 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 37a440f72212422ca7c590e322afe56c | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | ec2cbdda740a4887b5737fe885b4b86e | | service_name | barbican | | service_type | key-manager | | url | http://controller:9311 | +--------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne key-manager admin http://controller:9311 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 2ad3a9aabcb840cc832470039ee37b00 | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | ec2cbdda740a4887b5737fe885b4b86e | | service_name | barbican | | service_type | key-manager | | url | http://controller:9311 | +--------------+----------------------------------+ # # Barbican 유저의 DB를 생성합니다. # # $ controller\u0026gt; mysql -u root -p $ MariaDB\u0026gt; create database barbican; $ MariaDB\u0026gt; grant all privileges on barbican.* to barbican@\u0026#39;localhost\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; grant all privileges on barbican.* to barbican@\u0026#39;%\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; flush privileges; $ MariaDB\u0026gt; exit; # # contoller node Barbican 설치 # $ controller ~(keystone)\u0026gt; dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install openstack-barbican # Barbucan 서비스 및 관련 모듈을 설치합니다. $ controller ~(keystone)\u0026gt; vi /etc/barbican/barbican.conf [DEFAULT] bind_host = 0.0.0.0 bind_port = 9311 host_href = http://controller:9311 log_file = /var/log/barbican/api.log sql_connection = mysql+pymysql://barbican:qwer1234@controller/barbican transport_url = rabbit://openstack:qwer1234@controller [oslo_policy] policy_file = /etc/barbican/policy.json policy_default_rule = default [secretstore] namespace = barbican.secretstore.plugin enabled_secretstore_plugins = store_crypto [crypto] namespace = barbican.crypto.plugin enabled_crypto_plugins = simple_crypto [simple_crypto_plugin] kek = \u0026#39;YWJjZGVmZ2hpamtsbW5vcHFyc3R1dnd4eXoxMjM0NTY=\u0026#39; [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = barbican password = qwer1234 $ controller ~(keystone)\u0026gt; su -s /bin/bash barbican -c \u0026#34;barbican-manage db upgrade\u0026#34; $ controller ~(keystone)\u0026gt; systemctl enable --now openstack-barbican-api # Barbican 서비스를 DB에 임포트 시킨 후, 서비스를 등록합니다. $ controller ~(keystone)\u0026gt; firewall-cmd --add-port=9311/tcp --permanent $ controller ~(keystone)\u0026gt; firewall-cmd --reload # 방화벽을 설정합니다. # # 확인 # # $ controller ~(keystone)\u0026gt; openstack secret store --name secret01 --payload secretkey +---------------+------------------------------------------------------------------------+ | Field | Value | +---------------+------------------------------------------------------------------------+ | Secret href | http://controller:9311/v1/secrets/86cbaa20-0cb9-479f-82ed-80a02f34b83d | | Name | secret01 | | Created | None | | Status | None | | Content types | None | | Algorithm | aes | | Bit length | 256 | | Secret type | opaque | | Mode | cbc | | Expiration | None | +---------------+------------------------------------------------------------------------+ $ controller ~(keystone)\u0026gt; openstack secret list +------------------------------------------------------------------------+----------+---------------------------+--------+---------------------------+-----------+------------+-------------+------+------------+ | Secret href | Name | Created | Status | Content types | Algorithm | Bit length | Secret type | Mode | Expiration | +------------------------------------------------------------------------+----------+---------------------------+--------+---------------------------+-----------+------------+-------------+------+------------+ | http://controller:9311/v1/secrets/86cbaa20-0cb9-479f-82ed-80a02f34b83d | secret01 | 2020-08-16T09:00:00+00:00 | ACTIVE | {\u0026#39;default\u0026#39;: \u0026#39;text/plain\u0026#39;} | aes | 256 | opaque | cbc | None | +------------------------------------------------------------------------+----------+---------------------------+--------+---------------------------+-----------+------------+-------------+------+------------+ $ controller ~(keystone)\u0026gt; openstack secret get http://controller:9311/v1/secrets/86cbaa20-0cb9-479f-82ed-80a02f34b83d +---------------+------------------------------------------------------------------------+ | Field | Value | +---------------+------------------------------------------------------------------------+ | Secret href | http://controller:9311/v1/secrets/86cbaa20-0cb9-479f-82ed-80a02f34b83d | | Name | secret01 | | Created | 2020-08-16T09:00:00+00:00 | | Status | ACTIVE | | Content types | {\u0026#39;default\u0026#39;: \u0026#39;text/plain\u0026#39;} | | Algorithm | aes | | Bit length | 256 | | Secret type | opaque | | Mode | cbc | | Expiration | None | +---------------+------------------------------------------------------------------------+ # get 뒤에는 키 생성시 생성되는 값을 입력해주셔야 됩니다 ! $ controller ~(keystone)\u0026gt; openstack secret get http://controller:9311/v1/secrets/86cbaa20-0cb9-479f-82ed-80a02f34b83d --payload +---------+-----------+ | Field | Value | +---------+-----------+ | Payload | secretkey | +---------+-----------+ $ controller ~(keystone)\u0026gt; openstack secret order create --name secret02 --algorithm aes --bit-length 256 --mode cbc --payload-content-type application/octet-stream key +----------------+-----------------------------------------------------------------------+ | Field | Value | +----------------+-----------------------------------------------------------------------+ | Order href | http://controller:9311/v1/orders/ffe9a05e-db5e-4b7d-8b5a-86f1349863c3 | | Type | Key | | Container href | N/A | | Secret href | None | | Created | None | | Status | None | | Error code | None | | Error message | None | +----------------+-----------------------------------------------------------------------+ $ controller ~(keystone)\u0026gt; openstack secret order list +-----------------------------------------------------------------------+------+----------------+------------------------------------------------------------------------+---------------------------+--------+------------+---------------+ | Order href | Type | Container href | Secret href | Created | Status | Error code | Error message | +-----------------------------------------------------------------------+------+----------------+------------------------------------------------------------------------+---------------------------+--------+------------+---------------+ | http://controller:9311/v1/orders/ffe9a05e-db5e-4b7d-8b5a-86f1349863c3 | Key | N/A | http://controller:9311/v1/secrets/4c3e2e5b-3585-44ae-901a-25dee6ede5a7 | 2020-08-16T09:08:06+00:00 | ACTIVE | None | None | +-----------------------------------------------------------------------+------+----------------+------------------------------------------------------------------------+---------------------------+--------+------------+---------------+ $ controller ~(keystone)\u0026gt; openstack secret order get http://controller:9311/v1/orders/ffe9a05e-db5e-4B7D-8B5A-86f1349863c3 +----------------+------------------------------------------------------------------------+ | Field | Value | +----------------+------------------------------------------------------------------------+ | Order href | http://controller:9311/v1/orders/ffe9a05e-db5e-4b7d-8b5a-86f1349863c3 | | Type | Key | | Container href | N/A | | Secret href | http://controller:9311/v1/secrets/4c3e2e5b-3585-44ae-901a-25dee6ede5a7 | | Created | 2020-08-16T09:08:06+00:00 | | Status | ACTIVE | | Error code | None | | Error message | None | +----------------+------------------------------------------------------------------------+ # get 뒤에는 키 생성시 생성되는 값을 입력해주셔야 됩니다 ! $ controller ~(keystone)\u0026gt; openstack secret get http://controller:9311/v1/secrets/4c3e2e5b-3585-44ae-901a-25dee6ede5a7 +---------------+------------------------------------------------------------------------+ | Field | Value | +---------------+------------------------------------------------------------------------+ | Secret href | http://controller:9311/v1/secrets/4c3e2e5b-3585-44ae-901a-25dee6ede5a7 | | Name | secret02 | | Created | 2020-08-16T09:08:06+00:00 | | Status | ACTIVE | | Content types | {\u0026#39;default\u0026#39;: \u0026#39;application/octet-stream\u0026#39;} | | Algorithm | aes | | Bit length | 256 | | Secret type | symmetric | | Mode | cbc | | Expiration | None | +---------------+------------------------------------------------------------------------+ # get 뒤에는 키 생성시 생성되는 값을 입력해주셔야 됩니다 ! # "},{"id":31,"href":"/erp/docs/OpenStack/OpenStackTraining/OpenStack-Ussuri-15/","title":"OpenStack Ussuri : Rally","section":"OpenStack Training","content":" OpenStack Ussuri : Rally # # ----------------------- ----------------------- ----------------------- | [ Controller Node ] | | [ Compute Node ] | | [ Network Node ] | | | | Libvirt | | Open vSwitch | | MariaDB RabbitMQ | | Nova compute | | L2 Agent | | Memcached Keystone | | Open vSwitch | | L3 Agent | | httpd Cinder API | | L2 Agent | | metadata agent | | Nova-API Compute | | Cinder-LVM | | Swift-proxy | | L2 agent L3 agent | | NFS | | Heat API | | metadata agent | ----------------------- | API-CFN | | Neutron Server | | Heat Engine | | Gnocchi Trove API | | Designate Services | | Barbican API | ----------------------- | Rally | ----------------------- # OpenStack Ussuri : Rally # # Rally는 오픈스택 소스를 GUI 환경으로 보여주는 서비스입니다. Rally의 자세한 설명은 Rally를 참조해주세요. # # Rally 유저의 DB를 생성합니다. # # $ controller\u0026gt; mysql -u root -p $ MariaDB\u0026gt; create database rally; $ MariaDB\u0026gt; grant all privileges on Rally.* to rally@\u0026#39;localhost\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; grant all privileges on Rally.* to rally@\u0026#39;%\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; flush privileges; $ MariaDB\u0026gt; exit; # # Rally 설치 # $ controller ~(keystone)\u0026gt; dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install openstack-rally openstack-rally-plugins python3-fixtures # Rally 서비스 및 관련 모듈을 설치합니다. $ controller ~(keystone)\u0026gt; vi /etc/rally/rally.conf [DEFAULT] log_file = rally.log log_dir = /var/log/rally connection = mysql+pymysql://rally:qwer1234@controller/rally $ controller ~(keystone)\u0026gt; mkdir /var/log/rally $ controller ~(keystone)\u0026gt; rally db create # log 파일을 저장할 폴더를 만들고 db를 임포트 시킵니다. $ controller ~(keystone)\u0026gt; rally deployment create --fromenv --name=my_cloud +--------------------------------------+----------------------------+----------+------------------+--------+ | uuid | created_at | name | status | active | +--------------------------------------+----------------------------+----------+------------------+--------+ | 35f9c79c-a47e-49d3-af88-b06b6020b92a | 2020-08-16T09:16:23.793238 | my_cloud | deploy-\u0026gt;finished | | +--------------------------------------+----------------------------+----------+------------------+--------+ $ controller ~(keystone)\u0026gt; source ~/.rally/openrc $ controller ~(keystone)\u0026gt; rally deployment show my_cloud +---------------------------+----------+----------+-------------+-------------+---------------+ | auth_url | username | password | tenant_name | region_name | endpoint_type | +---------------------------+----------+----------+-------------+-------------+---------------+ | http://controller:5000/v3 | admin | *** | admin | | None | +---------------------------+----------+----------+-------------+-------------+---------------+ $ controller ~(keystone)\u0026gt; rally deployment check -------------------------------------------------------------------------------- Platform openstack: -------------------------------------------------------------------------------- Available services: +-------------+----------------+-----------+ | Service | Service Type | Status | +-------------+----------------+-----------+ | __unknown__ | placement | Available | | barbican | key-manager | Available | | cinder | volumev3 | Available | | cloud | cloudformation | Available | | glance | image | Available | | gnocchi | metric | Available | | heat | orchestration | Available | | keystone | identity | Available | | neutron | network | Available | | nova | compute | Available | | swift | object-store | Available | | trove | database | Available | +-------------+----------------+-----------+ $ controller ~(keystone)\u0026gt; vi ~/boot-and-delete.json { \u0026#34;NovaServers.boot_and_delete_server\u0026#34;: [ { \u0026#34;args\u0026#34;: { \u0026#34;flavor\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;m1.small\u0026#34; }, \u0026#34;image\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Ubuntu1804\u0026#34; }, \u0026#34;force_delete\u0026#34;: false }, \u0026#34;runner\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;constant\u0026#34;, \u0026#34;times\u0026#34;: 10, \u0026#34;concurrency\u0026#34;: 2 }, \u0026#34;context\u0026#34;: {} } ] } $ controller ~(keystone)\u0026gt; rally task start ~/boot-and-delete.json -------------------------------------------------------------------------------- Preparing input task -------------------------------------------------------------------------------- Task is: { \u0026#34;NovaServers.boot_and_delete_server\u0026#34;: [ { \u0026#34;args\u0026#34;: { \u0026#34;flavor\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;m1.small\u0026#34; }, \u0026#34;image\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Ubuntu1804\u0026#34; }, \u0026#34;force_delete\u0026#34;: false }, \u0026#34;runner\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;constant\u0026#34;, \u0026#34;times\u0026#34;: 10, \u0026#34;concurrency\u0026#34;: 2 }, \u0026#34;context\u0026#34;: {} } ] } Task syntax is correct :) Running Rally version 3.0.0 -------------------------------------------------------------------------------- Task 887a0d20-37ad-4351-aeca-00f646634552: started -------------------------------------------------------------------------------- .... .... -------------------------------------------------------------------------------- Task 887a0d20-37ad-4351-aeca-00f646634552 has 0 error(s) -------------------------------------------------------------------------------- +-----------------------------------------------------------------------------------------------------------------------+ | Response Times (sec) | +--------------------+-----------+--------------+--------------+--------------+-----------+-----------+---------+-------+ | Action | Min (sec) | Median (sec) | 90%ile (sec) | 95%ile (sec) | Max (sec) | Avg (sec) | Success | Count | +--------------------+-----------+--------------+--------------+--------------+-----------+-----------+---------+-------+ | nova.boot_server | 4.466 | 5.409 | 34.472 | 40.002 | 45.533 | 14.471 | 100.0% | 10 | | nova.delete_server | 2.412 | 2.736 | 13.708 | 15.417 | 17.127 | 6.352 | 100.0% | 10 | | total | 6.922 | 12.091 | 40.809 | 44.416 | 48.023 | 20.823 | 100.0% | 10 | | -\u0026gt; duration | 5.922 | 11.091 | 39.809 | 43.416 | 47.023 | 19.823 | 100.0% | 10 | | -\u0026gt; idle_duration | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 100.0% | 10 | +--------------------+-----------+--------------+--------------+--------------+-----------+-----------+---------+-------+ Load duration: 108.251619 Full duration: 134.177109 HINTS: * To plot HTML graphics with this data, run: rally task report 887a0d20-37ad-4351-aeca-00f646634552 --out output.html * To generate a JUnit report, run: rally task export 887a0d20-37ad-4351-aeca-00f646634552 --type junit-xml --to output.xml * To get raw JSON output of task results, run: rally task report 887a0d20-37ad-4351-aeca-00f646634552 --json --out output.json # "},{"id":32,"href":"/erp/docs/OpenStack/OpenStackTraining/OpenStack-Ussuri-16/","title":"OpenStack Ussuri : Manila","section":"OpenStack Training","content":" OpenStack Ussuri : Manila # # ----------------------- ----------------------- ----------------------- | [ Controller Node ] | | [ Compute Node ] | | [ Network Node ] | | | | Libvirt | | Open vSwitch | | MariaDB RabbitMQ | | Nova compute | | L2 Agent | | Memcached Keystone | | Open vSwitch | | L3 Agent | | httpd Cinder API | | L2 Agent | | metadata agent | | Nova-API Compute | | Cinder-LVM | | Swift-proxy | | L2 agent L3 agent | | NFS | | Heat API | | metadata agent | | Manila Share | | API-CFN | | Neutron Server | ----------------------- | Heat Engine | | Gnocchi Trove API | | Designate Services | | Barbican API | ----------------------- | Rally Manila API | ----------------------- # OpenStack Ussuri : Manila # # Manila는 OpenStack에서 맡는 서비스입니다. Manila의 대한 보다 자세한 설명은 Manila를 참조해주세요. # # Manila service 및 User 생성 # $ controller ~(keystone)\u0026gt; openstack user create --domain default --project service --password qwer1234 manila +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | default_project_id | b470c69e28db47cdbfc81e06cc67f627 | | domain_id | default | | enabled | True | | id | 4ea7c62d89194d9883e6773a977133b6 | | name | manila | | options | {} | | password_expires_at | None | +---------------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack role add --project service --user manila admin $ controller ~(keystone)\u0026gt; openstack service create --name manila --description \u0026#34;OpenStack Shared Filesystem\u0026#34; share +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | OpenStack Shared Filesystem | | enabled | True | | id | 1129696ac3f5449293b638e0daec3bde | | name | manila | | type | share | +-------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack service create --name manilav2 --description \u0026#34;OpenStack Shared Filesystem V2\u0026#34; sharev2 +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | OpenStack Shared Filesystem V2 | | enabled | True | | id | 1d94787a2d34489dbe880faa5e165e5e | | name | manilav2 | | type | sharev2 | +-------------+----------------------------------+ $ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne share public http://controller:8786/v1/%\\(tenant_id\\)s +--------------+-----------------------------------------+ | Field | Value | +--------------+-----------------------------------------+ | enabled | True | | id | d3d6590a342047eab8abca304701d90d | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | 1129696ac3f5449293b638e0daec3bde | | service_name | manila | | service_type | share | | url | http://controller:8786/v1/%(tenant_id)s | +--------------+-----------------------------------------+ $ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne share internal http://controller:8786/v1/%\\(tenant_id\\)s +--------------+-----------------------------------------+ | Field | Value | +--------------+-----------------------------------------+ | enabled | True | | id | 0a6516d199d346febe62800b87a10eb9 | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | 1129696ac3f5449293b638e0daec3bde | | service_name | manila | | service_type | share | | url | http://controller:8786/v1/%(tenant_id)s | +--------------+-----------------------------------------+ $ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne share admin http://controller:8786/v1/%\\(tenant_id\\)s +--------------+-----------------------------------------+ | Field | Value | +--------------+-----------------------------------------+ | enabled | True | | id | 6153d88f7eab40caa669c3130f03226a | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | 1129696ac3f5449293b638e0daec3bde | | service_name | manila | | service_type | share | | url | http://controller:8786/v1/%(tenant_id)s | +--------------+-----------------------------------------+ $ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne sharev2 public http://controller:8786/v2/%\\(tenant_id\\)s +--------------+-----------------------------------------+ | Field | Value | +--------------+-----------------------------------------+ | enabled | True | | id | 22e9d4e2b62a4203ae182041c9c10049 | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | 1d94787a2d34489dbe880faa5e165e5e | | service_name | manilav2 | | service_type | sharev2 | | url | http://controller:8786/v2/%(tenant_id)s | +--------------+-----------------------------------------+ $ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne sharev2 internal http://controller:8786/v2/%\\(tenant_id\\)s +--------------+-----------------------------------------+ | Field | Value | +--------------+-----------------------------------------+ | enabled | True | | id | eb4e0b33fae7432d87078a0ba2c2e8de | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | 1d94787a2d34489dbe880faa5e165e5e | | service_name | manilav2 | | service_type | sharev2 | | url | http://controller:8786/v2/%(tenant_id)s | +--------------+-----------------------------------------+ $ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne sharev2 admin http://controller:8786/v2/%\\(tenant_id\\)s +--------------+-----------------------------------------+ | Field | Value | +--------------+-----------------------------------------+ | enabled | True | | id | 212c9ddfbc554dfb83f80e3a252db235 | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | 1d94787a2d34489dbe880faa5e165e5e | | service_name | manilav2 | | service_type | sharev2 | | url | http://controller:8786/v2/%(tenant_id)s | +--------------+-----------------------------------------+ # Manila 유저의 DB를 생성합니다. # # $ controller\u0026gt; mysql -u root -p $ MariaDB\u0026gt; create database manila; $ MariaDB\u0026gt; grant all privileges on manila.* to manila@\u0026#39;localhost\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; grant all privileges on manila.* to manila@\u0026#39;%\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; flush privileges; $ MariaDB\u0026gt; exit; # # controller node manila api 설치 # # $ controller\u0026gt; dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install openstack-manila python3-manilaclient # manila 및 관련 모듈을 설치합니다. $ controller\u0026gt; vi /etc/manila/manila.conf [DEFAULT] my_ip = controller api_paste_config = /etc/manila/api-paste.ini rootwrap_config = /etc/manila/rootwrap.conf state_path = /var/lib/manila auth_strategy = keystone default_share_type = default_share_type share_name_template = share-%s transport_url = rabbit://openstack:qwer1234@controller [database] connection = mysql+pymysql://manila:qwer1234@controller/manila [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = manila password = qwer1234 [oslo_concurrency] lock_path = $state_path/tmp $ controller\u0026gt; su -s /bin/bash manila -c \u0026#34;manila-manage db sync\u0026#34; $ controller\u0026gt; systemctl enable --now openstack-manila-api openstack-manila-scheduler $ controller\u0026gt; firewall-cmd --add-port=8786/tcp --permanent $ controller\u0026gt; firewall-cmd --reload # 방화벽을 설정합니다. $ controller ~(keystone)\u0026gt; manila service-list +----+------------------+------------+------+---------+-------+----------------------------+ | Id | Binary | Host | Zone | Status | State | Updated_at | +----+------------------+------------+------+---------+-------+----------------------------+ | 1 | manila-scheduler | controller | nova | enabled | up | 2020-08-21T01:27:53.000000 | +----+------------------+------------+------+---------+-------+----------------------------+ # 확인 # # compute node manila share 설차 # # $ compute\u0026gt; dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install openstack-manila-share python3-manilaclient mariadb-devel python3-devel gcc make $ compute\u0026gt; pip3 install mysqlclient $ compute\u0026gt; vi /etc/manila/manila.conf [DEFAULT] my_ip = compute api_paste_config = /etc/manila/api-paste.ini rootwrap_config = /etc/manila/rootwrap.conf state_path = /var/lib/manila auth_strategy = keystone default_share_type = default_share_type share_name_template = share-%s transport_url = rabbit://openstack:qwer1234@controller [database] connection = mysql+pymysql://manila:qwer1234@controller/manila [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = manila password = qwer1234 [oslo_concurrency] lock_path = $state_path/tmp $ compute\u0026gt; mkdir /var/lib/manila $ compute\u0026gt; chown manila. /var/lib/manila $ compute\u0026gt; firewall-cmd --add-service=nfs --permanent $ compute\u0026gt; firewall-cmd --reload # 새로운 Disk install $ compute\u0026gt; dnf -y install nfs-utils nfs4-acl-tools $ compute\u0026gt; fdisk /dev/sdc ... ... $ compute\u0026gt; pvcreate /dev/sdc1 $ compute\u0026gt; vgcreate manila-volumes /dev/sdc1 $ compute\u0026gt; vi /etc/manila/manila.conf [DEFAULT] enabled_share_backends = lvm [lvm] share_backend_name = LVM share_driver = manila.share.drivers.lvm.LVMShareDriver driver_handles_share_servers = False lvm_share_volume_group = manila-volumes lvm_share_export_ips = compute $ compute\u0026gt; systemctl enable --now openstack-manila-share nfs-server # # 확인 # # $ controller ~(keystone)\u0026gt; manila type-create default_share_type False +----------------------+--------------------------------------+ | Property | Value | +----------------------+--------------------------------------+ | ID | 9f6323f6-7443-4a83-ba70-7c10f78366c9 | | Name | default_share_type | | Visibility | public | | is_default | YES | | required_extra_specs | driver_handles_share_servers : False | | optional_extra_specs | | | Description | None | +----------------------+--------------------------------------+ $ controller ~(keystone)\u0026gt; manila type-list +--------------------------------------+--------------------+------------+------------+--------------------------------------+----------------------+-------------+ | ID | Name | visibility | is_default | required_extra_specs | optional_extra_specs | Description | +--------------------------------------+--------------------+------------+------------+--------------------------------------+----------------------+-------------+ | 9f6323f6-7443-4a83-ba70-7c10f78366c9 | default_share_type | public | YES | driver_handles_share_servers : False | | None | +--------------------------------------+--------------------+------------+------------+--------------------------------------+----------------------+-------------+ $ controller ~(keystone)\u0026gt; manila create NFS 10 --name share01 +---------------------------------------+--------------------------------------+ | Property | Value | +---------------------------------------+--------------------------------------+ | id | 72ab96c5-80f5-401a-b414-ab76a240acf1 | | size | 10 | | availability_zone | None | | created_at | 2020-08-21T01:52:16.000000 | | status | creating | | name | share01 | | description | None | | project_id | edd7025c02574d3aa2d3ab6e56208320 | | snapshot_id | None | | share_network_id | None | | share_proto | NFS | | metadata | {} | | share_type | 9f6323f6-7443-4a83-ba70-7c10f78366c9 | | is_public | False | | snapshot_support | False | | task_state | None | | share_type_name | default_share_type | | access_rules_status | active | | replication_type | None | | has_replicas | False | | user_id | 4ebf85318da84b5cb1257152f9fc35ba | | create_share_from_snapshot_support | False | | revert_to_snapshot_support | False | | share_group_id | None | | source_share_group_snapshot_member_id | None | | mount_snapshot_support | False | | progress | None | | share_server_id | None | | host | | +---------------------------------------+--------------------------------------+ $ controller ~(keystone)\u0026gt; manila list $ controller ~(keystone)\u0026gt; manila access-allow share01 ip 1.1.1.0/24 --access-level rw $ controller ~(keystone)\u0026gt; manila access-list share01 $ controller ~(keystone)\u0026gt; openstack server start CentOS_8 $ controller ~(keystone)\u0026gt; manila show share01 | grep path | cut -d\u0026#39;|\u0026#39; -f3 $ controller ~(keystone)\u0026gt; ssh centos@10.0.0.247 $ controller ~(keystone)\u0026gt; sudo mount -t nfs \\ 10.0.0.50:/var/lib/manila/mnt/share-3544d5a3-7157-4c10-aaa3-edd4b6fd2512 /mnt $ controller ~(keystone)\u0026gt; df -hT $ controller ~(keystone)\u0026gt; # "},{"id":33,"href":"/erp/docs/OpenStack/OpenStackTraining/Devstack/","title":"DevStack","section":"OpenStack Training","content":" # DevStack Stein 설치 # # DevStack # # Devbian 계열 ( ex : Ubuntu )의 OpenStack 자동화 설치 툴 # DevStack 설치 # # Update Ubuntu System $ sudo apt -y update $ sudo apt -y upgrade $ sudo apt -y dist-upgrade # Ubuntu의 시스템 및 패키지를 업데이트 합니다. $ sudo init 6 # 시스템을 재시작합니다. # # Add Stack User $ sudo useradd -s /bin/bash -d /opt/stack -m stack # devstack 설치를 위해 stack 유저를 생성합니다. $ echo \u0026#34;stack ALL=(ALL) NOPASSWD: ALL\u0026#34; | sudo tee /etc/sudoers.d/stack # 암호 없이 접근할 권한을 부여합니다. # # Download DevStack loacl.conf 파일의 추가적인 설정은 DevStack을 참조해주세요. $ su - stack $ sudo apt -y install git $ git clone https://git.openstack.org/openstack-dev/devstack # stack user로 진입하여, devstack을 다운받습니다. $ vi local.conf [[local|localrc]] ADMIN_PASSWORD=[ PW ] DATABASE_PASSWORD=$ADMIN_PASSWORD RABBIT_PASSWORD=$ADMIN_PASSWORD SERVICE_PASSWORD=$ADMIN_PASSWORD HOST_IP=[ 현재 호스트의 IP ] # 설치를 위한 설정 파일을 생성합니다. # PW, IP에는 사용자가 원하는 PW, 설치 호스트 네트워크의 IP를 기입합니다. # # Start OpenStack Deployment On Ubuntu 18.04 with DevStack $ cd devstack ./stack.sh # # Access OpenStack Dashboard http://[ HOST IP ]/dashboard로 접속합니다. User Name = admin Password = local.conf의 PW # "},{"id":34,"href":"/erp/docs/OpenStack/OpenStackTraining/Packstack/","title":"Packstack","section":"OpenStack Training","content":" Packstack Stein 설치 # # Packstack # # Redhat 계열 ( ex : CentOS )의 OpenStack 자동화 설치 툴 # # Packstack stain 설치 # # 기본적으로 PackStack은 올인원 or 다중노드로 구성할 수 있으며, 여기서는 올인원으로 설치를 진행하며, 다중노드에 대한 설정은 추가하도록 하겠습니다. # 설치사양 # # OS CPU RAM DISK CenOS7 4/ 2 10240 100G # 만약 다중 노드에 경우 소스를 분산시키고 각 노드에 설정을 추가합니다. hosts, hostname 등록 및 설정 다중 노드의 경우 controller node에서 다른 노드의 ssh 접속을 위한 키를 등록시킵니다. $ controller\u0026gt; $ ssh-keygen $ controller\u0026gt; $ ssh-copy-id network $ controller\u0026gt; $ ssh-copy-id compute $ controller\u0026gt; $ ssh-copy-id ... 다른 노드 # # 설치 순서 # # firewalld 설정 setenforce을 진행합니다. $ systemctl stop firewalld $ systemctl disable firewalld $ systemctl stop NetworkManagaer $ systemctl disable NetworkManagaer # 방화벽 및 네트워크 매니저 설정을 진행합니다. $ setenforce 0 $ sed -i \u0026#39;s/=enforcing/=disabled/g\u0026#39; /etc/sysconfig/selinux # setenforce 설정을 진행합니다. # # OpenStack stain release를 등록합니다. $ yum -y update # 기본 패키지를 업데이트 합니다. $ yum install -y centos-release-openstack-stein $ yum -y update # stein 레포지터리를 등록 후, 다시 업데이트를 진행합니다. # # 올인원의 경우 $ yum install -y openstack-packstack $ packstack --allinone # packstack을 통해 OpenStack 설치를 진행합니다. # # 다중노드의 경우 $ packstack --gen-answer-file=/root/stein-answer.txt # Packstack 설정 파일을 설치합니다. $ vi /root/stein.answer.txt CONFIG_CONTROLLER_HOST=contoller CONFIG_COMPUTE_HOSTS=compute1,compute2,compute3.... CONFIG_NETWORK_HOSTS=network1,network2.... CONFIG_PROVISION_DEMO=n CONFIG_NTP_SERVERS=0.centos.pool.ntp.org iburst, 1.centos.pool.ntp.org iburst, 2.centos.pool.ntp.org iburst, 3.centos.pool.ntp.org iburst CONFIG_CINDER_VOLUMES_SIZE=100G # 기본적인 설정을 진행합니다. # 설치 시 각 OpenStack의 서비스들을 원하는 Node의 설치할 수 있습니다. $ packstack --answer-file=/root/stein-answer.txt # packstack 설치를 진행합니다. # # 접속 IP, PW 확인 $ /var/tmp/packstack/....../openstack-setup.log | cat USERNAME= $ /var/tmp/packstack/....../openstack-setup.log | cat ADMIN_PW= # 사용자 이름 및 암호 출력 # "}]